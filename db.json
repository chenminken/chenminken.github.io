{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"9974667df9b265e6f96eee00aa2b6bfbca2b3f2e","modified":1605943576610},{"_id":"source/.DS_Store","hash":"48ac6c6bfd5e9e3f10722142653009130766b200","modified":1605972510728},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1605943576677},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1605943576677},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1605943576678},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1605943576680},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1605943576681},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1605943576681},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1605943576681},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1605943576682},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1605943576682},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1605943576683},{"_id":"themes/next/README.cn.md","hash":"58ffe752bc4b7f0069fcd6304bbc2d5ff7b80f89","modified":1605943576683},{"_id":"themes/next/README.md","hash":"898213e66d34a46c3cf8446bf693bd50db0d3269","modified":1605943576684},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1605943576686},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1605943576686},{"_id":"themes/next/_config.yml","hash":"a7c1f292fd71246976403ad096770a444653ad23","modified":1605943576685},{"_id":"themes/next/package.json","hash":"036d3a1346203d2f1a3958024df7f74e7ac07bfe","modified":1605943576732},{"_id":"source/_draft/.DS_Store","hash":"4a964544ad1ccee311fa5b3cf15b61d3adf9f635","modified":1606032168755},{"_id":"source/_draft/Ubuntu-Mysql数据库初始化设置.md","hash":"38a2fd0260e22de932ef8ab5f35cd1b24ff24502","modified":1605943576613},{"_id":"source/_draft/CNN学习笔记.md","hash":"b43ce7a31b8118c74b9816e1186eda4dee4de0bd","modified":1605943576612},{"_id":"source/_draft/CV-Project-1-Image-Filtering-and-Hybrid-Images.md","hash":"99557a7839afed41200f065fdaf2c8ea7a72b36f","modified":1605943576612},{"_id":"source/_draft/我为何想读研.md","hash":"76c6e4c65eddc6664f59afa3e059506122d77d12","modified":1605943576613},{"_id":"source/_draft/计算机视觉课堂学习笔记week4.md","hash":"f00ca7ab8092da26d805f377bede2903495d304e","modified":1605943576614},{"_id":"source/_draft/项目中的反思-和队友的沟通.md","hash":"8e1d681563a17a327e29d04b0fd23c1089762e4b","modified":1605943576614},{"_id":"source/_images/.DS_Store","hash":"70fb3d6c54bbe3f21896c5d84c4fe8bc9d7f96a1","modified":1605943576615},{"_id":"source/_posts/.DS_Store","hash":"b95c4e68996a719059ab333fc4a8cfe647831c1f","modified":1605943576623},{"_id":"source/_posts/Kaldi入门-一-yesno项目.md","hash":"61c3f3d879dcb6f3912bd4f7d9b787338f6be1df","modified":1605943576624},{"_id":"source/_posts/OpenCV和numpy笔记.md","hash":"8d2fd721eeaef3cc636e17235de2c31eb7c70118","modified":1606121112704},{"_id":"source/_posts/Machine-Learning-Week1-Note.md","hash":"514aa8f16c9ee94e5bed36864f172e7e9ba931e9","modified":1605943576625},{"_id":"source/_posts/如何测试那些难以测试的方法.md","hash":"98d9cb4ab712645936d2b16f27e90a9f7502f094","modified":1605943576626},{"_id":"source/_posts/python程序同步webdav网盘（坚果云、owncloud）.md","hash":"1b471edba1eb64e0662e44d60c2f4a9c8fa852eb","modified":1605943576625},{"_id":"source/_posts/我为什么决定读研究生.md","hash":"35a02d94c2947749432dad7e45b43f7b188231c5","modified":1605943576627},{"_id":"source/_posts/新服务器迁移postgres数据库.md","hash":"7932db912ba648b03502e5081e12284d47999198","modified":1605943576628},{"_id":"source/_posts/用python写一个过滤器.md","hash":"7b3fc0cae4660849f1e74b2bc6b7e051c32f5c19","modified":1605943576628},{"_id":"source/_posts/软件测试week1-note.md","hash":"ec6cd1e31c65ee89bda08c0d04d8eb212bc381c0","modified":1605943576629},{"_id":"source/_posts/软件测试week2-note.md","hash":"283c4c8e35f08f79e60b1310381f1898d3145627","modified":1605943576630},{"_id":"source/_posts/软件测试week3-note.md","hash":"33b75693f2e2ad8253500823e8e03f19a74ff63d","modified":1605943576630},{"_id":"source/_posts/用阿里云服务器自己搭建2do同步caldav服务器.md","hash":"072618976b40e7ac47eec2c29bf9c3b664c8f45a","modified":1605972510728},{"_id":"source/_posts/阅读日报20190911.md","hash":"d885114c4c49b9a6f5d3e8dc25d760bdf1d28e15","modified":1605943576632},{"_id":"source/_posts/重建博客.md","hash":"bd2fb33fc480adcea5169cd908815a380588b890","modified":1605943576631},{"_id":"source/tags/index.md","hash":"ad160b88c822ef55f7b50be3e6ad8efa71ef3be5","modified":1605943576632},{"_id":"themes/next/.git/config","hash":"bf7d1df65cf34d0f25a7184a58c37a09f72e4be7","modified":1602657289774},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1602657196694},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1605943576678},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1602657289770},{"_id":"themes/next/.git/index","hash":"e38052dfaae0f2d895005f6a070163c42908ba68","modified":1602657289858},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"352093a1b210c72136687fd2eee649244cee402c","modified":1605943576679},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1605943576679},{"_id":"themes/next/.git/packed-refs","hash":"3bb2e8e3fad44742d3e3bfadfb0b4d791fe9fe9e","modified":1602657289767},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1605943576680},{"_id":"themes/next/.git/FETCH_HEAD","hash":"d5932b39c178826b7db21fb111fe59f356b4e644","modified":1605944131765},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1605943576687},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1605943576687},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1605943576687},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1605943576688},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1605943576689},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1605943576688},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1605943576689},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1605943576689},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1605943576690},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1605943576690},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1605943576691},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1605943576691},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1605943576691},{"_id":"themes/next/languages/zh-Hans.yml","hash":"16ef56d0dea94638de7d200984c90ae56f26b4fe","modified":1605943576692},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1605943576692},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1605943576693},{"_id":"themes/next/layout/_layout.swig","hash":"da0929166674ea637e0ad454f85ad0d7bac4aff2","modified":1605943576694},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1605943576729},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1605943576729},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1605943576730},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1605943576730},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1605943576731},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1605943576731},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1605943576733},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1605943576732},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1605943576733},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1605943576828},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1605943576828},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1605943576829},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1605943576774},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1602657196695},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1602657196696},{"_id":"themes/next/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1602657196697},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1602657196698},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"33729ad4ce51acda35094e581e4088f3167a0af8","modified":1602657196696},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1602657196698},{"_id":"themes/next/.git/hooks/pre-merge-commit.sample","hash":"04c64e58bc25c149482ed45dbd79e40effb89eb7","modified":1602657196698},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1602657196695},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1602657196697},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1602657196697},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1602657196694},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1602657196698},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1602657196699},{"_id":"themes/next/.git/logs/HEAD","hash":"1b538855604dfbc8538669a797ca4941f40cbe37","modified":1602657289772},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1605943576693},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1605943576694},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1605943576695},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1605943576695},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1605943576696},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1605943576696},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1605943576697},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"4e2a078509b8ac53a501f46e99a4e7cecd64a7e6","modified":1605943576697},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c4d6181f5d3db5365e622f78714af8cc58d7a45e","modified":1605943576698},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1605943576698},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1605943576708},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1605943576707},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1605943576698},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1605943576708},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1605943576709},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1605943576713},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1605943576714},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1605943576715},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1605943576724},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1605943576724},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1605943576725},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1605943576725},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1605943576725},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1605943576726},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1605943576726},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1605943576734},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1605943576734},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1605943576734},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1605943576735},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1605943576735},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1605943576735},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1605943576735},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1605943576736},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1605943576736},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1605943576774},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1605943576775},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1605943576775},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1605943576775},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1605943576775},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1605943576775},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1605943576776},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1605943576776},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1605943576776},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1605943576776},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1605943576776},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1605943576777},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1605943576777},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1605943576777},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1605943576777},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1605943576777},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1605943576778},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1605943576778},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1605943576778},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1605943576715},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1605943576715},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1605943576761},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1605943576761},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1605943576762},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1605943576773},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1605943576774},{"_id":"themes/next/.git/refs/heads/master","hash":"3c959678e3fe6e51e935526c19927d21443a3be3","modified":1602657289771},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1605943576706},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1605943576699},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1605943576709},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1605943576710},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1605943576710},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1605943576711},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1605943576711},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1605943576711},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1605943576712},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1605943576714},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1605943576714},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1605943576716},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1605943576715},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1605943576716},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1605943576717},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1605943576718},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1605943576717},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1605943576718},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1605943576718},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1605943576719},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1605943576720},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1605943576719},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1605943576719},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1605943576720},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1605943576721},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1605943576721},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1605943576721},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1605943576722},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1605943576722},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1605943576723},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1605943576723},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1605943576723},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1605943576724},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1605943576724},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1605943576728},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1605943576728},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1605943576727},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1605943576728},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1605943576760},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1605943576761},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1605943576761},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1605943576762},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1605943576772},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1605943576772},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1605943576773},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1605943576774},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1605943576778},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1605943576779},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1605943576779},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1605943576779},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1605943576779},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1605943576780},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1605943576781},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1605943576781},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1605943576786},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1605943576789},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1605943576789},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1602657289829},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1602657289829},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1605943576794},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1605943576794},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1605943576795},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1605943576795},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1605943576799},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1605943576800},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1605943576800},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1605943576801},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1605943576801},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1605943576779},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1605943576780},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1605943576780},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1605943576814},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1605943576815},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1605943576815},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1605943576816},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1605943576816},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1605943576811},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1605943576817},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1605943576815},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1605943576817},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1605943576818},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1605943576818},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1605943576818},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1605943576818},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1605943576818},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1605943576819},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1605943576819},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1605943576819},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1605943576819},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1605943576819},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1605943576820},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1605943576820},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1605943576820},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1605943576820},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1605943576821},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1605943576825},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1605943576825},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1605943576816},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1605943576817},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1605943576827},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1605943576817},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1605943576827},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1605943576828},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1605943576812},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"1b538855604dfbc8538669a797ca4941f40cbe37","modified":1602657289772},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1602657289769},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1605943576726},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1605943576727},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1605943576736},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1605943576737},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1605943576737},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1605943576738},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1605943576737},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1605943576742},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1605943576750},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1605943576758},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1605943576759},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1605943576758},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1605943576759},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1605943576760},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1605943576759},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1605943576760},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1605943576763},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1605943576762},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1605943576763},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1605943576763},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1605943576764},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1605943576764},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1605943576765},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1605943576765},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1605943576767},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1605943576767},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"02fb8fa6b6c252b6bed469539cd057716606a787","modified":1605943576768},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1605943576768},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1605943576769},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1605943576771},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1605943576769},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1605943576770},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1605943576771},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1605943576770},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1605943576772},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1605943576780},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1605943576786},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1605943576786},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1605943576790},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1605943576790},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1605943576790},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1605943576790},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1605943576791},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1605943576791},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1605943576792},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1605943576792},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1605943576798},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1605943576796},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1605943576802},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1605943576802},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1605943576802},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1605943576824},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1605943576824},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1605943576785},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1605943576785},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1605943576792},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1605943576809},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1605943576809},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1605943576826},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"1b538855604dfbc8538669a797ca4941f40cbe37","modified":1602657289769},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1605943576738},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1605943576738},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1605943576739},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1605943576739},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1605943576739},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1605943576740},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1605943576739},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1605943576740},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1605943576741},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1605943576740},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1605943576741},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1605943576741},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1605943576741},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1605943576742},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1605943576742},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1605943576743},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1605943576743},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1605943576743},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1605943576743},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1605943576744},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1605943576744},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1605943576745},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1605943576745},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1605943576745},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1605943576745},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1605943576746},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1605943576747},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1605943576746},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1605943576746},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1605943576747},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1605943576746},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1605943576748},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1605943576748},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"ce93e6796a586b0c018956a9071be878e62f91de","modified":1605943576747},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1605943576748},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1605943576748},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1605943576749},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1605943576749},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1605943576750},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1605943576752},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1605943576751},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1605943576753},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1605943576751},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1605943576754},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1605943576751},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1605943576753},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1605943576753},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1605943576754},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1605943576755},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1605943576755},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1605943576756},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1605943576756},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1605943576756},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1605943576757},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1605943576757},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1605943576757},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1605943576757},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1605943576758},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1605943576766},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1605943576766},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1605943576769},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1605943576782},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1605943576781},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1605943576784},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1605943576784},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1605943576784},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1605943576791},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1605943576791},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1605943576792},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1605943576791},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1605943576791},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1605943576792},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1605943576803},{"_id":"themes/next/.git/objects/pack/pack-ae89971e70af08f4b3bb5947d4cf94714e28852a.idx","hash":"7bfa6f4a2b71a14480813648d406cc74cd001ffa","modified":1602657289750},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1605943576805},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1605943576808},{"_id":"source/_images/2do.png","hash":"b7e279f356b86c8fdab5c2657fb8c2f650f469b7","modified":1605943576622},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1605943576789},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1605943576807},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1605943576823},{"_id":"themes/next/.git/objects/pack/pack-ae89971e70af08f4b3bb5947d4cf94714e28852a.pack","hash":"27240647aeb20b44776f9503824b2c516ab23196","modified":1602657289744},{"_id":"public/tags/index.html","hash":"33d73c21385412cfe44a2d6da49dc293f01608c5","modified":1606121130061},{"_id":"public/2019/09/16/软件测试week3-note/index.html","hash":"0c2f9afea8cd78f165efca0e4ae7c325d4114d6c","modified":1606121130061},{"_id":"public/2019/09/09/软件测试week1-note/index.html","hash":"8fa1050bb0a40486fdf1e3f96a04fef7e993e7bb","modified":1606121130062},{"_id":"public/2019/09/08/重建博客/index.html","hash":"e54890b653df1ba77278613b38fa3d816d55463f","modified":1606121130062},{"_id":"public/archives/page/2/index.html","hash":"1fdc047af903f34ef71f2984b5b8733b47ab9774","modified":1606121130062},{"_id":"public/archives/2020/index.html","hash":"9c90d28b8cc09a0f326b67ac441337d0f3ca23ca","modified":1606121130062},{"_id":"public/archives/2020/01/index.html","hash":"0f74db888d621bb91c6c1fde21a1fc20a447aec4","modified":1606121130062},{"_id":"public/archives/2020/05/index.html","hash":"706588157b1659a0c51460f5879654d99f250c85","modified":1606121130062},{"_id":"public/archives/2020/11/index.html","hash":"edd86deefe621713a99092e15fb1927046eef11b","modified":1606121130062},{"_id":"public/tags/OpenCV/index.html","hash":"3f064550ed81f15d827b33b110f6832dc11fe3af","modified":1606121130062},{"_id":"public/tags/Numpy/index.html","hash":"a5fea2220f247ca0f6eec252a8f7802fbacea14a","modified":1606121130062},{"_id":"public/tags/Python/index.html","hash":"244a269d6ea677cf10a5b85a7b0c43b10e4c9306","modified":1606121130062},{"_id":"public/tags/编程记录/index.html","hash":"fddc8dfaffbed00a0acfe6dbbf5082fae70f07e3","modified":1606121130062},{"_id":"public/tags/MachineLearning/index.html","hash":"9758d9464de0a8f8e40db3cc6548d41c7bad0e08","modified":1606121130062},{"_id":"public/tags/Coursera/index.html","hash":"89074377ed4ccf64bc8f0d537f6346f5d2a09239","modified":1606121130062},{"_id":"public/tags/软件测试/index.html","hash":"68d038ef88b371d8abd9ad7878efa3da184031b7","modified":1606121130062},{"_id":"public/tags/Java/index.html","hash":"dfda789e6574b2ac49af2da2f95366401b1b6447","modified":1606121130062},{"_id":"public/tags/个人/index.html","hash":"ed4c0079e1ee5bbfc3bacbf908878a95b4484f7a","modified":1606121130062},{"_id":"public/tags/运维/index.html","hash":"44844fe1cd80e4f3f69224dba4bb2c8211edb364","modified":1606121130062},{"_id":"public/tags/Postgres/index.html","hash":"f033b2c12c3bee69daef4b54ceb7c1d2ba1c7e7a","modified":1606121130062},{"_id":"public/tags/Linux/index.html","hash":"dc27945cd5d7df0363a568e543c52b02cd8be8f0","modified":1606121130062},{"_id":"public/tags/阅读日报/index.html","hash":"d18c34f9299e8d3cfd22c129ea1fa5b39d61e174","modified":1606121130062},{"_id":"public/tags/Kaldi/index.html","hash":"b0e0ed18e8d591e8865a10e0c4c74d6479669314","modified":1606121130062},{"_id":"public/tags/ASR/index.html","hash":"0716800b5a3e7ebeee33dc8e478fb40eb53100fc","modified":1606121130062},{"_id":"public/2020/11/22/OpenCV和numpy笔记/index.html","hash":"17ec14b68f50b4f81d09e2492fe4ebcb5f59e658","modified":1606121130063},{"_id":"public/2020/11/21/新服务器迁移postgres数据库/index.html","hash":"683799683c341bd26c6470cca045a373207849df","modified":1606121130063},{"_id":"public/2020/05/01/Kaldi入门-一-yesno项目/index.html","hash":"d903d40ba8e842cd07b05afce62dff0c1caa7c36","modified":1606121130063},{"_id":"public/2020/01/10/python程序同步webdav网盘（坚果云、owncloud）/index.html","hash":"b6f3d4ef167c51910ddd0868afefb580f6b6e4a5","modified":1606121130063},{"_id":"public/2019/09/27/用python写一个过滤器/index.html","hash":"f4283851f51a3be09bc517bd9297a36a509b9bb6","modified":1606121130063},{"_id":"public/2019/09/26/我为什么决定读研究生/index.html","hash":"8877347a7fa7bf3ccc8fed6500931e98147649ad","modified":1606121130063},{"_id":"public/2019/09/23/用阿里云服务器自己搭建2do同步caldav服务器/index.html","hash":"c4ad0a9c780fb495be7432ed2e7e5c5a09b07d4e","modified":1606121130063},{"_id":"public/2019/09/23/如何测试那些难以测试的方法/index.html","hash":"922d23694aee06e6fe027e48f0bd30018732f69e","modified":1606121130063},{"_id":"public/2019/09/16/软件测试week2-note/index.html","hash":"de142ff7d0e5420cf1564ace65e749401cb846c1","modified":1606121130063},{"_id":"public/2019/09/13/Machine-Learning-Week1-Note/index.html","hash":"a1f09f4cc4bbeba3fbdc83b79e3815691999fe85","modified":1606121130063},{"_id":"public/2019/09/11/阅读日报20190911/index.html","hash":"06d512053e5fec14588b7450a6c71b1fcd5ec966","modified":1606121130063},{"_id":"public/archives/index.html","hash":"44245550b0c9392ff3680af70e353d30ea13a1c8","modified":1606121130063},{"_id":"public/archives/2019/index.html","hash":"2c0786aa648687231f3800db73e25d89001fdf9d","modified":1606121130063},{"_id":"public/archives/2019/09/index.html","hash":"509a3129f7acadfaedc8e2c6eab3cb7a43290635","modified":1606121130063},{"_id":"public/index.html","hash":"eb5ee62a040fc44ab092b2b72235f47e10701804","modified":1606121130063},{"_id":"public/page/2/index.html","hash":"450f659bf039b40d3b127123161868295a6becf5","modified":1606121130063},{"_id":"public/CNAME","hash":"9974667df9b265e6f96eee00aa2b6bfbca2b3f2e","modified":1606121130068},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1606121130068},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1606121130068},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1606121130068},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1606121130068},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1606121130068},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1606121130068},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1606121130068},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1606121130068},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1606121130069},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1606121130069},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1606121130069},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1606121130069},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1606121130069},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1606121130069},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1606121130069},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1606121130069},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1606121130069},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1606121130069},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1606121130069},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1606121130069},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1606121130069},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1606121130069},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1606121130069},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1606121130069},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1606121130069},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1606121130069},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1606121130069},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1606121130069},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1606121130069},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1606121130069},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1606121130069},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1606121130069},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1606121130069},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1606121130636},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1606121130640},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1606121130650},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1606121130650},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1606121130651},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1606121130651},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1606121130651},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1606121130651},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1606121130651},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1606121130651},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1606121130651},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1606121130651},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1606121130651},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1606121130651},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1606121130651},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1606121130651},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1606121130651},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1606121130651},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1606121130651},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1606121130651},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1606121130652},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1606121130652},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1606121130652},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1606121130652},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1606121130652},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1606121130652},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1606121130652},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1606121130652},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1606121130652},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1606121130652},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1606121130652},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1606121130652},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1606121130652},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1606121130652},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1606121130652},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1606121130652},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1606121130652},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1606121130652},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1606121130652},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1606121130652},{"_id":"public/lib/fastclick/README.html","hash":"c5a4c05ca80132b7e343d5fd1d1d1a976b4ad151","modified":1606121130652},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"81057e5b518b8ab60474e1ad027e868b558f15b7","modified":1606121130652},{"_id":"public/lib/jquery_lazyload/README.html","hash":"18a600ca1aafd3bf08af52b6a6fe5f056aeed9f4","modified":1606121130652},{"_id":"public/css/main.css","hash":"8f5a787fd4ec144db130b1408019b2c002ea707f","modified":1606121130652},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1606121130652},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1606121130652},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1606121130652},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1606121130657},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1606121130657},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1606121130657},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1606121130658},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1606121130658},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1606121130658},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1606121130658},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1606121130658},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1606121130663},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1606121130663},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1606121130667},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1606121130667},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1606121130672},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1606121130672},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1606121130674},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1606121130674},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1606121130674},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1606121130674},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1606121130674},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1606121130678},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1606121130679},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1606121130679},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1606121130686},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1606121130694},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1606121130697},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1606121130705},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1606121130709},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1606121130718},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1606121130720}],"Category":[],"Data":[],"Page":[{"title":"Tags","date":"2019-09-16T01:09:23.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\ndate: 2019-09-16 09:09:23\ntype: \"tags\"\n---\n","updated":"2020-11-21T07:26:16.632Z","path":"tags/index.html","comments":1,"layout":"page","_id":"ckhub1tvs0001p92r8emqfadz","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"OpenCV和numpy笔记","date":"2020-11-22T12:33:53.000Z","_content":"\n使用K-means做视频图像分割的作业的时候，遇到一个问题值得记录。\n\n该代码的功能是使用K-means做图像分割并保存图像与视频。\n\n遇到的问题是kmean之后得到的是每一个像素点的分类标签，需要将这些标签可视化成为分割(Segmentation)的结果视频。一开始没有直接找到相关的函数，搜索查询后给的建议都是使用`matlibplot`的`plt.imshow()`函数来实现，但是我不想要每次保存成为图片再转成视频，考虑到opencv-python的底层使用`numpy.array`来表述图像的。一个BGR图像相当于一个3维uint8数组（w,h,c)。要做的就是生成一个标签到像素的映射关系。这里我用的是`matplotlib.cm` color map 功能。\n\n```\nx = np.linspace(0.0, 1.0, 10)\nrgb = cm.get_cmap(plt.get_cmap('Set1'))(x)[np.newaxis, :, :3][0]\n```\n\n\n\n\n\n```\nimport numpy as np\nfrom numpy.matlib import repmat\nfrom sklearn.preprocessing import normalize\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom sklearn.cluster import KMeans\nimport cv2\nfrom PIL import Image\n\nn_cl=5\n\nvideoCapture = cv2.VideoCapture('road_video.mov')\nfps = videoCapture.get(cv2.CAP_PROP_FPS)\nsize = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)),\n        int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\nvideoWriter = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), (fps/10), size)\n\ndef kmeans(data, n_cl, verbose=True):\n    n_samples, dim = data.shape\n    centers = data[np.random.choice(range(n_samples), size=n_cl)]\n    old_labels = np.zeros(shape=n_cl)\n    while True:\n        distances = np.zeros((n_samples, n_cl))\n        for cluster_idx, cluster in enumerate(centers):\n            distances[:, cluster_idx] = np.sum(np.square(data - repmat(cluster, n_samples, 1)), axis=1)\n        new_labels = np.argmin(distances, axis=1)\n        for l in range(0, n_cl):\n            centers[l] = np.mean(data[new_labels==l], axis=0)\n        if verbose:\n            fig, ax = plt.subplots()\n            ax.scatter(data[:, 0], data[:, 1], cluster=new_labels, s=40)\n            ax.plot(centers[:, 0], centers[:, 1], 'r*', markersize=20)\n            plt.waitforbuttonpress()\n            plt.close()\n        if np.array_equal(new_labels, old_labels):\n            break\n        old_labels = new_labels\n    return new_labels\n\ncount = 0\nx = np.linspace(0.0, 1.0, 10)\nrgb = cm.get_cmap(plt.get_cmap('Set1'))(x)[np.newaxis, :, :3][0]\n\nwhile True:\n    print(count)\n    count += 1\n#   load the frame\n    success, frame = videoCapture.read()\n    if not success:\n        break\n    img = np.float32(frame)\n    h,w,c = img.shape\n    # print(h,w,c)\n#   add coordinates\n    row_indexes = np.arange(0, h)\n    col_indexes = np.arange(0, w)\n    coordinates = np.zeros(shape=(h,w,2))\n    coordinates[..., 0] = normalize(repmat(row_indexes, w, 1).T)\n    coordinates[..., 1] = normalize(repmat(col_indexes, h, 1))\n    data = np.concatenate((img, coordinates), axis=-1)\n    data = np.reshape(data, newshape=(w *h, 5))\n    labels = kmeans(data, n_cl=n_cl, verbose=False)\n\n    print('after')\n    labels = labels.flatten()\n    segmented_image = rgb[labels.flatten()]\n    # print(segmented_image)\n    segmented_image = segmented_image.reshape(img.shape)\n    plt.imshow(segmented_image)\n    # # plt.imshow(np.reshape(labels, (h, w)), cmap=\"hsv\")\n    plt.savefig(\"lab9/\"+str(count-1))\n    segmented_image = segmented_image *255\n    segmented_image = segmented_image.astype('uint8')\n    videoWriter.write(segmented_image)\n\nvideoWriter.release()\nvideoCapture.release()\n```\n\n## 参考网页\n\nhttps://blog.csdn.net/llh_1178/article/details/77833447\n\nhttps://docs.opencv.org/3.4/d4/dba/classcv_1_1viz_1_1Color.html\n\nhttps://realpython.com/python-opencv-color-spaces/\n\nhttps://www.thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python\n\nhttps://pvss.github.io/Opencv+Python.html\n\nhttps://stackoverflow.com/questions/9280653/writing-numpy-arrays-using-cv2-videowriter\n\n","source":"_posts/OpenCV和numpy笔记.md","raw":"---\ntitle: OpenCV和numpy笔记\ndate: 2020-11-22 20:33:53\ntags:\n- OpenCV\n- Numpy\n- Python\n- 编程记录\n---\n\n使用K-means做视频图像分割的作业的时候，遇到一个问题值得记录。\n\n该代码的功能是使用K-means做图像分割并保存图像与视频。\n\n遇到的问题是kmean之后得到的是每一个像素点的分类标签，需要将这些标签可视化成为分割(Segmentation)的结果视频。一开始没有直接找到相关的函数，搜索查询后给的建议都是使用`matlibplot`的`plt.imshow()`函数来实现，但是我不想要每次保存成为图片再转成视频，考虑到opencv-python的底层使用`numpy.array`来表述图像的。一个BGR图像相当于一个3维uint8数组（w,h,c)。要做的就是生成一个标签到像素的映射关系。这里我用的是`matplotlib.cm` color map 功能。\n\n```\nx = np.linspace(0.0, 1.0, 10)\nrgb = cm.get_cmap(plt.get_cmap('Set1'))(x)[np.newaxis, :, :3][0]\n```\n\n\n\n\n\n```\nimport numpy as np\nfrom numpy.matlib import repmat\nfrom sklearn.preprocessing import normalize\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom sklearn.cluster import KMeans\nimport cv2\nfrom PIL import Image\n\nn_cl=5\n\nvideoCapture = cv2.VideoCapture('road_video.mov')\nfps = videoCapture.get(cv2.CAP_PROP_FPS)\nsize = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)),\n        int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\nvideoWriter = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), (fps/10), size)\n\ndef kmeans(data, n_cl, verbose=True):\n    n_samples, dim = data.shape\n    centers = data[np.random.choice(range(n_samples), size=n_cl)]\n    old_labels = np.zeros(shape=n_cl)\n    while True:\n        distances = np.zeros((n_samples, n_cl))\n        for cluster_idx, cluster in enumerate(centers):\n            distances[:, cluster_idx] = np.sum(np.square(data - repmat(cluster, n_samples, 1)), axis=1)\n        new_labels = np.argmin(distances, axis=1)\n        for l in range(0, n_cl):\n            centers[l] = np.mean(data[new_labels==l], axis=0)\n        if verbose:\n            fig, ax = plt.subplots()\n            ax.scatter(data[:, 0], data[:, 1], cluster=new_labels, s=40)\n            ax.plot(centers[:, 0], centers[:, 1], 'r*', markersize=20)\n            plt.waitforbuttonpress()\n            plt.close()\n        if np.array_equal(new_labels, old_labels):\n            break\n        old_labels = new_labels\n    return new_labels\n\ncount = 0\nx = np.linspace(0.0, 1.0, 10)\nrgb = cm.get_cmap(plt.get_cmap('Set1'))(x)[np.newaxis, :, :3][0]\n\nwhile True:\n    print(count)\n    count += 1\n#   load the frame\n    success, frame = videoCapture.read()\n    if not success:\n        break\n    img = np.float32(frame)\n    h,w,c = img.shape\n    # print(h,w,c)\n#   add coordinates\n    row_indexes = np.arange(0, h)\n    col_indexes = np.arange(0, w)\n    coordinates = np.zeros(shape=(h,w,2))\n    coordinates[..., 0] = normalize(repmat(row_indexes, w, 1).T)\n    coordinates[..., 1] = normalize(repmat(col_indexes, h, 1))\n    data = np.concatenate((img, coordinates), axis=-1)\n    data = np.reshape(data, newshape=(w *h, 5))\n    labels = kmeans(data, n_cl=n_cl, verbose=False)\n\n    print('after')\n    labels = labels.flatten()\n    segmented_image = rgb[labels.flatten()]\n    # print(segmented_image)\n    segmented_image = segmented_image.reshape(img.shape)\n    plt.imshow(segmented_image)\n    # # plt.imshow(np.reshape(labels, (h, w)), cmap=\"hsv\")\n    plt.savefig(\"lab9/\"+str(count-1))\n    segmented_image = segmented_image *255\n    segmented_image = segmented_image.astype('uint8')\n    videoWriter.write(segmented_image)\n\nvideoWriter.release()\nvideoCapture.release()\n```\n\n## 参考网页\n\nhttps://blog.csdn.net/llh_1178/article/details/77833447\n\nhttps://docs.opencv.org/3.4/d4/dba/classcv_1_1viz_1_1Color.html\n\nhttps://realpython.com/python-opencv-color-spaces/\n\nhttps://www.thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python\n\nhttps://pvss.github.io/Opencv+Python.html\n\nhttps://stackoverflow.com/questions/9280653/writing-numpy-arrays-using-cv2-videowriter\n\n","slug":"OpenCV和numpy笔记","published":1,"updated":"2020-11-23T08:45:12.704Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1tvn0000p92rvjm9bpif","content":"<p>使用K-means做视频图像分割的作业的时候，遇到一个问题值得记录。</p>\n<p>该代码的功能是使用K-means做图像分割并保存图像与视频。</p>\n<p>遇到的问题是kmean之后得到的是每一个像素点的分类标签，需要将这些标签可视化成为分割(Segmentation)的结果视频。一开始没有直接找到相关的函数，搜索查询后给的建议都是使用<code>matlibplot</code>的<code>plt.imshow()</code>函数来实现，但是我不想要每次保存成为图片再转成视频，考虑到opencv-python的底层使用<code>numpy.array</code>来表述图像的。一个BGR图像相当于一个3维uint8数组（w,h,c)。要做的就是生成一个标签到像素的映射关系。这里我用的是<code>matplotlib.cm</code> color map 功能。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.linspace(0.0, 1.0, 10)</span><br><span class=\"line\">rgb = cm.get_cmap(plt.get_cmap(&apos;Set1&apos;))(x)[np.newaxis, :, :3][0]</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\">from numpy.matlib import repmat</span><br><span class=\"line\">from sklearn.preprocessing import normalize</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">from matplotlib import cm</span><br><span class=\"line\">from sklearn.cluster import KMeans</span><br><span class=\"line\">import cv2</span><br><span class=\"line\">from PIL import Image</span><br><span class=\"line\"></span><br><span class=\"line\">n_cl=5</span><br><span class=\"line\"></span><br><span class=\"line\">videoCapture = cv2.VideoCapture(&apos;road_video.mov&apos;)</span><br><span class=\"line\">fps = videoCapture.get(cv2.CAP_PROP_FPS)</span><br><span class=\"line\">size = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)),</span><br><span class=\"line\">        int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)))</span><br><span class=\"line\">videoWriter = cv2.VideoWriter(&apos;output.mp4&apos;, cv2.VideoWriter_fourcc(*&apos;mp4v&apos;), (fps/10), size)</span><br><span class=\"line\"></span><br><span class=\"line\">def kmeans(data, n_cl, verbose=True):</span><br><span class=\"line\">    n_samples, dim = data.shape</span><br><span class=\"line\">    centers = data[np.random.choice(range(n_samples), size=n_cl)]</span><br><span class=\"line\">    old_labels = np.zeros(shape=n_cl)</span><br><span class=\"line\">    while True:</span><br><span class=\"line\">        distances = np.zeros((n_samples, n_cl))</span><br><span class=\"line\">        for cluster_idx, cluster in enumerate(centers):</span><br><span class=\"line\">            distances[:, cluster_idx] = np.sum(np.square(data - repmat(cluster, n_samples, 1)), axis=1)</span><br><span class=\"line\">        new_labels = np.argmin(distances, axis=1)</span><br><span class=\"line\">        for l in range(0, n_cl):</span><br><span class=\"line\">            centers[l] = np.mean(data[new_labels==l], axis=0)</span><br><span class=\"line\">        if verbose:</span><br><span class=\"line\">            fig, ax = plt.subplots()</span><br><span class=\"line\">            ax.scatter(data[:, 0], data[:, 1], cluster=new_labels, s=40)</span><br><span class=\"line\">            ax.plot(centers[:, 0], centers[:, 1], &apos;r*&apos;, markersize=20)</span><br><span class=\"line\">            plt.waitforbuttonpress()</span><br><span class=\"line\">            plt.close()</span><br><span class=\"line\">        if np.array_equal(new_labels, old_labels):</span><br><span class=\"line\">            break</span><br><span class=\"line\">        old_labels = new_labels</span><br><span class=\"line\">    return new_labels</span><br><span class=\"line\"></span><br><span class=\"line\">count = 0</span><br><span class=\"line\">x = np.linspace(0.0, 1.0, 10)</span><br><span class=\"line\">rgb = cm.get_cmap(plt.get_cmap(&apos;Set1&apos;))(x)[np.newaxis, :, :3][0]</span><br><span class=\"line\"></span><br><span class=\"line\">while True:</span><br><span class=\"line\">    print(count)</span><br><span class=\"line\">    count += 1</span><br><span class=\"line\">#   load the frame</span><br><span class=\"line\">    success, frame = videoCapture.read()</span><br><span class=\"line\">    if not success:</span><br><span class=\"line\">        break</span><br><span class=\"line\">    img = np.float32(frame)</span><br><span class=\"line\">    h,w,c = img.shape</span><br><span class=\"line\">    # print(h,w,c)</span><br><span class=\"line\">#   add coordinates</span><br><span class=\"line\">    row_indexes = np.arange(0, h)</span><br><span class=\"line\">    col_indexes = np.arange(0, w)</span><br><span class=\"line\">    coordinates = np.zeros(shape=(h,w,2))</span><br><span class=\"line\">    coordinates[..., 0] = normalize(repmat(row_indexes, w, 1).T)</span><br><span class=\"line\">    coordinates[..., 1] = normalize(repmat(col_indexes, h, 1))</span><br><span class=\"line\">    data = np.concatenate((img, coordinates), axis=-1)</span><br><span class=\"line\">    data = np.reshape(data, newshape=(w *h, 5))</span><br><span class=\"line\">    labels = kmeans(data, n_cl=n_cl, verbose=False)</span><br><span class=\"line\"></span><br><span class=\"line\">    print(&apos;after&apos;)</span><br><span class=\"line\">    labels = labels.flatten()</span><br><span class=\"line\">    segmented_image = rgb[labels.flatten()]</span><br><span class=\"line\">    # print(segmented_image)</span><br><span class=\"line\">    segmented_image = segmented_image.reshape(img.shape)</span><br><span class=\"line\">    plt.imshow(segmented_image)</span><br><span class=\"line\">    # # plt.imshow(np.reshape(labels, (h, w)), cmap=&quot;hsv&quot;)</span><br><span class=\"line\">    plt.savefig(&quot;lab9/&quot;+str(count-1))</span><br><span class=\"line\">    segmented_image = segmented_image *255</span><br><span class=\"line\">    segmented_image = segmented_image.astype(&apos;uint8&apos;)</span><br><span class=\"line\">    videoWriter.write(segmented_image)</span><br><span class=\"line\"></span><br><span class=\"line\">videoWriter.release()</span><br><span class=\"line\">videoCapture.release()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"参考网页\"><a href=\"#参考网页\" class=\"headerlink\" title=\"参考网页\"></a>参考网页</h2><p><a href=\"https://blog.csdn.net/llh_1178/article/details/77833447\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/llh_1178/article/details/77833447</a></p>\n<p><a href=\"https://docs.opencv.org/3.4/d4/dba/classcv_1_1viz_1_1Color.html\" target=\"_blank\" rel=\"noopener\">https://docs.opencv.org/3.4/d4/dba/classcv_1_1viz_1_1Color.html</a></p>\n<p><a href=\"https://realpython.com/python-opencv-color-spaces/\" target=\"_blank\" rel=\"noopener\">https://realpython.com/python-opencv-color-spaces/</a></p>\n<p><a href=\"https://www.thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python\" target=\"_blank\" rel=\"noopener\">https://www.thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python</a></p>\n<p><a href=\"https://pvss.github.io/Opencv+Python.html\" target=\"_blank\" rel=\"noopener\">https://pvss.github.io/Opencv+Python.html</a></p>\n<p><a href=\"https://stackoverflow.com/questions/9280653/writing-numpy-arrays-using-cv2-videowriter\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/questions/9280653/writing-numpy-arrays-using-cv2-videowriter</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>使用K-means做视频图像分割的作业的时候，遇到一个问题值得记录。</p>\n<p>该代码的功能是使用K-means做图像分割并保存图像与视频。</p>\n<p>遇到的问题是kmean之后得到的是每一个像素点的分类标签，需要将这些标签可视化成为分割(Segmentation)的结果视频。一开始没有直接找到相关的函数，搜索查询后给的建议都是使用<code>matlibplot</code>的<code>plt.imshow()</code>函数来实现，但是我不想要每次保存成为图片再转成视频，考虑到opencv-python的底层使用<code>numpy.array</code>来表述图像的。一个BGR图像相当于一个3维uint8数组（w,h,c)。要做的就是生成一个标签到像素的映射关系。这里我用的是<code>matplotlib.cm</code> color map 功能。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.linspace(0.0, 1.0, 10)</span><br><span class=\"line\">rgb = cm.get_cmap(plt.get_cmap(&apos;Set1&apos;))(x)[np.newaxis, :, :3][0]</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\">from numpy.matlib import repmat</span><br><span class=\"line\">from sklearn.preprocessing import normalize</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">from matplotlib import cm</span><br><span class=\"line\">from sklearn.cluster import KMeans</span><br><span class=\"line\">import cv2</span><br><span class=\"line\">from PIL import Image</span><br><span class=\"line\"></span><br><span class=\"line\">n_cl=5</span><br><span class=\"line\"></span><br><span class=\"line\">videoCapture = cv2.VideoCapture(&apos;road_video.mov&apos;)</span><br><span class=\"line\">fps = videoCapture.get(cv2.CAP_PROP_FPS)</span><br><span class=\"line\">size = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)),</span><br><span class=\"line\">        int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)))</span><br><span class=\"line\">videoWriter = cv2.VideoWriter(&apos;output.mp4&apos;, cv2.VideoWriter_fourcc(*&apos;mp4v&apos;), (fps/10), size)</span><br><span class=\"line\"></span><br><span class=\"line\">def kmeans(data, n_cl, verbose=True):</span><br><span class=\"line\">    n_samples, dim = data.shape</span><br><span class=\"line\">    centers = data[np.random.choice(range(n_samples), size=n_cl)]</span><br><span class=\"line\">    old_labels = np.zeros(shape=n_cl)</span><br><span class=\"line\">    while True:</span><br><span class=\"line\">        distances = np.zeros((n_samples, n_cl))</span><br><span class=\"line\">        for cluster_idx, cluster in enumerate(centers):</span><br><span class=\"line\">            distances[:, cluster_idx] = np.sum(np.square(data - repmat(cluster, n_samples, 1)), axis=1)</span><br><span class=\"line\">        new_labels = np.argmin(distances, axis=1)</span><br><span class=\"line\">        for l in range(0, n_cl):</span><br><span class=\"line\">            centers[l] = np.mean(data[new_labels==l], axis=0)</span><br><span class=\"line\">        if verbose:</span><br><span class=\"line\">            fig, ax = plt.subplots()</span><br><span class=\"line\">            ax.scatter(data[:, 0], data[:, 1], cluster=new_labels, s=40)</span><br><span class=\"line\">            ax.plot(centers[:, 0], centers[:, 1], &apos;r*&apos;, markersize=20)</span><br><span class=\"line\">            plt.waitforbuttonpress()</span><br><span class=\"line\">            plt.close()</span><br><span class=\"line\">        if np.array_equal(new_labels, old_labels):</span><br><span class=\"line\">            break</span><br><span class=\"line\">        old_labels = new_labels</span><br><span class=\"line\">    return new_labels</span><br><span class=\"line\"></span><br><span class=\"line\">count = 0</span><br><span class=\"line\">x = np.linspace(0.0, 1.0, 10)</span><br><span class=\"line\">rgb = cm.get_cmap(plt.get_cmap(&apos;Set1&apos;))(x)[np.newaxis, :, :3][0]</span><br><span class=\"line\"></span><br><span class=\"line\">while True:</span><br><span class=\"line\">    print(count)</span><br><span class=\"line\">    count += 1</span><br><span class=\"line\">#   load the frame</span><br><span class=\"line\">    success, frame = videoCapture.read()</span><br><span class=\"line\">    if not success:</span><br><span class=\"line\">        break</span><br><span class=\"line\">    img = np.float32(frame)</span><br><span class=\"line\">    h,w,c = img.shape</span><br><span class=\"line\">    # print(h,w,c)</span><br><span class=\"line\">#   add coordinates</span><br><span class=\"line\">    row_indexes = np.arange(0, h)</span><br><span class=\"line\">    col_indexes = np.arange(0, w)</span><br><span class=\"line\">    coordinates = np.zeros(shape=(h,w,2))</span><br><span class=\"line\">    coordinates[..., 0] = normalize(repmat(row_indexes, w, 1).T)</span><br><span class=\"line\">    coordinates[..., 1] = normalize(repmat(col_indexes, h, 1))</span><br><span class=\"line\">    data = np.concatenate((img, coordinates), axis=-1)</span><br><span class=\"line\">    data = np.reshape(data, newshape=(w *h, 5))</span><br><span class=\"line\">    labels = kmeans(data, n_cl=n_cl, verbose=False)</span><br><span class=\"line\"></span><br><span class=\"line\">    print(&apos;after&apos;)</span><br><span class=\"line\">    labels = labels.flatten()</span><br><span class=\"line\">    segmented_image = rgb[labels.flatten()]</span><br><span class=\"line\">    # print(segmented_image)</span><br><span class=\"line\">    segmented_image = segmented_image.reshape(img.shape)</span><br><span class=\"line\">    plt.imshow(segmented_image)</span><br><span class=\"line\">    # # plt.imshow(np.reshape(labels, (h, w)), cmap=&quot;hsv&quot;)</span><br><span class=\"line\">    plt.savefig(&quot;lab9/&quot;+str(count-1))</span><br><span class=\"line\">    segmented_image = segmented_image *255</span><br><span class=\"line\">    segmented_image = segmented_image.astype(&apos;uint8&apos;)</span><br><span class=\"line\">    videoWriter.write(segmented_image)</span><br><span class=\"line\"></span><br><span class=\"line\">videoWriter.release()</span><br><span class=\"line\">videoCapture.release()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"参考网页\"><a href=\"#参考网页\" class=\"headerlink\" title=\"参考网页\"></a>参考网页</h2><p><a href=\"https://blog.csdn.net/llh_1178/article/details/77833447\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/llh_1178/article/details/77833447</a></p>\n<p><a href=\"https://docs.opencv.org/3.4/d4/dba/classcv_1_1viz_1_1Color.html\" target=\"_blank\" rel=\"noopener\">https://docs.opencv.org/3.4/d4/dba/classcv_1_1viz_1_1Color.html</a></p>\n<p><a href=\"https://realpython.com/python-opencv-color-spaces/\" target=\"_blank\" rel=\"noopener\">https://realpython.com/python-opencv-color-spaces/</a></p>\n<p><a href=\"https://www.thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python\" target=\"_blank\" rel=\"noopener\">https://www.thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python</a></p>\n<p><a href=\"https://pvss.github.io/Opencv+Python.html\" target=\"_blank\" rel=\"noopener\">https://pvss.github.io/Opencv+Python.html</a></p>\n<p><a href=\"https://stackoverflow.com/questions/9280653/writing-numpy-arrays-using-cv2-videowriter\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/questions/9280653/writing-numpy-arrays-using-cv2-videowriter</a></p>\n"},{"title":"Machine Learning Week1 Note","date":"2019-09-13T15:02:24.000Z","_content":"\n此系列博文是学习Coursera上Stanford的《Machine Learning》MOOC的笔记。\n\n课程共分为11周，大约需要56小时学习。\n\n此文是第一周的笔记。\n\n## 学习动机\n\n本课程是在同学的推荐下学习的，因为大四的两个项目都需要用到机器学习的知识和工具，所以需要先对机器学习有一个大概的了解。这门课程是很有名的。且Coursera上的评分为4.9，有250多万人注册过这门课！\n\nNg的课程说明中对本门课程的定义为不仅\n\n## 介绍\n\n机器学习的应用有：\n\n1. 垃圾邮件拦截。\n2. 网页搜索。\n3. 电子相册的面孔识别\n\n机器学习适用的场景：代码不能手动编写出来。比如：\n\n1. 直升机自动驾驶\n2. 自然语言识别\n3. 视觉识别\n\n机器学习在数据挖掘中的应用。\n\n推荐系统，点击流。\n\n## 什么是机器学习？\n\n公认的机器学习的定义有两种说法。一种是Mitchell提出的ETP定义，一种是\n\nArthur Samuel提出的学科定义。\n\n### ETP（Experience, Task, Performance）\n\n> A computer is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n\nExperience，实验经历。\n\nTask：任务。\n\nPerformance：预测的准确度。\n\n### Samuel的学科定义\n\n> Machine Learning is the field of study gives computers that ability to learn without being explicitly programmed.\n\n机器学习是研究赋予不用显性的编写代码就能使计算机获得学习的能力的学科。\n\n## 监督学习\n\n监督学习是指输入训练特征数据，并且告诉你正确的答案是什么，然后再输入未知的特征，通过对之前的数据挖掘出规律预测未知输入对应的输出。\n\n监督学习需要使用标记好的特征数据训练。\n\n监督学习可分为两种形式。一种是分类（Classification），分类的结果是离散有限的。另外一种是回归（regression），回归的输出结果是连续的。以卖房为例，分类的结果可以是预测指定房子在未来三个月是否能卖出去。而回归的结果可以是符合指定条件（比如面积和地段）的房子的预期价格。价格既可以是10000，也可以是10001.23。\n\n在上面的例子中，面积和地段是学习模型的两个特征，除了它们以外，可能的特征还有小区绿化率、1公里内的地铁站的数量等等特征。如果需要，甚至可以使用近乎无穷数量的特征进行监督学习。\n\n## 非监督学习\n\n非监督学习不需要知道输入数据的标签和分类，而是通过输入数据的结构聚类（Clustering）。比如google新闻推荐，自动以一个新闻事件为主题，聚集相关的新闻报道。或者社交网络中人群分类都可以使用非监督学习进行。\n\n课程中举了一个Cokcktail Party Problem的问题。即通过两个不同位置的声源分离出环境音和人声。\n\n**TODO：实现一个算法**\n\n","source":"_posts/Machine-Learning-Week1-Note.md","raw":"---\ntitle: Machine Learning Week1 Note\ndate: 2019-09-13 23:02:24\ntags: \n- MachineLearning\n- Coursera\n---\n\n此系列博文是学习Coursera上Stanford的《Machine Learning》MOOC的笔记。\n\n课程共分为11周，大约需要56小时学习。\n\n此文是第一周的笔记。\n\n## 学习动机\n\n本课程是在同学的推荐下学习的，因为大四的两个项目都需要用到机器学习的知识和工具，所以需要先对机器学习有一个大概的了解。这门课程是很有名的。且Coursera上的评分为4.9，有250多万人注册过这门课！\n\nNg的课程说明中对本门课程的定义为不仅\n\n## 介绍\n\n机器学习的应用有：\n\n1. 垃圾邮件拦截。\n2. 网页搜索。\n3. 电子相册的面孔识别\n\n机器学习适用的场景：代码不能手动编写出来。比如：\n\n1. 直升机自动驾驶\n2. 自然语言识别\n3. 视觉识别\n\n机器学习在数据挖掘中的应用。\n\n推荐系统，点击流。\n\n## 什么是机器学习？\n\n公认的机器学习的定义有两种说法。一种是Mitchell提出的ETP定义，一种是\n\nArthur Samuel提出的学科定义。\n\n### ETP（Experience, Task, Performance）\n\n> A computer is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n\nExperience，实验经历。\n\nTask：任务。\n\nPerformance：预测的准确度。\n\n### Samuel的学科定义\n\n> Machine Learning is the field of study gives computers that ability to learn without being explicitly programmed.\n\n机器学习是研究赋予不用显性的编写代码就能使计算机获得学习的能力的学科。\n\n## 监督学习\n\n监督学习是指输入训练特征数据，并且告诉你正确的答案是什么，然后再输入未知的特征，通过对之前的数据挖掘出规律预测未知输入对应的输出。\n\n监督学习需要使用标记好的特征数据训练。\n\n监督学习可分为两种形式。一种是分类（Classification），分类的结果是离散有限的。另外一种是回归（regression），回归的输出结果是连续的。以卖房为例，分类的结果可以是预测指定房子在未来三个月是否能卖出去。而回归的结果可以是符合指定条件（比如面积和地段）的房子的预期价格。价格既可以是10000，也可以是10001.23。\n\n在上面的例子中，面积和地段是学习模型的两个特征，除了它们以外，可能的特征还有小区绿化率、1公里内的地铁站的数量等等特征。如果需要，甚至可以使用近乎无穷数量的特征进行监督学习。\n\n## 非监督学习\n\n非监督学习不需要知道输入数据的标签和分类，而是通过输入数据的结构聚类（Clustering）。比如google新闻推荐，自动以一个新闻事件为主题，聚集相关的新闻报道。或者社交网络中人群分类都可以使用非监督学习进行。\n\n课程中举了一个Cokcktail Party Problem的问题。即通过两个不同位置的声源分离出环境音和人声。\n\n**TODO：实现一个算法**\n\n","slug":"Machine-Learning-Week1-Note","published":1,"updated":"2020-11-21T07:26:16.625Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1tvt0002p92rlrjmeo7l","content":"<p>此系列博文是学习Coursera上Stanford的《Machine Learning》MOOC的笔记。</p>\n<p>课程共分为11周，大约需要56小时学习。</p>\n<p>此文是第一周的笔记。</p>\n<h2 id=\"学习动机\"><a href=\"#学习动机\" class=\"headerlink\" title=\"学习动机\"></a>学习动机</h2><p>本课程是在同学的推荐下学习的，因为大四的两个项目都需要用到机器学习的知识和工具，所以需要先对机器学习有一个大概的了解。这门课程是很有名的。且Coursera上的评分为4.9，有250多万人注册过这门课！</p>\n<p>Ng的课程说明中对本门课程的定义为不仅</p>\n<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>机器学习的应用有：</p>\n<ol>\n<li>垃圾邮件拦截。</li>\n<li>网页搜索。</li>\n<li>电子相册的面孔识别</li>\n</ol>\n<p>机器学习适用的场景：代码不能手动编写出来。比如：</p>\n<ol>\n<li>直升机自动驾驶</li>\n<li>自然语言识别</li>\n<li>视觉识别</li>\n</ol>\n<p>机器学习在数据挖掘中的应用。</p>\n<p>推荐系统，点击流。</p>\n<h2 id=\"什么是机器学习？\"><a href=\"#什么是机器学习？\" class=\"headerlink\" title=\"什么是机器学习？\"></a>什么是机器学习？</h2><p>公认的机器学习的定义有两种说法。一种是Mitchell提出的ETP定义，一种是</p>\n<p>Arthur Samuel提出的学科定义。</p>\n<h3 id=\"ETP（Experience-Task-Performance）\"><a href=\"#ETP（Experience-Task-Performance）\" class=\"headerlink\" title=\"ETP（Experience, Task, Performance）\"></a>ETP（Experience, Task, Performance）</h3><blockquote>\n<p>A computer is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>\n</blockquote>\n<p>Experience，实验经历。</p>\n<p>Task：任务。</p>\n<p>Performance：预测的准确度。</p>\n<h3 id=\"Samuel的学科定义\"><a href=\"#Samuel的学科定义\" class=\"headerlink\" title=\"Samuel的学科定义\"></a>Samuel的学科定义</h3><blockquote>\n<p>Machine Learning is the field of study gives computers that ability to learn without being explicitly programmed.</p>\n</blockquote>\n<p>机器学习是研究赋予不用显性的编写代码就能使计算机获得学习的能力的学科。</p>\n<h2 id=\"监督学习\"><a href=\"#监督学习\" class=\"headerlink\" title=\"监督学习\"></a>监督学习</h2><p>监督学习是指输入训练特征数据，并且告诉你正确的答案是什么，然后再输入未知的特征，通过对之前的数据挖掘出规律预测未知输入对应的输出。</p>\n<p>监督学习需要使用标记好的特征数据训练。</p>\n<p>监督学习可分为两种形式。一种是分类（Classification），分类的结果是离散有限的。另外一种是回归（regression），回归的输出结果是连续的。以卖房为例，分类的结果可以是预测指定房子在未来三个月是否能卖出去。而回归的结果可以是符合指定条件（比如面积和地段）的房子的预期价格。价格既可以是10000，也可以是10001.23。</p>\n<p>在上面的例子中，面积和地段是学习模型的两个特征，除了它们以外，可能的特征还有小区绿化率、1公里内的地铁站的数量等等特征。如果需要，甚至可以使用近乎无穷数量的特征进行监督学习。</p>\n<h2 id=\"非监督学习\"><a href=\"#非监督学习\" class=\"headerlink\" title=\"非监督学习\"></a>非监督学习</h2><p>非监督学习不需要知道输入数据的标签和分类，而是通过输入数据的结构聚类（Clustering）。比如google新闻推荐，自动以一个新闻事件为主题，聚集相关的新闻报道。或者社交网络中人群分类都可以使用非监督学习进行。</p>\n<p>课程中举了一个Cokcktail Party Problem的问题。即通过两个不同位置的声源分离出环境音和人声。</p>\n<p><strong>TODO：实现一个算法</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<p>此系列博文是学习Coursera上Stanford的《Machine Learning》MOOC的笔记。</p>\n<p>课程共分为11周，大约需要56小时学习。</p>\n<p>此文是第一周的笔记。</p>\n<h2 id=\"学习动机\"><a href=\"#学习动机\" class=\"headerlink\" title=\"学习动机\"></a>学习动机</h2><p>本课程是在同学的推荐下学习的，因为大四的两个项目都需要用到机器学习的知识和工具，所以需要先对机器学习有一个大概的了解。这门课程是很有名的。且Coursera上的评分为4.9，有250多万人注册过这门课！</p>\n<p>Ng的课程说明中对本门课程的定义为不仅</p>\n<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>机器学习的应用有：</p>\n<ol>\n<li>垃圾邮件拦截。</li>\n<li>网页搜索。</li>\n<li>电子相册的面孔识别</li>\n</ol>\n<p>机器学习适用的场景：代码不能手动编写出来。比如：</p>\n<ol>\n<li>直升机自动驾驶</li>\n<li>自然语言识别</li>\n<li>视觉识别</li>\n</ol>\n<p>机器学习在数据挖掘中的应用。</p>\n<p>推荐系统，点击流。</p>\n<h2 id=\"什么是机器学习？\"><a href=\"#什么是机器学习？\" class=\"headerlink\" title=\"什么是机器学习？\"></a>什么是机器学习？</h2><p>公认的机器学习的定义有两种说法。一种是Mitchell提出的ETP定义，一种是</p>\n<p>Arthur Samuel提出的学科定义。</p>\n<h3 id=\"ETP（Experience-Task-Performance）\"><a href=\"#ETP（Experience-Task-Performance）\" class=\"headerlink\" title=\"ETP（Experience, Task, Performance）\"></a>ETP（Experience, Task, Performance）</h3><blockquote>\n<p>A computer is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>\n</blockquote>\n<p>Experience，实验经历。</p>\n<p>Task：任务。</p>\n<p>Performance：预测的准确度。</p>\n<h3 id=\"Samuel的学科定义\"><a href=\"#Samuel的学科定义\" class=\"headerlink\" title=\"Samuel的学科定义\"></a>Samuel的学科定义</h3><blockquote>\n<p>Machine Learning is the field of study gives computers that ability to learn without being explicitly programmed.</p>\n</blockquote>\n<p>机器学习是研究赋予不用显性的编写代码就能使计算机获得学习的能力的学科。</p>\n<h2 id=\"监督学习\"><a href=\"#监督学习\" class=\"headerlink\" title=\"监督学习\"></a>监督学习</h2><p>监督学习是指输入训练特征数据，并且告诉你正确的答案是什么，然后再输入未知的特征，通过对之前的数据挖掘出规律预测未知输入对应的输出。</p>\n<p>监督学习需要使用标记好的特征数据训练。</p>\n<p>监督学习可分为两种形式。一种是分类（Classification），分类的结果是离散有限的。另外一种是回归（regression），回归的输出结果是连续的。以卖房为例，分类的结果可以是预测指定房子在未来三个月是否能卖出去。而回归的结果可以是符合指定条件（比如面积和地段）的房子的预期价格。价格既可以是10000，也可以是10001.23。</p>\n<p>在上面的例子中，面积和地段是学习模型的两个特征，除了它们以外，可能的特征还有小区绿化率、1公里内的地铁站的数量等等特征。如果需要，甚至可以使用近乎无穷数量的特征进行监督学习。</p>\n<h2 id=\"非监督学习\"><a href=\"#非监督学习\" class=\"headerlink\" title=\"非监督学习\"></a>非监督学习</h2><p>非监督学习不需要知道输入数据的标签和分类，而是通过输入数据的结构聚类（Clustering）。比如google新闻推荐，自动以一个新闻事件为主题，聚集相关的新闻报道。或者社交网络中人群分类都可以使用非监督学习进行。</p>\n<p>课程中举了一个Cokcktail Party Problem的问题。即通过两个不同位置的声源分离出环境音和人声。</p>\n<p><strong>TODO：实现一个算法</strong></p>\n"},{"title":"如何测试那些难以测试的方法？(stdin/stdout标准输入输出，文件，main方法，private私有方法)","date":"2019-09-23T07:23:33.000Z","_content":"\n\n\n\n\n## 如何测试标准输出中的内容\n\n标准输出：`System.out.println()`\n\n方法流程：\n1. 使用`OutputStream, System.setOut`重定向输出流\n2. 使用`System.getProperty(\"line.separator)`来正确的测试下一行\n3. 使用`System.setOut, System.out`恢复输出流\n---\n\n### 样例代码\n\n```java\n// PrintClass.java\npublic class PrintClass {\n\tpublic void printSome() {\n\t\tSystem.out.println(\"Just follow your heard\");\n\t}\n}\n// PrintClassTest.java,忽略掉import的部分\npublic class PrintClassTest {\n\t@Test\n\tpublic void testPrintSome() {\n\t\t// 重定向标准输出到指定printstream中\n\t\tOutputStream os = new ByteArrayOutputStream();\n\t\tPrintStream ps = new PrintStream(os);\n\t\tSystem.setOut(ps);\n\t\t// 执行测试代码\n\t\tPrintClass pc = new PrintClass();\n\t\tpc.printSome();\n\t\tassertEquals(\"Just follow your heard\"\n\t\t\t\t\t\t+ System.getProperty(\"line.separator\"),\n                os.toString());\n\t\t// 恢复重定向\n\t\tPrintStream originalOut = System.out;\n    \tSystem.setOut(originalOut);\n\t}\n}\n\n```\n\n完整代码在：[week2:printtest](https://github.com/chenminken/software-testing/tree/master/week2/printtest)\n\n## 如何测试私有方法\n\nJava中的私有方法不能通过obj.method()的方式来调用。如果需要测试私有方法怎么办？\n\n使用反射机制获取这个私有方法。\n\n需要使用到的方法`Class.getDeclaredMethod(), Method.setAccessible(), Method.invoke()`\n\n方法流程：\n\n1. 获取需要测试类的class对象（可以使用class.forname的方法，也可以直接使用[ClassName].class的方法）\n2. 获取该class对象指定的私有方法的Method对象（`[classobj.class].getDeclaredMethod(String name, Class<?>... parameterTypes)`）\n3. 修改私有方法的访问性为公开访问(`[MethodObj].setAccessible(true)`。\n4. 实例化需要测试类的object对象(`[ClassName.class].newInstance()`)\n5. 调用该私有方法测试(`[MethodObj].invoke([objectInstance], ...)`)\n\n---\n\n### 样例代码\n\n```java\n// PrivateMethodClass.java\npublic class PrivateMethodClass {\n\tprivate int privatePlus(int a, int b) {\n\t\treturn a + b;\n\t}\n}\n// PrivateMethodClassTest.java\npublic class PrivateMethodClassTest {\n    @Test\n    public void testPrivatePlus1() throws NoSuchMethodException, ClassNotFoundException, IllegalAccessException, InstantiationException, InvocationTargetException {\n        Class cls = Class.forName(\"PrivateMethodClass\");\n        Method msd = cls.getDeclaredMethod(\"privatePlus\", int.class, int.class);\n        msd.setAccessible(true);\n        Object obj = cls.newInstance();\n        int res = (Integer)msd.invoke(obj, 4,6);\n        assertEquals(10, res);\n    }\n}\n```\n\n完整代码在：[week2:privateTest](https://github.com/chenminken/software-testing/tree/master/week2/privateTest)","source":"_posts/如何测试那些难以测试的方法.md","raw":"---\ntitle: 如何测试那些难以测试的方法？(stdin/stdout标准输入输出，文件，main方法，private私有方法)\ndate: 2019-09-23 15:23:33\ntags: \n- 软件测试\n- Java\n---\n\n\n\n\n\n## 如何测试标准输出中的内容\n\n标准输出：`System.out.println()`\n\n方法流程：\n1. 使用`OutputStream, System.setOut`重定向输出流\n2. 使用`System.getProperty(\"line.separator)`来正确的测试下一行\n3. 使用`System.setOut, System.out`恢复输出流\n---\n\n### 样例代码\n\n```java\n// PrintClass.java\npublic class PrintClass {\n\tpublic void printSome() {\n\t\tSystem.out.println(\"Just follow your heard\");\n\t}\n}\n// PrintClassTest.java,忽略掉import的部分\npublic class PrintClassTest {\n\t@Test\n\tpublic void testPrintSome() {\n\t\t// 重定向标准输出到指定printstream中\n\t\tOutputStream os = new ByteArrayOutputStream();\n\t\tPrintStream ps = new PrintStream(os);\n\t\tSystem.setOut(ps);\n\t\t// 执行测试代码\n\t\tPrintClass pc = new PrintClass();\n\t\tpc.printSome();\n\t\tassertEquals(\"Just follow your heard\"\n\t\t\t\t\t\t+ System.getProperty(\"line.separator\"),\n                os.toString());\n\t\t// 恢复重定向\n\t\tPrintStream originalOut = System.out;\n    \tSystem.setOut(originalOut);\n\t}\n}\n\n```\n\n完整代码在：[week2:printtest](https://github.com/chenminken/software-testing/tree/master/week2/printtest)\n\n## 如何测试私有方法\n\nJava中的私有方法不能通过obj.method()的方式来调用。如果需要测试私有方法怎么办？\n\n使用反射机制获取这个私有方法。\n\n需要使用到的方法`Class.getDeclaredMethod(), Method.setAccessible(), Method.invoke()`\n\n方法流程：\n\n1. 获取需要测试类的class对象（可以使用class.forname的方法，也可以直接使用[ClassName].class的方法）\n2. 获取该class对象指定的私有方法的Method对象（`[classobj.class].getDeclaredMethod(String name, Class<?>... parameterTypes)`）\n3. 修改私有方法的访问性为公开访问(`[MethodObj].setAccessible(true)`。\n4. 实例化需要测试类的object对象(`[ClassName.class].newInstance()`)\n5. 调用该私有方法测试(`[MethodObj].invoke([objectInstance], ...)`)\n\n---\n\n### 样例代码\n\n```java\n// PrivateMethodClass.java\npublic class PrivateMethodClass {\n\tprivate int privatePlus(int a, int b) {\n\t\treturn a + b;\n\t}\n}\n// PrivateMethodClassTest.java\npublic class PrivateMethodClassTest {\n    @Test\n    public void testPrivatePlus1() throws NoSuchMethodException, ClassNotFoundException, IllegalAccessException, InstantiationException, InvocationTargetException {\n        Class cls = Class.forName(\"PrivateMethodClass\");\n        Method msd = cls.getDeclaredMethod(\"privatePlus\", int.class, int.class);\n        msd.setAccessible(true);\n        Object obj = cls.newInstance();\n        int res = (Integer)msd.invoke(obj, 4,6);\n        assertEquals(10, res);\n    }\n}\n```\n\n完整代码在：[week2:privateTest](https://github.com/chenminken/software-testing/tree/master/week2/privateTest)","slug":"如何测试那些难以测试的方法","published":1,"updated":"2020-11-21T07:26:16.626Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1tvx0004p92r9uvmxsar","content":"<h2 id=\"如何测试标准输出中的内容\"><a href=\"#如何测试标准输出中的内容\" class=\"headerlink\" title=\"如何测试标准输出中的内容\"></a>如何测试标准输出中的内容</h2><p>标准输出：<code>System.out.println()</code></p>\n<p>方法流程：</p>\n<ol>\n<li>使用<code>OutputStream, System.setOut</code>重定向输出流</li>\n<li>使用<code>System.getProperty(&quot;line.separator)</code>来正确的测试下一行</li>\n<li>使用<code>System.setOut, System.out</code>恢复输出流</li>\n</ol>\n<hr>\n<h3 id=\"样例代码\"><a href=\"#样例代码\" class=\"headerlink\" title=\"样例代码\"></a>样例代码</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// PrintClass.java</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PrintClass</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">printSome</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"Just follow your heard\"</span>);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// PrintClassTest.java,忽略掉import的部分</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PrintClassTest</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"meta\">@Test</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">testPrintSome</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 重定向标准输出到指定printstream中</span></span><br><span class=\"line\">\t\tOutputStream os = <span class=\"keyword\">new</span> ByteArrayOutputStream();</span><br><span class=\"line\">\t\tPrintStream ps = <span class=\"keyword\">new</span> PrintStream(os);</span><br><span class=\"line\">\t\tSystem.setOut(ps);</span><br><span class=\"line\">\t\t<span class=\"comment\">// 执行测试代码</span></span><br><span class=\"line\">\t\tPrintClass pc = <span class=\"keyword\">new</span> PrintClass();</span><br><span class=\"line\">\t\tpc.printSome();</span><br><span class=\"line\">\t\tassertEquals(<span class=\"string\">\"Just follow your heard\"</span></span><br><span class=\"line\">\t\t\t\t\t\t+ System.getProperty(<span class=\"string\">\"line.separator\"</span>),</span><br><span class=\"line\">                os.toString());</span><br><span class=\"line\">\t\t<span class=\"comment\">// 恢复重定向</span></span><br><span class=\"line\">\t\tPrintStream originalOut = System.out;</span><br><span class=\"line\">    \tSystem.setOut(originalOut);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>完整代码在：<a href=\"https://github.com/chenminken/software-testing/tree/master/week2/printtest\" target=\"_blank\" rel=\"noopener\">week2:printtest</a></p>\n<h2 id=\"如何测试私有方法\"><a href=\"#如何测试私有方法\" class=\"headerlink\" title=\"如何测试私有方法\"></a>如何测试私有方法</h2><p>Java中的私有方法不能通过obj.method()的方式来调用。如果需要测试私有方法怎么办？</p>\n<p>使用反射机制获取这个私有方法。</p>\n<p>需要使用到的方法<code>Class.getDeclaredMethod(), Method.setAccessible(), Method.invoke()</code></p>\n<p>方法流程：</p>\n<ol>\n<li>获取需要测试类的class对象（可以使用class.forname的方法，也可以直接使用[ClassName].class的方法）</li>\n<li>获取该class对象指定的私有方法的Method对象（<code>[classobj.class].getDeclaredMethod(String name, Class&lt;?&gt;... parameterTypes)</code>）</li>\n<li>修改私有方法的访问性为公开访问(<code>[MethodObj].setAccessible(true)</code>。</li>\n<li>实例化需要测试类的object对象(<code>[ClassName.class].newInstance()</code>)</li>\n<li>调用该私有方法测试(<code>[MethodObj].invoke([objectInstance], ...)</code>)</li>\n</ol>\n<hr>\n<h3 id=\"样例代码-1\"><a href=\"#样例代码-1\" class=\"headerlink\" title=\"样例代码\"></a>样例代码</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// PrivateMethodClass.java</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PrivateMethodClass</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">int</span> <span class=\"title\">privatePlus</span><span class=\"params\">(<span class=\"keyword\">int</span> a, <span class=\"keyword\">int</span> b)</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> a + b;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// PrivateMethodClassTest.java</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PrivateMethodClassTest</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">testPrivatePlus1</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> NoSuchMethodException, ClassNotFoundException, IllegalAccessException, InstantiationException, InvocationTargetException </span>&#123;</span><br><span class=\"line\">        Class cls = Class.forName(<span class=\"string\">\"PrivateMethodClass\"</span>);</span><br><span class=\"line\">        Method msd = cls.getDeclaredMethod(<span class=\"string\">\"privatePlus\"</span>, <span class=\"keyword\">int</span>.class, <span class=\"keyword\">int</span>.class);</span><br><span class=\"line\">        msd.setAccessible(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">        Object obj = cls.newInstance();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> res = (Integer)msd.invoke(obj, <span class=\"number\">4</span>,<span class=\"number\">6</span>);</span><br><span class=\"line\">        assertEquals(<span class=\"number\">10</span>, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>完整代码在：<a href=\"https://github.com/chenminken/software-testing/tree/master/week2/privateTest\" target=\"_blank\" rel=\"noopener\">week2:privateTest</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"如何测试标准输出中的内容\"><a href=\"#如何测试标准输出中的内容\" class=\"headerlink\" title=\"如何测试标准输出中的内容\"></a>如何测试标准输出中的内容</h2><p>标准输出：<code>System.out.println()</code></p>\n<p>方法流程：</p>\n<ol>\n<li>使用<code>OutputStream, System.setOut</code>重定向输出流</li>\n<li>使用<code>System.getProperty(&quot;line.separator)</code>来正确的测试下一行</li>\n<li>使用<code>System.setOut, System.out</code>恢复输出流</li>\n</ol>\n<hr>\n<h3 id=\"样例代码\"><a href=\"#样例代码\" class=\"headerlink\" title=\"样例代码\"></a>样例代码</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// PrintClass.java</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PrintClass</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">printSome</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"Just follow your heard\"</span>);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// PrintClassTest.java,忽略掉import的部分</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PrintClassTest</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"meta\">@Test</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">testPrintSome</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 重定向标准输出到指定printstream中</span></span><br><span class=\"line\">\t\tOutputStream os = <span class=\"keyword\">new</span> ByteArrayOutputStream();</span><br><span class=\"line\">\t\tPrintStream ps = <span class=\"keyword\">new</span> PrintStream(os);</span><br><span class=\"line\">\t\tSystem.setOut(ps);</span><br><span class=\"line\">\t\t<span class=\"comment\">// 执行测试代码</span></span><br><span class=\"line\">\t\tPrintClass pc = <span class=\"keyword\">new</span> PrintClass();</span><br><span class=\"line\">\t\tpc.printSome();</span><br><span class=\"line\">\t\tassertEquals(<span class=\"string\">\"Just follow your heard\"</span></span><br><span class=\"line\">\t\t\t\t\t\t+ System.getProperty(<span class=\"string\">\"line.separator\"</span>),</span><br><span class=\"line\">                os.toString());</span><br><span class=\"line\">\t\t<span class=\"comment\">// 恢复重定向</span></span><br><span class=\"line\">\t\tPrintStream originalOut = System.out;</span><br><span class=\"line\">    \tSystem.setOut(originalOut);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>完整代码在：<a href=\"https://github.com/chenminken/software-testing/tree/master/week2/printtest\" target=\"_blank\" rel=\"noopener\">week2:printtest</a></p>\n<h2 id=\"如何测试私有方法\"><a href=\"#如何测试私有方法\" class=\"headerlink\" title=\"如何测试私有方法\"></a>如何测试私有方法</h2><p>Java中的私有方法不能通过obj.method()的方式来调用。如果需要测试私有方法怎么办？</p>\n<p>使用反射机制获取这个私有方法。</p>\n<p>需要使用到的方法<code>Class.getDeclaredMethod(), Method.setAccessible(), Method.invoke()</code></p>\n<p>方法流程：</p>\n<ol>\n<li>获取需要测试类的class对象（可以使用class.forname的方法，也可以直接使用[ClassName].class的方法）</li>\n<li>获取该class对象指定的私有方法的Method对象（<code>[classobj.class].getDeclaredMethod(String name, Class&lt;?&gt;... parameterTypes)</code>）</li>\n<li>修改私有方法的访问性为公开访问(<code>[MethodObj].setAccessible(true)</code>。</li>\n<li>实例化需要测试类的object对象(<code>[ClassName.class].newInstance()</code>)</li>\n<li>调用该私有方法测试(<code>[MethodObj].invoke([objectInstance], ...)</code>)</li>\n</ol>\n<hr>\n<h3 id=\"样例代码-1\"><a href=\"#样例代码-1\" class=\"headerlink\" title=\"样例代码\"></a>样例代码</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// PrivateMethodClass.java</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PrivateMethodClass</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">int</span> <span class=\"title\">privatePlus</span><span class=\"params\">(<span class=\"keyword\">int</span> a, <span class=\"keyword\">int</span> b)</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> a + b;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// PrivateMethodClassTest.java</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PrivateMethodClassTest</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">testPrivatePlus1</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> NoSuchMethodException, ClassNotFoundException, IllegalAccessException, InstantiationException, InvocationTargetException </span>&#123;</span><br><span class=\"line\">        Class cls = Class.forName(<span class=\"string\">\"PrivateMethodClass\"</span>);</span><br><span class=\"line\">        Method msd = cls.getDeclaredMethod(<span class=\"string\">\"privatePlus\"</span>, <span class=\"keyword\">int</span>.class, <span class=\"keyword\">int</span>.class);</span><br><span class=\"line\">        msd.setAccessible(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">        Object obj = cls.newInstance();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> res = (Integer)msd.invoke(obj, <span class=\"number\">4</span>,<span class=\"number\">6</span>);</span><br><span class=\"line\">        assertEquals(<span class=\"number\">10</span>, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>完整代码在：<a href=\"https://github.com/chenminken/software-testing/tree/master/week2/privateTest\" target=\"_blank\" rel=\"noopener\">week2:privateTest</a></p>\n"},{"title":"python程序同步webdav网盘（坚果云、owncloud)","date":"2020-01-10T03:58:06.000Z","_content":"\n在编写一个Django项目的时候有定时备份数据文件的需求。因为项目运行在云服务器中，担心的是服务器挂掉，所以备份的地方不能是同一台服务器。在这个项目里自己去建一个服务器来管理备份数据显得没有必要。目前网络存储提供商有许多家，我选取的方案是编写python定时程序连接支持WebDAV协议的网盘，例如坚果云和owncloud。\n\n分为两部分：\n\n1. Python同步代码编写\n2. Django定时任务编写\n\n## Python同步代码编写\n\n使用[webdavclient3](https://pypi.org/project/webdavclient3/)库来处理webDAV协议的部分。先安装：\n\n```\npip install webdavclient3\n```\n\n然后在自己的项目的某个地方建立一个py文件。我选的是`[项目目录]\\utils\\backup\\backup.py`\n\n编写python代码：\n\n```python\nfrom webdav3.client import Client\nfrom datetime import datetime\nfrom webdav3.exceptions import LocalResourceNotFound\n\nimport math\n# invoke this function every day.\ndef upload():\n    options = {\n        'webdav_hostname': \"网盘地址，如果是坚果云，不能只输入/dav/路径，似乎这个文件夹不能访问，在下面再建一个文件夹，比如backup。网址中需要有backup的地址比如https://dav.jianguoyun.com/dav/backup\",\n        'webdav_login': \"用户名\",\n        'webdav_password': \"密码，如果是坚果云填写应用密码\",\n        'disable_check': True, #有的网盘不支持check功能\n    }\n    client = Client(options)\n\t\t# 我选择用时间戳为备份文件命名\n    file_name = str(math.floor(datetime.now().timestamp())) + '.bak'\n    try:\n        # 写死的路径，第一个参数是网盘地址\n        client.upload('backup/' + file_name, '本地地址，绝对路径')\n        # 打印结果，之后会重定向到log\n        print('upload at ' + file_name)\n    except LocalResourceNotFound as exception:\n        print('An error happen: LocalResourceNotFound ---'  + file_name)\n\n# 如果是直接调用文件，执行upload()\nif __name__ == '__main__':\n    print('run upload')\n    upload()\n```\n\nPython的代码相对简短。只需要在服务器的命令行执行`python upload.py`，备份文件自动上传到网盘。然而还不完整，这个程序的目的是定期执行。接下来结合Django的定期执行接口做到每日备份。\n\n\n\n## Django定时任务编写\n\nDjango是目前很流行的python web服务框架。通过django自带的命令创建project和app（这一部分不讲）。\n\n当项目建好后，使用`python manage.py`可以运行项目、数据库代码整合。开发者可以通过继承BaseCommand类来自定义命令，然后再通过django_crontab定期执行命令。或者不通过自定义命令，直接使用django_crontab定期执行函数。\n\n### 创建backupCmd命令\n\n在之前创建的app下新建management/commands目录，在该目录下新建`backupCmd.py`\n\n```\nfrom django.core.management.base import BaseCommand\nfrom utils.backup.backup import upload\n\nclass Command(BaseCommand):\n    def handle(self, *args, **options):\n        upload()\n```\n\n当完成后，在项目根目录下执行`python manage.py backupCmd`就可以单次执行程序了。\n\n在setting.py尾部添加:\n\n```\n# 运行定时函数，每天1点运行。\nCRONJOBS = [\n    ('0 01 * * *', 'utils.backup.backup','>> ~/test_crontab.log')\n]\n```\n或\n```\n# 运行定时命令， \nCRONJOBS = [\n    ('*/1 * * * *', 'django.core.management.call_command', ['backupCmd'], {}, '>> ~/test_crontab.log'),\n]\n```\n\n然后执行`python manage.py crontab add`，定时任务加入其中。\n\n当时间到达的时候，程序将自动运行。日志会输出到`~/test_crontab.log`中。\n\nlinux中的定时任务crontab的语法如下:\n\n```\n*  *  *  *  * command\n分钟(0-59) 小时(0-23) 每个月的哪一天(1-31) 月份(1-12) 周几(0-6) shell脚本或者命令\n```\n\n\n\n对于使用到的网盘，希望能够付费支持一下。","source":"_posts/python程序同步webdav网盘（坚果云、owncloud）.md","raw":"---\ntitle: python程序同步webdav网盘（坚果云、owncloud)\ndate: 2020-01-10 11:58:06\ntags:\n---\n\n在编写一个Django项目的时候有定时备份数据文件的需求。因为项目运行在云服务器中，担心的是服务器挂掉，所以备份的地方不能是同一台服务器。在这个项目里自己去建一个服务器来管理备份数据显得没有必要。目前网络存储提供商有许多家，我选取的方案是编写python定时程序连接支持WebDAV协议的网盘，例如坚果云和owncloud。\n\n分为两部分：\n\n1. Python同步代码编写\n2. Django定时任务编写\n\n## Python同步代码编写\n\n使用[webdavclient3](https://pypi.org/project/webdavclient3/)库来处理webDAV协议的部分。先安装：\n\n```\npip install webdavclient3\n```\n\n然后在自己的项目的某个地方建立一个py文件。我选的是`[项目目录]\\utils\\backup\\backup.py`\n\n编写python代码：\n\n```python\nfrom webdav3.client import Client\nfrom datetime import datetime\nfrom webdav3.exceptions import LocalResourceNotFound\n\nimport math\n# invoke this function every day.\ndef upload():\n    options = {\n        'webdav_hostname': \"网盘地址，如果是坚果云，不能只输入/dav/路径，似乎这个文件夹不能访问，在下面再建一个文件夹，比如backup。网址中需要有backup的地址比如https://dav.jianguoyun.com/dav/backup\",\n        'webdav_login': \"用户名\",\n        'webdav_password': \"密码，如果是坚果云填写应用密码\",\n        'disable_check': True, #有的网盘不支持check功能\n    }\n    client = Client(options)\n\t\t# 我选择用时间戳为备份文件命名\n    file_name = str(math.floor(datetime.now().timestamp())) + '.bak'\n    try:\n        # 写死的路径，第一个参数是网盘地址\n        client.upload('backup/' + file_name, '本地地址，绝对路径')\n        # 打印结果，之后会重定向到log\n        print('upload at ' + file_name)\n    except LocalResourceNotFound as exception:\n        print('An error happen: LocalResourceNotFound ---'  + file_name)\n\n# 如果是直接调用文件，执行upload()\nif __name__ == '__main__':\n    print('run upload')\n    upload()\n```\n\nPython的代码相对简短。只需要在服务器的命令行执行`python upload.py`，备份文件自动上传到网盘。然而还不完整，这个程序的目的是定期执行。接下来结合Django的定期执行接口做到每日备份。\n\n\n\n## Django定时任务编写\n\nDjango是目前很流行的python web服务框架。通过django自带的命令创建project和app（这一部分不讲）。\n\n当项目建好后，使用`python manage.py`可以运行项目、数据库代码整合。开发者可以通过继承BaseCommand类来自定义命令，然后再通过django_crontab定期执行命令。或者不通过自定义命令，直接使用django_crontab定期执行函数。\n\n### 创建backupCmd命令\n\n在之前创建的app下新建management/commands目录，在该目录下新建`backupCmd.py`\n\n```\nfrom django.core.management.base import BaseCommand\nfrom utils.backup.backup import upload\n\nclass Command(BaseCommand):\n    def handle(self, *args, **options):\n        upload()\n```\n\n当完成后，在项目根目录下执行`python manage.py backupCmd`就可以单次执行程序了。\n\n在setting.py尾部添加:\n\n```\n# 运行定时函数，每天1点运行。\nCRONJOBS = [\n    ('0 01 * * *', 'utils.backup.backup','>> ~/test_crontab.log')\n]\n```\n或\n```\n# 运行定时命令， \nCRONJOBS = [\n    ('*/1 * * * *', 'django.core.management.call_command', ['backupCmd'], {}, '>> ~/test_crontab.log'),\n]\n```\n\n然后执行`python manage.py crontab add`，定时任务加入其中。\n\n当时间到达的时候，程序将自动运行。日志会输出到`~/test_crontab.log`中。\n\nlinux中的定时任务crontab的语法如下:\n\n```\n*  *  *  *  * command\n分钟(0-59) 小时(0-23) 每个月的哪一天(1-31) 月份(1-12) 周几(0-6) shell脚本或者命令\n```\n\n\n\n对于使用到的网盘，希望能够付费支持一下。","slug":"python程序同步webdav网盘（坚果云、owncloud）","published":1,"updated":"2020-11-21T07:26:16.625Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1tw00005p92rwowkz1m8","content":"<p>在编写一个Django项目的时候有定时备份数据文件的需求。因为项目运行在云服务器中，担心的是服务器挂掉，所以备份的地方不能是同一台服务器。在这个项目里自己去建一个服务器来管理备份数据显得没有必要。目前网络存储提供商有许多家，我选取的方案是编写python定时程序连接支持WebDAV协议的网盘，例如坚果云和owncloud。</p>\n<p>分为两部分：</p>\n<ol>\n<li>Python同步代码编写</li>\n<li>Django定时任务编写</li>\n</ol>\n<h2 id=\"Python同步代码编写\"><a href=\"#Python同步代码编写\" class=\"headerlink\" title=\"Python同步代码编写\"></a>Python同步代码编写</h2><p>使用<a href=\"https://pypi.org/project/webdavclient3/\" target=\"_blank\" rel=\"noopener\">webdavclient3</a>库来处理webDAV协议的部分。先安装：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install webdavclient3</span><br></pre></td></tr></table></figure>\n\n<p>然后在自己的项目的某个地方建立一个py文件。我选的是<code>[项目目录]\\utils\\backup\\backup.py</code></p>\n<p>编写python代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> webdav3.client <span class=\"keyword\">import</span> Client</span><br><span class=\"line\"><span class=\"keyword\">from</span> datetime <span class=\"keyword\">import</span> datetime</span><br><span class=\"line\"><span class=\"keyword\">from</span> webdav3.exceptions <span class=\"keyword\">import</span> LocalResourceNotFound</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"comment\"># invoke this function every day.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">upload</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    options = &#123;</span><br><span class=\"line\">        <span class=\"string\">'webdav_hostname'</span>: <span class=\"string\">\"网盘地址，如果是坚果云，不能只输入/dav/路径，似乎这个文件夹不能访问，在下面再建一个文件夹，比如backup。网址中需要有backup的地址比如https://dav.jianguoyun.com/dav/backup\"</span>,</span><br><span class=\"line\">        <span class=\"string\">'webdav_login'</span>: <span class=\"string\">\"用户名\"</span>,</span><br><span class=\"line\">        <span class=\"string\">'webdav_password'</span>: <span class=\"string\">\"密码，如果是坚果云填写应用密码\"</span>,</span><br><span class=\"line\">        <span class=\"string\">'disable_check'</span>: <span class=\"literal\">True</span>, <span class=\"comment\">#有的网盘不支持check功能</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    client = Client(options)</span><br><span class=\"line\">\t\t<span class=\"comment\"># 我选择用时间戳为备份文件命名</span></span><br><span class=\"line\">    file_name = str(math.floor(datetime.now().timestamp())) + <span class=\"string\">'.bak'</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"comment\"># 写死的路径，第一个参数是网盘地址</span></span><br><span class=\"line\">        client.upload(<span class=\"string\">'backup/'</span> + file_name, <span class=\"string\">'本地地址，绝对路径'</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 打印结果，之后会重定向到log</span></span><br><span class=\"line\">        print(<span class=\"string\">'upload at '</span> + file_name)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> LocalResourceNotFound <span class=\"keyword\">as</span> exception:</span><br><span class=\"line\">        print(<span class=\"string\">'An error happen: LocalResourceNotFound ---'</span>  + file_name)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果是直接调用文件，执行upload()</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'run upload'</span>)</span><br><span class=\"line\">    upload()</span><br></pre></td></tr></table></figure>\n\n<p>Python的代码相对简短。只需要在服务器的命令行执行<code>python upload.py</code>，备份文件自动上传到网盘。然而还不完整，这个程序的目的是定期执行。接下来结合Django的定期执行接口做到每日备份。</p>\n<h2 id=\"Django定时任务编写\"><a href=\"#Django定时任务编写\" class=\"headerlink\" title=\"Django定时任务编写\"></a>Django定时任务编写</h2><p>Django是目前很流行的python web服务框架。通过django自带的命令创建project和app（这一部分不讲）。</p>\n<p>当项目建好后，使用<code>python manage.py</code>可以运行项目、数据库代码整合。开发者可以通过继承BaseCommand类来自定义命令，然后再通过django_crontab定期执行命令。或者不通过自定义命令，直接使用django_crontab定期执行函数。</p>\n<h3 id=\"创建backupCmd命令\"><a href=\"#创建backupCmd命令\" class=\"headerlink\" title=\"创建backupCmd命令\"></a>创建backupCmd命令</h3><p>在之前创建的app下新建management/commands目录，在该目录下新建<code>backupCmd.py</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from django.core.management.base import BaseCommand</span><br><span class=\"line\">from utils.backup.backup import upload</span><br><span class=\"line\"></span><br><span class=\"line\">class Command(BaseCommand):</span><br><span class=\"line\">    def handle(self, *args, **options):</span><br><span class=\"line\">        upload()</span><br></pre></td></tr></table></figure>\n\n<p>当完成后，在项目根目录下执行<code>python manage.py backupCmd</code>就可以单次执行程序了。</p>\n<p>在setting.py尾部添加:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 运行定时函数，每天1点运行。</span><br><span class=\"line\">CRONJOBS = [</span><br><span class=\"line\">    (&apos;0 01 * * *&apos;, &apos;utils.backup.backup&apos;,&apos;&gt;&gt; ~/test_crontab.log&apos;)</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>或</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 运行定时命令， </span><br><span class=\"line\">CRONJOBS = [</span><br><span class=\"line\">    (&apos;*/1 * * * *&apos;, &apos;django.core.management.call_command&apos;, [&apos;backupCmd&apos;], &#123;&#125;, &apos;&gt;&gt; ~/test_crontab.log&apos;),</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>然后执行<code>python manage.py crontab add</code>，定时任务加入其中。</p>\n<p>当时间到达的时候，程序将自动运行。日志会输出到<code>~/test_crontab.log</code>中。</p>\n<p>linux中的定时任务crontab的语法如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">*  *  *  *  * command</span><br><span class=\"line\">分钟(0-59) 小时(0-23) 每个月的哪一天(1-31) 月份(1-12) 周几(0-6) shell脚本或者命令</span><br></pre></td></tr></table></figure>\n\n<p>对于使用到的网盘，希望能够付费支持一下。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在编写一个Django项目的时候有定时备份数据文件的需求。因为项目运行在云服务器中，担心的是服务器挂掉，所以备份的地方不能是同一台服务器。在这个项目里自己去建一个服务器来管理备份数据显得没有必要。目前网络存储提供商有许多家，我选取的方案是编写python定时程序连接支持WebDAV协议的网盘，例如坚果云和owncloud。</p>\n<p>分为两部分：</p>\n<ol>\n<li>Python同步代码编写</li>\n<li>Django定时任务编写</li>\n</ol>\n<h2 id=\"Python同步代码编写\"><a href=\"#Python同步代码编写\" class=\"headerlink\" title=\"Python同步代码编写\"></a>Python同步代码编写</h2><p>使用<a href=\"https://pypi.org/project/webdavclient3/\" target=\"_blank\" rel=\"noopener\">webdavclient3</a>库来处理webDAV协议的部分。先安装：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install webdavclient3</span><br></pre></td></tr></table></figure>\n\n<p>然后在自己的项目的某个地方建立一个py文件。我选的是<code>[项目目录]\\utils\\backup\\backup.py</code></p>\n<p>编写python代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> webdav3.client <span class=\"keyword\">import</span> Client</span><br><span class=\"line\"><span class=\"keyword\">from</span> datetime <span class=\"keyword\">import</span> datetime</span><br><span class=\"line\"><span class=\"keyword\">from</span> webdav3.exceptions <span class=\"keyword\">import</span> LocalResourceNotFound</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"comment\"># invoke this function every day.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">upload</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    options = &#123;</span><br><span class=\"line\">        <span class=\"string\">'webdav_hostname'</span>: <span class=\"string\">\"网盘地址，如果是坚果云，不能只输入/dav/路径，似乎这个文件夹不能访问，在下面再建一个文件夹，比如backup。网址中需要有backup的地址比如https://dav.jianguoyun.com/dav/backup\"</span>,</span><br><span class=\"line\">        <span class=\"string\">'webdav_login'</span>: <span class=\"string\">\"用户名\"</span>,</span><br><span class=\"line\">        <span class=\"string\">'webdav_password'</span>: <span class=\"string\">\"密码，如果是坚果云填写应用密码\"</span>,</span><br><span class=\"line\">        <span class=\"string\">'disable_check'</span>: <span class=\"literal\">True</span>, <span class=\"comment\">#有的网盘不支持check功能</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    client = Client(options)</span><br><span class=\"line\">\t\t<span class=\"comment\"># 我选择用时间戳为备份文件命名</span></span><br><span class=\"line\">    file_name = str(math.floor(datetime.now().timestamp())) + <span class=\"string\">'.bak'</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"comment\"># 写死的路径，第一个参数是网盘地址</span></span><br><span class=\"line\">        client.upload(<span class=\"string\">'backup/'</span> + file_name, <span class=\"string\">'本地地址，绝对路径'</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 打印结果，之后会重定向到log</span></span><br><span class=\"line\">        print(<span class=\"string\">'upload at '</span> + file_name)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> LocalResourceNotFound <span class=\"keyword\">as</span> exception:</span><br><span class=\"line\">        print(<span class=\"string\">'An error happen: LocalResourceNotFound ---'</span>  + file_name)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果是直接调用文件，执行upload()</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'run upload'</span>)</span><br><span class=\"line\">    upload()</span><br></pre></td></tr></table></figure>\n\n<p>Python的代码相对简短。只需要在服务器的命令行执行<code>python upload.py</code>，备份文件自动上传到网盘。然而还不完整，这个程序的目的是定期执行。接下来结合Django的定期执行接口做到每日备份。</p>\n<h2 id=\"Django定时任务编写\"><a href=\"#Django定时任务编写\" class=\"headerlink\" title=\"Django定时任务编写\"></a>Django定时任务编写</h2><p>Django是目前很流行的python web服务框架。通过django自带的命令创建project和app（这一部分不讲）。</p>\n<p>当项目建好后，使用<code>python manage.py</code>可以运行项目、数据库代码整合。开发者可以通过继承BaseCommand类来自定义命令，然后再通过django_crontab定期执行命令。或者不通过自定义命令，直接使用django_crontab定期执行函数。</p>\n<h3 id=\"创建backupCmd命令\"><a href=\"#创建backupCmd命令\" class=\"headerlink\" title=\"创建backupCmd命令\"></a>创建backupCmd命令</h3><p>在之前创建的app下新建management/commands目录，在该目录下新建<code>backupCmd.py</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from django.core.management.base import BaseCommand</span><br><span class=\"line\">from utils.backup.backup import upload</span><br><span class=\"line\"></span><br><span class=\"line\">class Command(BaseCommand):</span><br><span class=\"line\">    def handle(self, *args, **options):</span><br><span class=\"line\">        upload()</span><br></pre></td></tr></table></figure>\n\n<p>当完成后，在项目根目录下执行<code>python manage.py backupCmd</code>就可以单次执行程序了。</p>\n<p>在setting.py尾部添加:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 运行定时函数，每天1点运行。</span><br><span class=\"line\">CRONJOBS = [</span><br><span class=\"line\">    (&apos;0 01 * * *&apos;, &apos;utils.backup.backup&apos;,&apos;&gt;&gt; ~/test_crontab.log&apos;)</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>或</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 运行定时命令， </span><br><span class=\"line\">CRONJOBS = [</span><br><span class=\"line\">    (&apos;*/1 * * * *&apos;, &apos;django.core.management.call_command&apos;, [&apos;backupCmd&apos;], &#123;&#125;, &apos;&gt;&gt; ~/test_crontab.log&apos;),</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>然后执行<code>python manage.py crontab add</code>，定时任务加入其中。</p>\n<p>当时间到达的时候，程序将自动运行。日志会输出到<code>~/test_crontab.log</code>中。</p>\n<p>linux中的定时任务crontab的语法如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">*  *  *  *  * command</span><br><span class=\"line\">分钟(0-59) 小时(0-23) 每个月的哪一天(1-31) 月份(1-12) 周几(0-6) shell脚本或者命令</span><br></pre></td></tr></table></figure>\n\n<p>对于使用到的网盘，希望能够付费支持一下。</p>\n"},{"title":"我为什么决定读研究生","date":"2019-09-26T10:46:26.000Z","_content":"\n在这个时刻，为了研招网确认报名而未睡，想想写写自己为什么从一个就业党到保研的路。\n\n我从很久以前就“认清事实”，自己的学习能力只能算是中游偏下，自律和耐力也不行。\n\n本来想着自己这不高的GPA是没有可能继续读书的，别说保研了，连考研，出国等等继续读书的可能都一一被自己否决了。还好，有编程的能力，对代码还算喜欢。本来在华为实习，基本拿到了offer。跟父母也说了自己去就业有什么不好的，拿着一份很有吸引力的薪水，做着自己也算是喜欢的编写代码的工作。\n\n对我而言考研是不可能的。因为我害怕考研的失败和浪费自己生命的一年时间。\n\n后来在本科导师的建议下试着投递保研申请，没想到过程是意外的轻松，也许是因为南科大的计算机系2/5左右的人都有申请出国或者就业的打算吧。\n\n所以第一个条件满足了。但是我心里是犹豫的，因为华为的offer最终还没下来，想着一边是马上就能赚钱，购置自己喜欢的商品，另外一边是3年的研究生生活，只有奖学金的收入。\n\n我这个人对钱很在乎的。\n\n但是我其实是对现在进入工作抱有疑虑的，我觉得如果马上进入工作，可能很快我的生活会失衡，因为我自己的生活单调，如果再在安静的园区里工作，没有人脉，没有社交活动。生活会很枯燥的。在实习中工作的状态不是我想要的。起码工作以外很少有与人沟通交流的机会。\n\n最终走上了读研究生这条道路，只是我应该警觉研究生3年不是一个避风港，而是自己意识到不足而该改进的一个宝贵的成长的，准备将来进入社会的一个阶段。","source":"_posts/我为什么决定读研究生.md","raw":"---\ntitle: 我为什么决定读研究生\ndate: 2019-09-26 18:46:26\ntags:\n- 个人\n---\n\n在这个时刻，为了研招网确认报名而未睡，想想写写自己为什么从一个就业党到保研的路。\n\n我从很久以前就“认清事实”，自己的学习能力只能算是中游偏下，自律和耐力也不行。\n\n本来想着自己这不高的GPA是没有可能继续读书的，别说保研了，连考研，出国等等继续读书的可能都一一被自己否决了。还好，有编程的能力，对代码还算喜欢。本来在华为实习，基本拿到了offer。跟父母也说了自己去就业有什么不好的，拿着一份很有吸引力的薪水，做着自己也算是喜欢的编写代码的工作。\n\n对我而言考研是不可能的。因为我害怕考研的失败和浪费自己生命的一年时间。\n\n后来在本科导师的建议下试着投递保研申请，没想到过程是意外的轻松，也许是因为南科大的计算机系2/5左右的人都有申请出国或者就业的打算吧。\n\n所以第一个条件满足了。但是我心里是犹豫的，因为华为的offer最终还没下来，想着一边是马上就能赚钱，购置自己喜欢的商品，另外一边是3年的研究生生活，只有奖学金的收入。\n\n我这个人对钱很在乎的。\n\n但是我其实是对现在进入工作抱有疑虑的，我觉得如果马上进入工作，可能很快我的生活会失衡，因为我自己的生活单调，如果再在安静的园区里工作，没有人脉，没有社交活动。生活会很枯燥的。在实习中工作的状态不是我想要的。起码工作以外很少有与人沟通交流的机会。\n\n最终走上了读研究生这条道路，只是我应该警觉研究生3年不是一个避风港，而是自己意识到不足而该改进的一个宝贵的成长的，准备将来进入社会的一个阶段。","slug":"我为什么决定读研究生","published":1,"updated":"2020-11-21T07:26:16.627Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1tw10006p92raj4y6282","content":"<p>在这个时刻，为了研招网确认报名而未睡，想想写写自己为什么从一个就业党到保研的路。</p>\n<p>我从很久以前就“认清事实”，自己的学习能力只能算是中游偏下，自律和耐力也不行。</p>\n<p>本来想着自己这不高的GPA是没有可能继续读书的，别说保研了，连考研，出国等等继续读书的可能都一一被自己否决了。还好，有编程的能力，对代码还算喜欢。本来在华为实习，基本拿到了offer。跟父母也说了自己去就业有什么不好的，拿着一份很有吸引力的薪水，做着自己也算是喜欢的编写代码的工作。</p>\n<p>对我而言考研是不可能的。因为我害怕考研的失败和浪费自己生命的一年时间。</p>\n<p>后来在本科导师的建议下试着投递保研申请，没想到过程是意外的轻松，也许是因为南科大的计算机系2/5左右的人都有申请出国或者就业的打算吧。</p>\n<p>所以第一个条件满足了。但是我心里是犹豫的，因为华为的offer最终还没下来，想着一边是马上就能赚钱，购置自己喜欢的商品，另外一边是3年的研究生生活，只有奖学金的收入。</p>\n<p>我这个人对钱很在乎的。</p>\n<p>但是我其实是对现在进入工作抱有疑虑的，我觉得如果马上进入工作，可能很快我的生活会失衡，因为我自己的生活单调，如果再在安静的园区里工作，没有人脉，没有社交活动。生活会很枯燥的。在实习中工作的状态不是我想要的。起码工作以外很少有与人沟通交流的机会。</p>\n<p>最终走上了读研究生这条道路，只是我应该警觉研究生3年不是一个避风港，而是自己意识到不足而该改进的一个宝贵的成长的，准备将来进入社会的一个阶段。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在这个时刻，为了研招网确认报名而未睡，想想写写自己为什么从一个就业党到保研的路。</p>\n<p>我从很久以前就“认清事实”，自己的学习能力只能算是中游偏下，自律和耐力也不行。</p>\n<p>本来想着自己这不高的GPA是没有可能继续读书的，别说保研了，连考研，出国等等继续读书的可能都一一被自己否决了。还好，有编程的能力，对代码还算喜欢。本来在华为实习，基本拿到了offer。跟父母也说了自己去就业有什么不好的，拿着一份很有吸引力的薪水，做着自己也算是喜欢的编写代码的工作。</p>\n<p>对我而言考研是不可能的。因为我害怕考研的失败和浪费自己生命的一年时间。</p>\n<p>后来在本科导师的建议下试着投递保研申请，没想到过程是意外的轻松，也许是因为南科大的计算机系2/5左右的人都有申请出国或者就业的打算吧。</p>\n<p>所以第一个条件满足了。但是我心里是犹豫的，因为华为的offer最终还没下来，想着一边是马上就能赚钱，购置自己喜欢的商品，另外一边是3年的研究生生活，只有奖学金的收入。</p>\n<p>我这个人对钱很在乎的。</p>\n<p>但是我其实是对现在进入工作抱有疑虑的，我觉得如果马上进入工作，可能很快我的生活会失衡，因为我自己的生活单调，如果再在安静的园区里工作，没有人脉，没有社交活动。生活会很枯燥的。在实习中工作的状态不是我想要的。起码工作以外很少有与人沟通交流的机会。</p>\n<p>最终走上了读研究生这条道路，只是我应该警觉研究生3年不是一个避风港，而是自己意识到不足而该改进的一个宝贵的成长的，准备将来进入社会的一个阶段。</p>\n"},{"title":"新服务器迁移postgres数据库","date":"2020-11-21T01:39:33.000Z","_content":"\n课题组要部署一个云服务，之前用的是我个人申请的学生版服务器，价格优惠。快到期需要续费，但是优惠版的服务器不能够继续以优惠价格续费，正常的价格是一千多每年。所以我决定新申请一个优惠版服务器，然后迁移Postgres数据库和部署的web server。\n\n基本步骤是购买新服务器（Ubuntu18.04版本），密码重设，为了方便登录也可以设置ssh免密码登录。\n\n## 安装配置Postgres\n\n我觉得Postgres比MySQL好的一点在于其开源。且能实现数据库的基本功能。\n\n### 服务器安装Postgres\n\n`sudo apt-get install postgresql postgresql-client `\n\n可以在本机安装一个postgres-client，免得在两个服务器之间来回登录。\n\n### 配置postgres数据库账号和远程连接\n\n登录新服务器，设置linux中的postgres用户密码\n\n`sudo passwd postgres`或者`sudo -i -u postgres`，免密码登录。\n\n设置postgres 中postgres用户密码(以postgres用户登录)\n\n`psql`进入数据库clinet软件\n\n`postgres \\password`设置数据库管理员postgres的密码\n\n### 远程连接设置\n\n修改Postgres远程连接允许 \n\n`sudo vim /etc/postgres/10/main/postgres.conf`， \n\n修改`listen_addresses`那一行为`listen_addresses = '*'`（有的公司要求不允许数据库开放外网访问，请谨慎该选项）。\n\n修改远程登录选项\n\n`vim  /etc/postgres/10/main/pg_dba.conf`\n\n添加新行\n\n` host  all  all 0.0.0.0/0 md5` (不要使用trust，除非你想任何人能够访问你的数据库内容)\n\n重启服务以应用刚刚的修改\n\n`sudo service postgresql restart`\n\n[参考](http://lazybios.com/2016/11/how-to-make-postgreSQL-can-be-accessed-from-remote-client/)\n\n## 数据库迁移\n\nPostgres 提供导出数据库数据和结构的程序。有两种方法，一种是在本机安装psql程序，新旧服务器都进行上面的远程连接设置，然后本机连接源服务器数据库dump备份文件，然后本机连接目标服务器数据库导入数据；另外一种是登录源服务器dump备份文件，再导出到目标服务器，在目标服务器执行导入操作。我使用的是比较麻烦的第二种。\n\n### 备份数据库\n\n`pg_dump -h (ip or localhost) -U postgres databasename > databasename.bak`\n\n我是在服务器本地进行的操作，所以可以使用localhost。\n\n### 恢复数据库\n\n恢复数据库指令不会创建新的数据库，所以需要先行在新的数据库中创建数据库 `create database databasename;`\n\n然后执行`psql -h localhost -U postgres -d databasename < databasename.bak`恢复数据库。\n\n[参考](https://juejin.cn/post/6844904002409201671)","source":"_posts/新服务器迁移postgres数据库.md","raw":"---\ntitle: 新服务器迁移postgres数据库\ndate: 2020-11-21 09:39:33\ntags: \n- 运维\n- Postgres\n---\n\n课题组要部署一个云服务，之前用的是我个人申请的学生版服务器，价格优惠。快到期需要续费，但是优惠版的服务器不能够继续以优惠价格续费，正常的价格是一千多每年。所以我决定新申请一个优惠版服务器，然后迁移Postgres数据库和部署的web server。\n\n基本步骤是购买新服务器（Ubuntu18.04版本），密码重设，为了方便登录也可以设置ssh免密码登录。\n\n## 安装配置Postgres\n\n我觉得Postgres比MySQL好的一点在于其开源。且能实现数据库的基本功能。\n\n### 服务器安装Postgres\n\n`sudo apt-get install postgresql postgresql-client `\n\n可以在本机安装一个postgres-client，免得在两个服务器之间来回登录。\n\n### 配置postgres数据库账号和远程连接\n\n登录新服务器，设置linux中的postgres用户密码\n\n`sudo passwd postgres`或者`sudo -i -u postgres`，免密码登录。\n\n设置postgres 中postgres用户密码(以postgres用户登录)\n\n`psql`进入数据库clinet软件\n\n`postgres \\password`设置数据库管理员postgres的密码\n\n### 远程连接设置\n\n修改Postgres远程连接允许 \n\n`sudo vim /etc/postgres/10/main/postgres.conf`， \n\n修改`listen_addresses`那一行为`listen_addresses = '*'`（有的公司要求不允许数据库开放外网访问，请谨慎该选项）。\n\n修改远程登录选项\n\n`vim  /etc/postgres/10/main/pg_dba.conf`\n\n添加新行\n\n` host  all  all 0.0.0.0/0 md5` (不要使用trust，除非你想任何人能够访问你的数据库内容)\n\n重启服务以应用刚刚的修改\n\n`sudo service postgresql restart`\n\n[参考](http://lazybios.com/2016/11/how-to-make-postgreSQL-can-be-accessed-from-remote-client/)\n\n## 数据库迁移\n\nPostgres 提供导出数据库数据和结构的程序。有两种方法，一种是在本机安装psql程序，新旧服务器都进行上面的远程连接设置，然后本机连接源服务器数据库dump备份文件，然后本机连接目标服务器数据库导入数据；另外一种是登录源服务器dump备份文件，再导出到目标服务器，在目标服务器执行导入操作。我使用的是比较麻烦的第二种。\n\n### 备份数据库\n\n`pg_dump -h (ip or localhost) -U postgres databasename > databasename.bak`\n\n我是在服务器本地进行的操作，所以可以使用localhost。\n\n### 恢复数据库\n\n恢复数据库指令不会创建新的数据库，所以需要先行在新的数据库中创建数据库 `create database databasename;`\n\n然后执行`psql -h localhost -U postgres -d databasename < databasename.bak`恢复数据库。\n\n[参考](https://juejin.cn/post/6844904002409201671)","slug":"新服务器迁移postgres数据库","published":1,"updated":"2020-11-21T07:26:16.628Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1tw20008p92ruwivphej","content":"<p>课题组要部署一个云服务，之前用的是我个人申请的学生版服务器，价格优惠。快到期需要续费，但是优惠版的服务器不能够继续以优惠价格续费，正常的价格是一千多每年。所以我决定新申请一个优惠版服务器，然后迁移Postgres数据库和部署的web server。</p>\n<p>基本步骤是购买新服务器（Ubuntu18.04版本），密码重设，为了方便登录也可以设置ssh免密码登录。</p>\n<h2 id=\"安装配置Postgres\"><a href=\"#安装配置Postgres\" class=\"headerlink\" title=\"安装配置Postgres\"></a>安装配置Postgres</h2><p>我觉得Postgres比MySQL好的一点在于其开源。且能实现数据库的基本功能。</p>\n<h3 id=\"服务器安装Postgres\"><a href=\"#服务器安装Postgres\" class=\"headerlink\" title=\"服务器安装Postgres\"></a>服务器安装Postgres</h3><p><code>sudo apt-get install postgresql postgresql-client</code></p>\n<p>可以在本机安装一个postgres-client，免得在两个服务器之间来回登录。</p>\n<h3 id=\"配置postgres数据库账号和远程连接\"><a href=\"#配置postgres数据库账号和远程连接\" class=\"headerlink\" title=\"配置postgres数据库账号和远程连接\"></a>配置postgres数据库账号和远程连接</h3><p>登录新服务器，设置linux中的postgres用户密码</p>\n<p><code>sudo passwd postgres</code>或者<code>sudo -i -u postgres</code>，免密码登录。</p>\n<p>设置postgres 中postgres用户密码(以postgres用户登录)</p>\n<p><code>psql</code>进入数据库clinet软件</p>\n<p><code>postgres \\password</code>设置数据库管理员postgres的密码</p>\n<h3 id=\"远程连接设置\"><a href=\"#远程连接设置\" class=\"headerlink\" title=\"远程连接设置\"></a>远程连接设置</h3><p>修改Postgres远程连接允许 </p>\n<p><code>sudo vim /etc/postgres/10/main/postgres.conf</code>， </p>\n<p>修改<code>listen_addresses</code>那一行为<code>listen_addresses = &#39;*&#39;</code>（有的公司要求不允许数据库开放外网访问，请谨慎该选项）。</p>\n<p>修改远程登录选项</p>\n<p><code>vim  /etc/postgres/10/main/pg_dba.conf</code></p>\n<p>添加新行</p>\n<p><code>host  all  all 0.0.0.0/0 md5</code> (不要使用trust，除非你想任何人能够访问你的数据库内容)</p>\n<p>重启服务以应用刚刚的修改</p>\n<p><code>sudo service postgresql restart</code></p>\n<p><a href=\"http://lazybios.com/2016/11/how-to-make-postgreSQL-can-be-accessed-from-remote-client/\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n<h2 id=\"数据库迁移\"><a href=\"#数据库迁移\" class=\"headerlink\" title=\"数据库迁移\"></a>数据库迁移</h2><p>Postgres 提供导出数据库数据和结构的程序。有两种方法，一种是在本机安装psql程序，新旧服务器都进行上面的远程连接设置，然后本机连接源服务器数据库dump备份文件，然后本机连接目标服务器数据库导入数据；另外一种是登录源服务器dump备份文件，再导出到目标服务器，在目标服务器执行导入操作。我使用的是比较麻烦的第二种。</p>\n<h3 id=\"备份数据库\"><a href=\"#备份数据库\" class=\"headerlink\" title=\"备份数据库\"></a>备份数据库</h3><p><code>pg_dump -h (ip or localhost) -U postgres databasename &gt; databasename.bak</code></p>\n<p>我是在服务器本地进行的操作，所以可以使用localhost。</p>\n<h3 id=\"恢复数据库\"><a href=\"#恢复数据库\" class=\"headerlink\" title=\"恢复数据库\"></a>恢复数据库</h3><p>恢复数据库指令不会创建新的数据库，所以需要先行在新的数据库中创建数据库 <code>create database databasename;</code></p>\n<p>然后执行<code>psql -h localhost -U postgres -d databasename &lt; databasename.bak</code>恢复数据库。</p>\n<p><a href=\"https://juejin.cn/post/6844904002409201671\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>课题组要部署一个云服务，之前用的是我个人申请的学生版服务器，价格优惠。快到期需要续费，但是优惠版的服务器不能够继续以优惠价格续费，正常的价格是一千多每年。所以我决定新申请一个优惠版服务器，然后迁移Postgres数据库和部署的web server。</p>\n<p>基本步骤是购买新服务器（Ubuntu18.04版本），密码重设，为了方便登录也可以设置ssh免密码登录。</p>\n<h2 id=\"安装配置Postgres\"><a href=\"#安装配置Postgres\" class=\"headerlink\" title=\"安装配置Postgres\"></a>安装配置Postgres</h2><p>我觉得Postgres比MySQL好的一点在于其开源。且能实现数据库的基本功能。</p>\n<h3 id=\"服务器安装Postgres\"><a href=\"#服务器安装Postgres\" class=\"headerlink\" title=\"服务器安装Postgres\"></a>服务器安装Postgres</h3><p><code>sudo apt-get install postgresql postgresql-client</code></p>\n<p>可以在本机安装一个postgres-client，免得在两个服务器之间来回登录。</p>\n<h3 id=\"配置postgres数据库账号和远程连接\"><a href=\"#配置postgres数据库账号和远程连接\" class=\"headerlink\" title=\"配置postgres数据库账号和远程连接\"></a>配置postgres数据库账号和远程连接</h3><p>登录新服务器，设置linux中的postgres用户密码</p>\n<p><code>sudo passwd postgres</code>或者<code>sudo -i -u postgres</code>，免密码登录。</p>\n<p>设置postgres 中postgres用户密码(以postgres用户登录)</p>\n<p><code>psql</code>进入数据库clinet软件</p>\n<p><code>postgres \\password</code>设置数据库管理员postgres的密码</p>\n<h3 id=\"远程连接设置\"><a href=\"#远程连接设置\" class=\"headerlink\" title=\"远程连接设置\"></a>远程连接设置</h3><p>修改Postgres远程连接允许 </p>\n<p><code>sudo vim /etc/postgres/10/main/postgres.conf</code>， </p>\n<p>修改<code>listen_addresses</code>那一行为<code>listen_addresses = &#39;*&#39;</code>（有的公司要求不允许数据库开放外网访问，请谨慎该选项）。</p>\n<p>修改远程登录选项</p>\n<p><code>vim  /etc/postgres/10/main/pg_dba.conf</code></p>\n<p>添加新行</p>\n<p><code>host  all  all 0.0.0.0/0 md5</code> (不要使用trust，除非你想任何人能够访问你的数据库内容)</p>\n<p>重启服务以应用刚刚的修改</p>\n<p><code>sudo service postgresql restart</code></p>\n<p><a href=\"http://lazybios.com/2016/11/how-to-make-postgreSQL-can-be-accessed-from-remote-client/\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n<h2 id=\"数据库迁移\"><a href=\"#数据库迁移\" class=\"headerlink\" title=\"数据库迁移\"></a>数据库迁移</h2><p>Postgres 提供导出数据库数据和结构的程序。有两种方法，一种是在本机安装psql程序，新旧服务器都进行上面的远程连接设置，然后本机连接源服务器数据库dump备份文件，然后本机连接目标服务器数据库导入数据；另外一种是登录源服务器dump备份文件，再导出到目标服务器，在目标服务器执行导入操作。我使用的是比较麻烦的第二种。</p>\n<h3 id=\"备份数据库\"><a href=\"#备份数据库\" class=\"headerlink\" title=\"备份数据库\"></a>备份数据库</h3><p><code>pg_dump -h (ip or localhost) -U postgres databasename &gt; databasename.bak</code></p>\n<p>我是在服务器本地进行的操作，所以可以使用localhost。</p>\n<h3 id=\"恢复数据库\"><a href=\"#恢复数据库\" class=\"headerlink\" title=\"恢复数据库\"></a>恢复数据库</h3><p>恢复数据库指令不会创建新的数据库，所以需要先行在新的数据库中创建数据库 <code>create database databasename;</code></p>\n<p>然后执行<code>psql -h localhost -U postgres -d databasename &lt; databasename.bak</code>恢复数据库。</p>\n<p><a href=\"https://juejin.cn/post/6844904002409201671\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n"},{"title":"用python写一个过滤器","date":"2019-09-27T09:17:31.000Z","_content":"\n计算机视觉课Assigment2的内容.\n\n要求写出一个图像的过滤器出来。\n\n## 要求\n\n**Image Filtering.**Image filtering (or convolution) is a fundamental image processing tool. You will be writing your own function to implement image filtering from scratch. More specifically, you will implement my_imfilter()which imitates the filter2Dfunction in the OpenCV library. As specified in student_code.py, your filtering algorithm must\n\n(1) support grayscale and color images\n\n(2) support arbitrary shaped filters, as long as both dimensions are odd\n\n(e.g. 7x9 filters but not 4x5 filters)\n\n(3) pad the input image with zeros or reflected image content\n\n(4) return a filtered image which is the same resolution as the input image.\n\n使用numpy，PIL。\n\n## 步骤：\n\n1. 导入图像（不属于本次任务）\n2. 生成过滤核（不属于本次任务）\n3. padding\n4. calculate\n5. normalization（不属于本次任务，即假设过滤核和图像已经做好了正则化）\n6. 截断\n\n---\n\n## 函数定义\n\n图像过滤器的核心是一个function，函数的定义\n\n```python\ndef my_imfilter(image, filter)\n\treturn filtered_image\n```\n\n## 设计思路\n\n输入的图像为image，image可以为黑白或者RGB彩色。（channel=1 or channel=3)\n\n所以输入的image.ndim可能为2或者为3.\n\n第一部分代码需要判断黑白和彩色的情况.\n\n图片过滤器会将原图片的尺寸缩小,因为进行卷积操作是将原图像的一个矩阵框中的点和过滤核相乘后在求和得到新图像的一个点的值。\n\n【公式和图像】\n\n假设原始图像的高和宽为img_h, img_w，过滤核的大小为filter_h, filter_w。这里有一个条件过滤核的长宽都必须为为奇数。（为什么呢？）那么除掉最中心的像素外，过滤核可以被分为四等份。每一部分的长宽(padding)为 pad_h = (filter_h-1)/2和pad_w = (filter_w-1)/2\n\n那么输出的尺寸为img_h - pad_h * 2 和 img_w - pad_w *2，因为图像两边都需要减去pad_h的尺寸。\n\n同时我们还要给过滤器做一个上下左右变换。因为卷积操作和相关操作的过滤核移动方向是相反的，应该是从下往上，从右往左。\n\n## 实例代码：\n\n```python\ndef my_imfilter(image, filter):\n  \"\"\"\n  Apply a filter to an image. Return the filtered image.\n\n  Args\n  - image: numpy nd-array of dim (m, n, c)\n  - filter: numpy nd-array of dim (k, k)\n  Returns\n  - filtered_image: numpy nd-array of dim (m, n, c)\n\n  HINTS:\n  - You may not use any libraries that do the work for you. Using numpy to work\n   with matrices is fine and encouraged. Using opencv or similar to do the\n   filtering for you is not allowed.\n  - I encourage you to try implementing this naively first, just be aware that\n   it may take an absurdly long time to run. You will need to get a function\n   that takes a reasonable amount of time to run so that the TAs can verify\n   your code works.\n  - Remember these are RGB images, accounting for the final image dimension.\n  \"\"\"\n\n  assert filter.shape[0] % 2 == 1\n  assert filter.shape[1] % 2 == 1\n  # 默认设置图像的通道数，如果只有一个通道，即输入图片是二维的。\n  channel = 1\n  # 如果输入维度=3，通道数等于第三个维度的元素数量\n  if image.ndim == 3:\n    channel = image.shape[2]\n  # 获取图片的长度和宽度\n  image_h, image_w = image.shape[:2]\n  # 将过滤核做上下、左右翻转，以确保是卷积操作而不是相关操作\n  filter = np.flipud(filter)\n  filter = np.fliplr(filter)\n  # 获取过滤核的长度和宽度\n  filter_h, filter_w = filter.shape\n  pad_h = (filter_h - 1) // 2\n  pad_w = (filter_w - 1) // 2\n  # 先扩充原图像,为了不影响原图像，需要复制一份图像\n  image_cp = image.copy()\n  image_cp = np.pad(image_cp,[(pad_h,pad_h),(pad_w,pad_w),(0,0)],\"constant\")\n  # 生成过滤后的图片的空容器，尺寸可以是原本的尺寸，在这里为了下面坐标转换方便，扩大了长宽。\n  filtered_image = np.zeros(image_cp.shape)\n  # 第一层是对于不同的channel做卷积  \n  for i in range(channel):\n    # 第二层是高度y轴像素遍历\n    for j in range(pad_h,image_h+pad_h):\n        # 第三层是宽度x轴像素遍历\n        for k in range(pad_w,image_w+pad_w):\n            # 算法核心，加上上面的翻转式卷积操作，单独来看是相关操作。其实可以通过 step=-1来做\n            filtered_image[j,k,i] = np.sum(np.multiply(image_cp[j-pad_h:j+pad_h+1,k-pad_w:k+pad_w+1,i],filter))\n\n  return filtered_image[pad_h:image_h+pad_h, pad_w:image_w+pad_w,:]\n```\n\n\n\n---\n\n相关链接：\n\n[卷积与互相关的一点探讨](https://zhuanlan.zhihu.com/p/33194385)\n\n","source":"_posts/用python写一个过滤器.md","raw":"---\ntitle: 用python写一个过滤器\ndate: 2019-09-27 17:17:31\ntags:\n---\n\n计算机视觉课Assigment2的内容.\n\n要求写出一个图像的过滤器出来。\n\n## 要求\n\n**Image Filtering.**Image filtering (or convolution) is a fundamental image processing tool. You will be writing your own function to implement image filtering from scratch. More specifically, you will implement my_imfilter()which imitates the filter2Dfunction in the OpenCV library. As specified in student_code.py, your filtering algorithm must\n\n(1) support grayscale and color images\n\n(2) support arbitrary shaped filters, as long as both dimensions are odd\n\n(e.g. 7x9 filters but not 4x5 filters)\n\n(3) pad the input image with zeros or reflected image content\n\n(4) return a filtered image which is the same resolution as the input image.\n\n使用numpy，PIL。\n\n## 步骤：\n\n1. 导入图像（不属于本次任务）\n2. 生成过滤核（不属于本次任务）\n3. padding\n4. calculate\n5. normalization（不属于本次任务，即假设过滤核和图像已经做好了正则化）\n6. 截断\n\n---\n\n## 函数定义\n\n图像过滤器的核心是一个function，函数的定义\n\n```python\ndef my_imfilter(image, filter)\n\treturn filtered_image\n```\n\n## 设计思路\n\n输入的图像为image，image可以为黑白或者RGB彩色。（channel=1 or channel=3)\n\n所以输入的image.ndim可能为2或者为3.\n\n第一部分代码需要判断黑白和彩色的情况.\n\n图片过滤器会将原图片的尺寸缩小,因为进行卷积操作是将原图像的一个矩阵框中的点和过滤核相乘后在求和得到新图像的一个点的值。\n\n【公式和图像】\n\n假设原始图像的高和宽为img_h, img_w，过滤核的大小为filter_h, filter_w。这里有一个条件过滤核的长宽都必须为为奇数。（为什么呢？）那么除掉最中心的像素外，过滤核可以被分为四等份。每一部分的长宽(padding)为 pad_h = (filter_h-1)/2和pad_w = (filter_w-1)/2\n\n那么输出的尺寸为img_h - pad_h * 2 和 img_w - pad_w *2，因为图像两边都需要减去pad_h的尺寸。\n\n同时我们还要给过滤器做一个上下左右变换。因为卷积操作和相关操作的过滤核移动方向是相反的，应该是从下往上，从右往左。\n\n## 实例代码：\n\n```python\ndef my_imfilter(image, filter):\n  \"\"\"\n  Apply a filter to an image. Return the filtered image.\n\n  Args\n  - image: numpy nd-array of dim (m, n, c)\n  - filter: numpy nd-array of dim (k, k)\n  Returns\n  - filtered_image: numpy nd-array of dim (m, n, c)\n\n  HINTS:\n  - You may not use any libraries that do the work for you. Using numpy to work\n   with matrices is fine and encouraged. Using opencv or similar to do the\n   filtering for you is not allowed.\n  - I encourage you to try implementing this naively first, just be aware that\n   it may take an absurdly long time to run. You will need to get a function\n   that takes a reasonable amount of time to run so that the TAs can verify\n   your code works.\n  - Remember these are RGB images, accounting for the final image dimension.\n  \"\"\"\n\n  assert filter.shape[0] % 2 == 1\n  assert filter.shape[1] % 2 == 1\n  # 默认设置图像的通道数，如果只有一个通道，即输入图片是二维的。\n  channel = 1\n  # 如果输入维度=3，通道数等于第三个维度的元素数量\n  if image.ndim == 3:\n    channel = image.shape[2]\n  # 获取图片的长度和宽度\n  image_h, image_w = image.shape[:2]\n  # 将过滤核做上下、左右翻转，以确保是卷积操作而不是相关操作\n  filter = np.flipud(filter)\n  filter = np.fliplr(filter)\n  # 获取过滤核的长度和宽度\n  filter_h, filter_w = filter.shape\n  pad_h = (filter_h - 1) // 2\n  pad_w = (filter_w - 1) // 2\n  # 先扩充原图像,为了不影响原图像，需要复制一份图像\n  image_cp = image.copy()\n  image_cp = np.pad(image_cp,[(pad_h,pad_h),(pad_w,pad_w),(0,0)],\"constant\")\n  # 生成过滤后的图片的空容器，尺寸可以是原本的尺寸，在这里为了下面坐标转换方便，扩大了长宽。\n  filtered_image = np.zeros(image_cp.shape)\n  # 第一层是对于不同的channel做卷积  \n  for i in range(channel):\n    # 第二层是高度y轴像素遍历\n    for j in range(pad_h,image_h+pad_h):\n        # 第三层是宽度x轴像素遍历\n        for k in range(pad_w,image_w+pad_w):\n            # 算法核心，加上上面的翻转式卷积操作，单独来看是相关操作。其实可以通过 step=-1来做\n            filtered_image[j,k,i] = np.sum(np.multiply(image_cp[j-pad_h:j+pad_h+1,k-pad_w:k+pad_w+1,i],filter))\n\n  return filtered_image[pad_h:image_h+pad_h, pad_w:image_w+pad_w,:]\n```\n\n\n\n---\n\n相关链接：\n\n[卷积与互相关的一点探讨](https://zhuanlan.zhihu.com/p/33194385)\n\n","slug":"用python写一个过滤器","published":1,"updated":"2020-11-21T07:26:16.628Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1tw40009p92rtpbt4wxr","content":"<p>计算机视觉课Assigment2的内容.</p>\n<p>要求写出一个图像的过滤器出来。</p>\n<h2 id=\"要求\"><a href=\"#要求\" class=\"headerlink\" title=\"要求\"></a>要求</h2><p><strong>Image Filtering.</strong>Image filtering (or convolution) is a fundamental image processing tool. You will be writing your own function to implement image filtering from scratch. More specifically, you will implement my_imfilter()which imitates the filter2Dfunction in the OpenCV library. As specified in student_code.py, your filtering algorithm must</p>\n<p>(1) support grayscale and color images</p>\n<p>(2) support arbitrary shaped filters, as long as both dimensions are odd</p>\n<p>(e.g. 7x9 filters but not 4x5 filters)</p>\n<p>(3) pad the input image with zeros or reflected image content</p>\n<p>(4) return a filtered image which is the same resolution as the input image.</p>\n<p>使用numpy，PIL。</p>\n<h2 id=\"步骤：\"><a href=\"#步骤：\" class=\"headerlink\" title=\"步骤：\"></a>步骤：</h2><ol>\n<li>导入图像（不属于本次任务）</li>\n<li>生成过滤核（不属于本次任务）</li>\n<li>padding</li>\n<li>calculate</li>\n<li>normalization（不属于本次任务，即假设过滤核和图像已经做好了正则化）</li>\n<li>截断</li>\n</ol>\n<hr>\n<h2 id=\"函数定义\"><a href=\"#函数定义\" class=\"headerlink\" title=\"函数定义\"></a>函数定义</h2><p>图像过滤器的核心是一个function，函数的定义</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">my_imfilter</span><span class=\"params\">(image, filter)</span></span></span><br><span class=\"line\"><span class=\"function\">\t<span class=\"title\">return</span> <span class=\"title\">filtered_image</span></span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"设计思路\"><a href=\"#设计思路\" class=\"headerlink\" title=\"设计思路\"></a>设计思路</h2><p>输入的图像为image，image可以为黑白或者RGB彩色。（channel=1 or channel=3)</p>\n<p>所以输入的image.ndim可能为2或者为3.</p>\n<p>第一部分代码需要判断黑白和彩色的情况.</p>\n<p>图片过滤器会将原图片的尺寸缩小,因为进行卷积操作是将原图像的一个矩阵框中的点和过滤核相乘后在求和得到新图像的一个点的值。</p>\n<p>【公式和图像】</p>\n<p>假设原始图像的高和宽为img_h, img_w，过滤核的大小为filter_h, filter_w。这里有一个条件过滤核的长宽都必须为为奇数。（为什么呢？）那么除掉最中心的像素外，过滤核可以被分为四等份。每一部分的长宽(padding)为 pad_h = (filter_h-1)/2和pad_w = (filter_w-1)/2</p>\n<p>那么输出的尺寸为img_h - pad_h * 2 和 img_w - pad_w *2，因为图像两边都需要减去pad_h的尺寸。</p>\n<p>同时我们还要给过滤器做一个上下左右变换。因为卷积操作和相关操作的过滤核移动方向是相反的，应该是从下往上，从右往左。</p>\n<h2 id=\"实例代码：\"><a href=\"#实例代码：\" class=\"headerlink\" title=\"实例代码：\"></a>实例代码：</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">my_imfilter</span><span class=\"params\">(image, filter)</span>:</span></span><br><span class=\"line\">  <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">  Apply a filter to an image. Return the filtered image.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">  Args</span></span><br><span class=\"line\"><span class=\"string\">  - image: numpy nd-array of dim (m, n, c)</span></span><br><span class=\"line\"><span class=\"string\">  - filter: numpy nd-array of dim (k, k)</span></span><br><span class=\"line\"><span class=\"string\">  Returns</span></span><br><span class=\"line\"><span class=\"string\">  - filtered_image: numpy nd-array of dim (m, n, c)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">  HINTS:</span></span><br><span class=\"line\"><span class=\"string\">  - You may not use any libraries that do the work for you. Using numpy to work</span></span><br><span class=\"line\"><span class=\"string\">   with matrices is fine and encouraged. Using opencv or similar to do the</span></span><br><span class=\"line\"><span class=\"string\">   filtering for you is not allowed.</span></span><br><span class=\"line\"><span class=\"string\">  - I encourage you to try implementing this naively first, just be aware that</span></span><br><span class=\"line\"><span class=\"string\">   it may take an absurdly long time to run. You will need to get a function</span></span><br><span class=\"line\"><span class=\"string\">   that takes a reasonable amount of time to run so that the TAs can verify</span></span><br><span class=\"line\"><span class=\"string\">   your code works.</span></span><br><span class=\"line\"><span class=\"string\">  - Remember these are RGB images, accounting for the final image dimension.</span></span><br><span class=\"line\"><span class=\"string\">  \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">assert</span> filter.shape[<span class=\"number\">0</span>] % <span class=\"number\">2</span> == <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">assert</span> filter.shape[<span class=\"number\">1</span>] % <span class=\"number\">2</span> == <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"comment\"># 默认设置图像的通道数，如果只有一个通道，即输入图片是二维的。</span></span><br><span class=\"line\">  channel = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"comment\"># 如果输入维度=3，通道数等于第三个维度的元素数量</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> image.ndim == <span class=\"number\">3</span>:</span><br><span class=\"line\">    channel = image.shape[<span class=\"number\">2</span>]</span><br><span class=\"line\">  <span class=\"comment\"># 获取图片的长度和宽度</span></span><br><span class=\"line\">  image_h, image_w = image.shape[:<span class=\"number\">2</span>]</span><br><span class=\"line\">  <span class=\"comment\"># 将过滤核做上下、左右翻转，以确保是卷积操作而不是相关操作</span></span><br><span class=\"line\">  filter = np.flipud(filter)</span><br><span class=\"line\">  filter = np.fliplr(filter)</span><br><span class=\"line\">  <span class=\"comment\"># 获取过滤核的长度和宽度</span></span><br><span class=\"line\">  filter_h, filter_w = filter.shape</span><br><span class=\"line\">  pad_h = (filter_h - <span class=\"number\">1</span>) // <span class=\"number\">2</span></span><br><span class=\"line\">  pad_w = (filter_w - <span class=\"number\">1</span>) // <span class=\"number\">2</span></span><br><span class=\"line\">  <span class=\"comment\"># 先扩充原图像,为了不影响原图像，需要复制一份图像</span></span><br><span class=\"line\">  image_cp = image.copy()</span><br><span class=\"line\">  image_cp = np.pad(image_cp,[(pad_h,pad_h),(pad_w,pad_w),(<span class=\"number\">0</span>,<span class=\"number\">0</span>)],<span class=\"string\">\"constant\"</span>)</span><br><span class=\"line\">  <span class=\"comment\"># 生成过滤后的图片的空容器，尺寸可以是原本的尺寸，在这里为了下面坐标转换方便，扩大了长宽。</span></span><br><span class=\"line\">  filtered_image = np.zeros(image_cp.shape)</span><br><span class=\"line\">  <span class=\"comment\"># 第一层是对于不同的channel做卷积  </span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(channel):</span><br><span class=\"line\">    <span class=\"comment\"># 第二层是高度y轴像素遍历</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(pad_h,image_h+pad_h):</span><br><span class=\"line\">        <span class=\"comment\"># 第三层是宽度x轴像素遍历</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(pad_w,image_w+pad_w):</span><br><span class=\"line\">            <span class=\"comment\"># 算法核心，加上上面的翻转式卷积操作，单独来看是相关操作。其实可以通过 step=-1来做</span></span><br><span class=\"line\">            filtered_image[j,k,i] = np.sum(np.multiply(image_cp[j-pad_h:j+pad_h+<span class=\"number\">1</span>,k-pad_w:k+pad_w+<span class=\"number\">1</span>,i],filter))</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">return</span> filtered_image[pad_h:image_h+pad_h, pad_w:image_w+pad_w,:]</span><br></pre></td></tr></table></figure>\n\n<hr>\n<p>相关链接：</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/33194385\" target=\"_blank\" rel=\"noopener\">卷积与互相关的一点探讨</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>计算机视觉课Assigment2的内容.</p>\n<p>要求写出一个图像的过滤器出来。</p>\n<h2 id=\"要求\"><a href=\"#要求\" class=\"headerlink\" title=\"要求\"></a>要求</h2><p><strong>Image Filtering.</strong>Image filtering (or convolution) is a fundamental image processing tool. You will be writing your own function to implement image filtering from scratch. More specifically, you will implement my_imfilter()which imitates the filter2Dfunction in the OpenCV library. As specified in student_code.py, your filtering algorithm must</p>\n<p>(1) support grayscale and color images</p>\n<p>(2) support arbitrary shaped filters, as long as both dimensions are odd</p>\n<p>(e.g. 7x9 filters but not 4x5 filters)</p>\n<p>(3) pad the input image with zeros or reflected image content</p>\n<p>(4) return a filtered image which is the same resolution as the input image.</p>\n<p>使用numpy，PIL。</p>\n<h2 id=\"步骤：\"><a href=\"#步骤：\" class=\"headerlink\" title=\"步骤：\"></a>步骤：</h2><ol>\n<li>导入图像（不属于本次任务）</li>\n<li>生成过滤核（不属于本次任务）</li>\n<li>padding</li>\n<li>calculate</li>\n<li>normalization（不属于本次任务，即假设过滤核和图像已经做好了正则化）</li>\n<li>截断</li>\n</ol>\n<hr>\n<h2 id=\"函数定义\"><a href=\"#函数定义\" class=\"headerlink\" title=\"函数定义\"></a>函数定义</h2><p>图像过滤器的核心是一个function，函数的定义</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">my_imfilter</span><span class=\"params\">(image, filter)</span></span></span><br><span class=\"line\"><span class=\"function\">\t<span class=\"title\">return</span> <span class=\"title\">filtered_image</span></span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"设计思路\"><a href=\"#设计思路\" class=\"headerlink\" title=\"设计思路\"></a>设计思路</h2><p>输入的图像为image，image可以为黑白或者RGB彩色。（channel=1 or channel=3)</p>\n<p>所以输入的image.ndim可能为2或者为3.</p>\n<p>第一部分代码需要判断黑白和彩色的情况.</p>\n<p>图片过滤器会将原图片的尺寸缩小,因为进行卷积操作是将原图像的一个矩阵框中的点和过滤核相乘后在求和得到新图像的一个点的值。</p>\n<p>【公式和图像】</p>\n<p>假设原始图像的高和宽为img_h, img_w，过滤核的大小为filter_h, filter_w。这里有一个条件过滤核的长宽都必须为为奇数。（为什么呢？）那么除掉最中心的像素外，过滤核可以被分为四等份。每一部分的长宽(padding)为 pad_h = (filter_h-1)/2和pad_w = (filter_w-1)/2</p>\n<p>那么输出的尺寸为img_h - pad_h * 2 和 img_w - pad_w *2，因为图像两边都需要减去pad_h的尺寸。</p>\n<p>同时我们还要给过滤器做一个上下左右变换。因为卷积操作和相关操作的过滤核移动方向是相反的，应该是从下往上，从右往左。</p>\n<h2 id=\"实例代码：\"><a href=\"#实例代码：\" class=\"headerlink\" title=\"实例代码：\"></a>实例代码：</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">my_imfilter</span><span class=\"params\">(image, filter)</span>:</span></span><br><span class=\"line\">  <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">  Apply a filter to an image. Return the filtered image.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">  Args</span></span><br><span class=\"line\"><span class=\"string\">  - image: numpy nd-array of dim (m, n, c)</span></span><br><span class=\"line\"><span class=\"string\">  - filter: numpy nd-array of dim (k, k)</span></span><br><span class=\"line\"><span class=\"string\">  Returns</span></span><br><span class=\"line\"><span class=\"string\">  - filtered_image: numpy nd-array of dim (m, n, c)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">  HINTS:</span></span><br><span class=\"line\"><span class=\"string\">  - You may not use any libraries that do the work for you. Using numpy to work</span></span><br><span class=\"line\"><span class=\"string\">   with matrices is fine and encouraged. Using opencv or similar to do the</span></span><br><span class=\"line\"><span class=\"string\">   filtering for you is not allowed.</span></span><br><span class=\"line\"><span class=\"string\">  - I encourage you to try implementing this naively first, just be aware that</span></span><br><span class=\"line\"><span class=\"string\">   it may take an absurdly long time to run. You will need to get a function</span></span><br><span class=\"line\"><span class=\"string\">   that takes a reasonable amount of time to run so that the TAs can verify</span></span><br><span class=\"line\"><span class=\"string\">   your code works.</span></span><br><span class=\"line\"><span class=\"string\">  - Remember these are RGB images, accounting for the final image dimension.</span></span><br><span class=\"line\"><span class=\"string\">  \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">assert</span> filter.shape[<span class=\"number\">0</span>] % <span class=\"number\">2</span> == <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">assert</span> filter.shape[<span class=\"number\">1</span>] % <span class=\"number\">2</span> == <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"comment\"># 默认设置图像的通道数，如果只有一个通道，即输入图片是二维的。</span></span><br><span class=\"line\">  channel = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"comment\"># 如果输入维度=3，通道数等于第三个维度的元素数量</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> image.ndim == <span class=\"number\">3</span>:</span><br><span class=\"line\">    channel = image.shape[<span class=\"number\">2</span>]</span><br><span class=\"line\">  <span class=\"comment\"># 获取图片的长度和宽度</span></span><br><span class=\"line\">  image_h, image_w = image.shape[:<span class=\"number\">2</span>]</span><br><span class=\"line\">  <span class=\"comment\"># 将过滤核做上下、左右翻转，以确保是卷积操作而不是相关操作</span></span><br><span class=\"line\">  filter = np.flipud(filter)</span><br><span class=\"line\">  filter = np.fliplr(filter)</span><br><span class=\"line\">  <span class=\"comment\"># 获取过滤核的长度和宽度</span></span><br><span class=\"line\">  filter_h, filter_w = filter.shape</span><br><span class=\"line\">  pad_h = (filter_h - <span class=\"number\">1</span>) // <span class=\"number\">2</span></span><br><span class=\"line\">  pad_w = (filter_w - <span class=\"number\">1</span>) // <span class=\"number\">2</span></span><br><span class=\"line\">  <span class=\"comment\"># 先扩充原图像,为了不影响原图像，需要复制一份图像</span></span><br><span class=\"line\">  image_cp = image.copy()</span><br><span class=\"line\">  image_cp = np.pad(image_cp,[(pad_h,pad_h),(pad_w,pad_w),(<span class=\"number\">0</span>,<span class=\"number\">0</span>)],<span class=\"string\">\"constant\"</span>)</span><br><span class=\"line\">  <span class=\"comment\"># 生成过滤后的图片的空容器，尺寸可以是原本的尺寸，在这里为了下面坐标转换方便，扩大了长宽。</span></span><br><span class=\"line\">  filtered_image = np.zeros(image_cp.shape)</span><br><span class=\"line\">  <span class=\"comment\"># 第一层是对于不同的channel做卷积  </span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(channel):</span><br><span class=\"line\">    <span class=\"comment\"># 第二层是高度y轴像素遍历</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(pad_h,image_h+pad_h):</span><br><span class=\"line\">        <span class=\"comment\"># 第三层是宽度x轴像素遍历</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(pad_w,image_w+pad_w):</span><br><span class=\"line\">            <span class=\"comment\"># 算法核心，加上上面的翻转式卷积操作，单独来看是相关操作。其实可以通过 step=-1来做</span></span><br><span class=\"line\">            filtered_image[j,k,i] = np.sum(np.multiply(image_cp[j-pad_h:j+pad_h+<span class=\"number\">1</span>,k-pad_w:k+pad_w+<span class=\"number\">1</span>,i],filter))</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">return</span> filtered_image[pad_h:image_h+pad_h, pad_w:image_w+pad_w,:]</span><br></pre></td></tr></table></figure>\n\n<hr>\n<p>相关链接：</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/33194385\" target=\"_blank\" rel=\"noopener\">卷积与互相关的一点探讨</a></p>\n"},{"title":"软件测试week1 note","date":"2019-09-09T07:23:33.000Z","_content":"\n","source":"_posts/软件测试week1-note.md","raw":"---\ntitle: 软件测试week1 note\ndate: 2019-09-09 15:23:33\ntags:\n---\n\n","slug":"软件测试week1-note","published":1,"updated":"2020-11-21T07:26:16.629Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1tw5000bp92rtlv3obv5","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"软件测试week2 note","date":"2019-09-16T00:33:07.000Z","_content":"\n\n\n## Fault, failure and error\n\nexample\n\n\n\n## 为什么软件测试变得越来越重要？\n\n\n\n## 测试过程的成熟等级\n\nLevel 0\n\nLevel 1\n\nLevel 2\n\nLevel 3\n\nLevel 4\n\n\n\n## 每一个测试的目的是什么？\n\n\n\n\n\n## 什么时候开始测试？\n\n\n\n\n\n## 不测试的代价？\n\n\n\n\n\n## 我们为何要测试软件？\n\n\n\n## 软件活动中的测试等级\n\n\n\n## 软件的可测试性（Software Testability）\n\n\n\n## 可观测性和可控制性（Observability and Controllability）\n\n\n\n## 一个测试样例的组成部分\n\n\n\n\n\n## 自动化测试框架\n\nJunit\n\n\n\n## Hamcrest Common Matchers \n\n\n\n## 如何测试标准输出中的字符串？\n\n## 如何测试Main方法？","source":"_posts/软件测试week2-note.md","raw":"---\ntitle: 软件测试week2 note\ndate: 2019-09-16 08:33:07\ntags: \n---\n\n\n\n## Fault, failure and error\n\nexample\n\n\n\n## 为什么软件测试变得越来越重要？\n\n\n\n## 测试过程的成熟等级\n\nLevel 0\n\nLevel 1\n\nLevel 2\n\nLevel 3\n\nLevel 4\n\n\n\n## 每一个测试的目的是什么？\n\n\n\n\n\n## 什么时候开始测试？\n\n\n\n\n\n## 不测试的代价？\n\n\n\n\n\n## 我们为何要测试软件？\n\n\n\n## 软件活动中的测试等级\n\n\n\n## 软件的可测试性（Software Testability）\n\n\n\n## 可观测性和可控制性（Observability and Controllability）\n\n\n\n## 一个测试样例的组成部分\n\n\n\n\n\n## 自动化测试框架\n\nJunit\n\n\n\n## Hamcrest Common Matchers \n\n\n\n## 如何测试标准输出中的字符串？\n\n## 如何测试Main方法？","slug":"软件测试week2-note","published":1,"updated":"2020-11-21T07:26:16.630Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1tw7000cp92rj2nm27u5","content":"<h2 id=\"Fault-failure-and-error\"><a href=\"#Fault-failure-and-error\" class=\"headerlink\" title=\"Fault, failure and error\"></a>Fault, failure and error</h2><p>example</p>\n<h2 id=\"为什么软件测试变得越来越重要？\"><a href=\"#为什么软件测试变得越来越重要？\" class=\"headerlink\" title=\"为什么软件测试变得越来越重要？\"></a>为什么软件测试变得越来越重要？</h2><h2 id=\"测试过程的成熟等级\"><a href=\"#测试过程的成熟等级\" class=\"headerlink\" title=\"测试过程的成熟等级\"></a>测试过程的成熟等级</h2><p>Level 0</p>\n<p>Level 1</p>\n<p>Level 2</p>\n<p>Level 3</p>\n<p>Level 4</p>\n<h2 id=\"每一个测试的目的是什么？\"><a href=\"#每一个测试的目的是什么？\" class=\"headerlink\" title=\"每一个测试的目的是什么？\"></a>每一个测试的目的是什么？</h2><h2 id=\"什么时候开始测试？\"><a href=\"#什么时候开始测试？\" class=\"headerlink\" title=\"什么时候开始测试？\"></a>什么时候开始测试？</h2><h2 id=\"不测试的代价？\"><a href=\"#不测试的代价？\" class=\"headerlink\" title=\"不测试的代价？\"></a>不测试的代价？</h2><h2 id=\"我们为何要测试软件？\"><a href=\"#我们为何要测试软件？\" class=\"headerlink\" title=\"我们为何要测试软件？\"></a>我们为何要测试软件？</h2><h2 id=\"软件活动中的测试等级\"><a href=\"#软件活动中的测试等级\" class=\"headerlink\" title=\"软件活动中的测试等级\"></a>软件活动中的测试等级</h2><h2 id=\"软件的可测试性（Software-Testability）\"><a href=\"#软件的可测试性（Software-Testability）\" class=\"headerlink\" title=\"软件的可测试性（Software Testability）\"></a>软件的可测试性（Software Testability）</h2><h2 id=\"可观测性和可控制性（Observability-and-Controllability）\"><a href=\"#可观测性和可控制性（Observability-and-Controllability）\" class=\"headerlink\" title=\"可观测性和可控制性（Observability and Controllability）\"></a>可观测性和可控制性（Observability and Controllability）</h2><h2 id=\"一个测试样例的组成部分\"><a href=\"#一个测试样例的组成部分\" class=\"headerlink\" title=\"一个测试样例的组成部分\"></a>一个测试样例的组成部分</h2><h2 id=\"自动化测试框架\"><a href=\"#自动化测试框架\" class=\"headerlink\" title=\"自动化测试框架\"></a>自动化测试框架</h2><p>Junit</p>\n<h2 id=\"Hamcrest-Common-Matchers\"><a href=\"#Hamcrest-Common-Matchers\" class=\"headerlink\" title=\"Hamcrest Common Matchers\"></a>Hamcrest Common Matchers</h2><h2 id=\"如何测试标准输出中的字符串？\"><a href=\"#如何测试标准输出中的字符串？\" class=\"headerlink\" title=\"如何测试标准输出中的字符串？\"></a>如何测试标准输出中的字符串？</h2><h2 id=\"如何测试Main方法？\"><a href=\"#如何测试Main方法？\" class=\"headerlink\" title=\"如何测试Main方法？\"></a>如何测试Main方法？</h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Fault-failure-and-error\"><a href=\"#Fault-failure-and-error\" class=\"headerlink\" title=\"Fault, failure and error\"></a>Fault, failure and error</h2><p>example</p>\n<h2 id=\"为什么软件测试变得越来越重要？\"><a href=\"#为什么软件测试变得越来越重要？\" class=\"headerlink\" title=\"为什么软件测试变得越来越重要？\"></a>为什么软件测试变得越来越重要？</h2><h2 id=\"测试过程的成熟等级\"><a href=\"#测试过程的成熟等级\" class=\"headerlink\" title=\"测试过程的成熟等级\"></a>测试过程的成熟等级</h2><p>Level 0</p>\n<p>Level 1</p>\n<p>Level 2</p>\n<p>Level 3</p>\n<p>Level 4</p>\n<h2 id=\"每一个测试的目的是什么？\"><a href=\"#每一个测试的目的是什么？\" class=\"headerlink\" title=\"每一个测试的目的是什么？\"></a>每一个测试的目的是什么？</h2><h2 id=\"什么时候开始测试？\"><a href=\"#什么时候开始测试？\" class=\"headerlink\" title=\"什么时候开始测试？\"></a>什么时候开始测试？</h2><h2 id=\"不测试的代价？\"><a href=\"#不测试的代价？\" class=\"headerlink\" title=\"不测试的代价？\"></a>不测试的代价？</h2><h2 id=\"我们为何要测试软件？\"><a href=\"#我们为何要测试软件？\" class=\"headerlink\" title=\"我们为何要测试软件？\"></a>我们为何要测试软件？</h2><h2 id=\"软件活动中的测试等级\"><a href=\"#软件活动中的测试等级\" class=\"headerlink\" title=\"软件活动中的测试等级\"></a>软件活动中的测试等级</h2><h2 id=\"软件的可测试性（Software-Testability）\"><a href=\"#软件的可测试性（Software-Testability）\" class=\"headerlink\" title=\"软件的可测试性（Software Testability）\"></a>软件的可测试性（Software Testability）</h2><h2 id=\"可观测性和可控制性（Observability-and-Controllability）\"><a href=\"#可观测性和可控制性（Observability-and-Controllability）\" class=\"headerlink\" title=\"可观测性和可控制性（Observability and Controllability）\"></a>可观测性和可控制性（Observability and Controllability）</h2><h2 id=\"一个测试样例的组成部分\"><a href=\"#一个测试样例的组成部分\" class=\"headerlink\" title=\"一个测试样例的组成部分\"></a>一个测试样例的组成部分</h2><h2 id=\"自动化测试框架\"><a href=\"#自动化测试框架\" class=\"headerlink\" title=\"自动化测试框架\"></a>自动化测试框架</h2><p>Junit</p>\n<h2 id=\"Hamcrest-Common-Matchers\"><a href=\"#Hamcrest-Common-Matchers\" class=\"headerlink\" title=\"Hamcrest Common Matchers\"></a>Hamcrest Common Matchers</h2><h2 id=\"如何测试标准输出中的字符串？\"><a href=\"#如何测试标准输出中的字符串？\" class=\"headerlink\" title=\"如何测试标准输出中的字符串？\"></a>如何测试标准输出中的字符串？</h2><h2 id=\"如何测试Main方法？\"><a href=\"#如何测试Main方法？\" class=\"headerlink\" title=\"如何测试Main方法？\"></a>如何测试Main方法？</h2>"},{"title":"软件测试week3 note","date":"2019-09-16T02:22:47.000Z","_content":"","source":"_posts/软件测试week3-note.md","raw":"---\ntitle: 软件测试week3 note\ndate: 2019-09-16 10:22:47\ntags:\n---\n","slug":"软件测试week3-note","published":1,"updated":"2020-11-21T07:26:16.630Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1tw9000ep92re0a0a0ky","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"用阿里云服务器自己搭建2do同步caldav服务器","date":"2019-09-23T09:33:29.000Z","_content":"\n\n\n## 用阿里云服务器自己搭建2do同步caldav服务器\n一个月前，把任务管理从Microsoft TO-DO迁移到2Do中，自己手上常用一台小米9和Macbook Pro，同步成了问题，不能使用icloud remainder。那就只有DropBox等服务可用。\n因为Dropbox在国内不稳定的连接以及近阶段自己手上的梯子挂掉了，所以琢磨自己搭一个同步使用的服务器。\n流程非常简单。一个小时以内就可办完。\n使用到的:\n1. 阿里云ECS低配版，如果使用学生版一个月9.5元。\n2. python3。\n3. radicale，使用python3编写的caldav服务器程序。\n\n### 步骤一览: \n1. 云服务器购买与登录\n2. 安全组设置\n3. Python3环境\n4. 安装radicale\n5. 配置radicale\n\n---\n\n### 云服务器准备\n\n购买与登录的流程就自己搜索吧。我购买的是Ubunt的主机，也可以使用CentOS。\n\n需要注意的是在创建服务器实例之前，最好先将本机的ssh public key导入到阿里云中，并且新建实例后选择该public key，不然不能够在终端中登录，而阿里云提供的浏览器vnc完全不能用。也可以在创建实例后再创建public key，只是需要重启实例罢了。\n\n### 安全组设置\n\n阿里云默认关闭大多数端口，包括了我们这次要使用到的5232端口。所以请自行搜索“阿里云 安全组 端口开启“完成该步骤。\n\n\n\n### Python3的环境搭建\n\n这一步大体上也是请搜索“Ubuntu python3“\n\n需要注意的是先使用`sudo apt-get update`更新本地软件目录。\n\n\n\n### 安装radicale\n\n这一步会很详细的讲。\n\n安装radicale\n\n```\npython3 -m pip install --upgrade radicale \n```\n若是在本地使用，可以使用下面这个命令。\n\n```shell\npython3 -m radicale --config \"\" --storage-filesystem-folder=~/.var/lib/radicale/collections\n```\n\n然后访问 http://localhost:5232\n\n但是若在服务器上访问就无法访问。需要配置参数。\n\n我使用的配置有，把配置文件放在`/etc/radicale/config`\n\n```\n[server]\n# Bind all addresses\nhosts = 0.0.0.0:5232\ndaemon = True\n[auth]\ntype = htpasswd\nhtpasswd_filename = /etc/radicale/users\nhtpasswd_encryption = plain\n[storage]\nfilesystem_folder = ~/.var/lib/radicale/collections\n```\n\n想详细了解更多配置选项可以查看[configuration](https://radicale.org/configuration/)\n\n然后可以设置用户名和密码, 把配置文件放在`/etc/radicale/users`\n\n```\nuser:password\n```\n\n然后启动就好了。\n\n```\npython3 -m radicale\n```\n\n正常访问。\n\n然后在2Do的配置上选择就好了。\n\n![2do](../_images/2do.png)","source":"_posts/用阿里云服务器自己搭建2do同步caldav服务器.md","raw":"---\ntitle: 用阿里云服务器自己搭建2do同步caldav服务器\ndate: 2019-09-23 17:33:29\ntags:\n- Linux\n---\n\n\n\n## 用阿里云服务器自己搭建2do同步caldav服务器\n一个月前，把任务管理从Microsoft TO-DO迁移到2Do中，自己手上常用一台小米9和Macbook Pro，同步成了问题，不能使用icloud remainder。那就只有DropBox等服务可用。\n因为Dropbox在国内不稳定的连接以及近阶段自己手上的梯子挂掉了，所以琢磨自己搭一个同步使用的服务器。\n流程非常简单。一个小时以内就可办完。\n使用到的:\n1. 阿里云ECS低配版，如果使用学生版一个月9.5元。\n2. python3。\n3. radicale，使用python3编写的caldav服务器程序。\n\n### 步骤一览: \n1. 云服务器购买与登录\n2. 安全组设置\n3. Python3环境\n4. 安装radicale\n5. 配置radicale\n\n---\n\n### 云服务器准备\n\n购买与登录的流程就自己搜索吧。我购买的是Ubunt的主机，也可以使用CentOS。\n\n需要注意的是在创建服务器实例之前，最好先将本机的ssh public key导入到阿里云中，并且新建实例后选择该public key，不然不能够在终端中登录，而阿里云提供的浏览器vnc完全不能用。也可以在创建实例后再创建public key，只是需要重启实例罢了。\n\n### 安全组设置\n\n阿里云默认关闭大多数端口，包括了我们这次要使用到的5232端口。所以请自行搜索“阿里云 安全组 端口开启“完成该步骤。\n\n\n\n### Python3的环境搭建\n\n这一步大体上也是请搜索“Ubuntu python3“\n\n需要注意的是先使用`sudo apt-get update`更新本地软件目录。\n\n\n\n### 安装radicale\n\n这一步会很详细的讲。\n\n安装radicale\n\n```\npython3 -m pip install --upgrade radicale \n```\n若是在本地使用，可以使用下面这个命令。\n\n```shell\npython3 -m radicale --config \"\" --storage-filesystem-folder=~/.var/lib/radicale/collections\n```\n\n然后访问 http://localhost:5232\n\n但是若在服务器上访问就无法访问。需要配置参数。\n\n我使用的配置有，把配置文件放在`/etc/radicale/config`\n\n```\n[server]\n# Bind all addresses\nhosts = 0.0.0.0:5232\ndaemon = True\n[auth]\ntype = htpasswd\nhtpasswd_filename = /etc/radicale/users\nhtpasswd_encryption = plain\n[storage]\nfilesystem_folder = ~/.var/lib/radicale/collections\n```\n\n想详细了解更多配置选项可以查看[configuration](https://radicale.org/configuration/)\n\n然后可以设置用户名和密码, 把配置文件放在`/etc/radicale/users`\n\n```\nuser:password\n```\n\n然后启动就好了。\n\n```\npython3 -m radicale\n```\n\n正常访问。\n\n然后在2Do的配置上选择就好了。\n\n![2do](../_images/2do.png)","slug":"用阿里云服务器自己搭建2do同步caldav服务器","published":1,"updated":"2020-11-21T15:28:30.728Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1twa000fp92rq3opatat","content":"<h2 id=\"用阿里云服务器自己搭建2do同步caldav服务器\"><a href=\"#用阿里云服务器自己搭建2do同步caldav服务器\" class=\"headerlink\" title=\"用阿里云服务器自己搭建2do同步caldav服务器\"></a>用阿里云服务器自己搭建2do同步caldav服务器</h2><p>一个月前，把任务管理从Microsoft TO-DO迁移到2Do中，自己手上常用一台小米9和Macbook Pro，同步成了问题，不能使用icloud remainder。那就只有DropBox等服务可用。<br>因为Dropbox在国内不稳定的连接以及近阶段自己手上的梯子挂掉了，所以琢磨自己搭一个同步使用的服务器。<br>流程非常简单。一个小时以内就可办完。<br>使用到的:</p>\n<ol>\n<li>阿里云ECS低配版，如果使用学生版一个月9.5元。</li>\n<li>python3。</li>\n<li>radicale，使用python3编写的caldav服务器程序。</li>\n</ol>\n<h3 id=\"步骤一览\"><a href=\"#步骤一览\" class=\"headerlink\" title=\"步骤一览:\"></a>步骤一览:</h3><ol>\n<li>云服务器购买与登录</li>\n<li>安全组设置</li>\n<li>Python3环境</li>\n<li>安装radicale</li>\n<li>配置radicale</li>\n</ol>\n<hr>\n<h3 id=\"云服务器准备\"><a href=\"#云服务器准备\" class=\"headerlink\" title=\"云服务器准备\"></a>云服务器准备</h3><p>购买与登录的流程就自己搜索吧。我购买的是Ubunt的主机，也可以使用CentOS。</p>\n<p>需要注意的是在创建服务器实例之前，最好先将本机的ssh public key导入到阿里云中，并且新建实例后选择该public key，不然不能够在终端中登录，而阿里云提供的浏览器vnc完全不能用。也可以在创建实例后再创建public key，只是需要重启实例罢了。</p>\n<h3 id=\"安全组设置\"><a href=\"#安全组设置\" class=\"headerlink\" title=\"安全组设置\"></a>安全组设置</h3><p>阿里云默认关闭大多数端口，包括了我们这次要使用到的5232端口。所以请自行搜索“阿里云 安全组 端口开启“完成该步骤。</p>\n<h3 id=\"Python3的环境搭建\"><a href=\"#Python3的环境搭建\" class=\"headerlink\" title=\"Python3的环境搭建\"></a>Python3的环境搭建</h3><p>这一步大体上也是请搜索“Ubuntu python3“</p>\n<p>需要注意的是先使用<code>sudo apt-get update</code>更新本地软件目录。</p>\n<h3 id=\"安装radicale\"><a href=\"#安装radicale\" class=\"headerlink\" title=\"安装radicale\"></a>安装radicale</h3><p>这一步会很详细的讲。</p>\n<p>安装radicale</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install --upgrade radicale</span><br></pre></td></tr></table></figure>\n\n<p>若是在本地使用，可以使用下面这个命令。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m radicale --config \"\" --storage-filesystem-folder=~/.var/lib/radicale/collections</span><br></pre></td></tr></table></figure>\n\n<p>然后访问 <a href=\"http://localhost:5232\" target=\"_blank\" rel=\"noopener\">http://localhost:5232</a></p>\n<p>但是若在服务器上访问就无法访问。需要配置参数。</p>\n<p>我使用的配置有，把配置文件放在<code>/etc/radicale/config</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[server]</span><br><span class=\"line\"># Bind all addresses</span><br><span class=\"line\">hosts = 0.0.0.0:5232</span><br><span class=\"line\">daemon = True</span><br><span class=\"line\">[auth]</span><br><span class=\"line\">type = htpasswd</span><br><span class=\"line\">htpasswd_filename = /etc/radicale/users</span><br><span class=\"line\">htpasswd_encryption = plain</span><br><span class=\"line\">[storage]</span><br><span class=\"line\">filesystem_folder = ~/.var/lib/radicale/collections</span><br></pre></td></tr></table></figure>\n\n<p>想详细了解更多配置选项可以查看<a href=\"https://radicale.org/configuration/\" target=\"_blank\" rel=\"noopener\">configuration</a></p>\n<p>然后可以设置用户名和密码, 把配置文件放在<code>/etc/radicale/users</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user:password</span><br></pre></td></tr></table></figure>\n\n<p>然后启动就好了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m radicale</span><br></pre></td></tr></table></figure>\n\n<p>正常访问。</p>\n<p>然后在2Do的配置上选择就好了。</p>\n<p><img src=\"../_images/2do.png\" alt=\"2do\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"用阿里云服务器自己搭建2do同步caldav服务器\"><a href=\"#用阿里云服务器自己搭建2do同步caldav服务器\" class=\"headerlink\" title=\"用阿里云服务器自己搭建2do同步caldav服务器\"></a>用阿里云服务器自己搭建2do同步caldav服务器</h2><p>一个月前，把任务管理从Microsoft TO-DO迁移到2Do中，自己手上常用一台小米9和Macbook Pro，同步成了问题，不能使用icloud remainder。那就只有DropBox等服务可用。<br>因为Dropbox在国内不稳定的连接以及近阶段自己手上的梯子挂掉了，所以琢磨自己搭一个同步使用的服务器。<br>流程非常简单。一个小时以内就可办完。<br>使用到的:</p>\n<ol>\n<li>阿里云ECS低配版，如果使用学生版一个月9.5元。</li>\n<li>python3。</li>\n<li>radicale，使用python3编写的caldav服务器程序。</li>\n</ol>\n<h3 id=\"步骤一览\"><a href=\"#步骤一览\" class=\"headerlink\" title=\"步骤一览:\"></a>步骤一览:</h3><ol>\n<li>云服务器购买与登录</li>\n<li>安全组设置</li>\n<li>Python3环境</li>\n<li>安装radicale</li>\n<li>配置radicale</li>\n</ol>\n<hr>\n<h3 id=\"云服务器准备\"><a href=\"#云服务器准备\" class=\"headerlink\" title=\"云服务器准备\"></a>云服务器准备</h3><p>购买与登录的流程就自己搜索吧。我购买的是Ubunt的主机，也可以使用CentOS。</p>\n<p>需要注意的是在创建服务器实例之前，最好先将本机的ssh public key导入到阿里云中，并且新建实例后选择该public key，不然不能够在终端中登录，而阿里云提供的浏览器vnc完全不能用。也可以在创建实例后再创建public key，只是需要重启实例罢了。</p>\n<h3 id=\"安全组设置\"><a href=\"#安全组设置\" class=\"headerlink\" title=\"安全组设置\"></a>安全组设置</h3><p>阿里云默认关闭大多数端口，包括了我们这次要使用到的5232端口。所以请自行搜索“阿里云 安全组 端口开启“完成该步骤。</p>\n<h3 id=\"Python3的环境搭建\"><a href=\"#Python3的环境搭建\" class=\"headerlink\" title=\"Python3的环境搭建\"></a>Python3的环境搭建</h3><p>这一步大体上也是请搜索“Ubuntu python3“</p>\n<p>需要注意的是先使用<code>sudo apt-get update</code>更新本地软件目录。</p>\n<h3 id=\"安装radicale\"><a href=\"#安装radicale\" class=\"headerlink\" title=\"安装radicale\"></a>安装radicale</h3><p>这一步会很详细的讲。</p>\n<p>安装radicale</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install --upgrade radicale</span><br></pre></td></tr></table></figure>\n\n<p>若是在本地使用，可以使用下面这个命令。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m radicale --config \"\" --storage-filesystem-folder=~/.var/lib/radicale/collections</span><br></pre></td></tr></table></figure>\n\n<p>然后访问 <a href=\"http://localhost:5232\" target=\"_blank\" rel=\"noopener\">http://localhost:5232</a></p>\n<p>但是若在服务器上访问就无法访问。需要配置参数。</p>\n<p>我使用的配置有，把配置文件放在<code>/etc/radicale/config</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[server]</span><br><span class=\"line\"># Bind all addresses</span><br><span class=\"line\">hosts = 0.0.0.0:5232</span><br><span class=\"line\">daemon = True</span><br><span class=\"line\">[auth]</span><br><span class=\"line\">type = htpasswd</span><br><span class=\"line\">htpasswd_filename = /etc/radicale/users</span><br><span class=\"line\">htpasswd_encryption = plain</span><br><span class=\"line\">[storage]</span><br><span class=\"line\">filesystem_folder = ~/.var/lib/radicale/collections</span><br></pre></td></tr></table></figure>\n\n<p>想详细了解更多配置选项可以查看<a href=\"https://radicale.org/configuration/\" target=\"_blank\" rel=\"noopener\">configuration</a></p>\n<p>然后可以设置用户名和密码, 把配置文件放在<code>/etc/radicale/users</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user:password</span><br></pre></td></tr></table></figure>\n\n<p>然后启动就好了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m radicale</span><br></pre></td></tr></table></figure>\n\n<p>正常访问。</p>\n<p>然后在2Do的配置上选择就好了。</p>\n<p><img src=\"../_images/2do.png\" alt=\"2do\"></p>\n"},{"title":"重建博客","date":"2019-09-08T13:51:25.000Z","_content":"\n2019年4月的时候用hexo和github page创建过一个博客，但是在更换电脑的时候意外丢失了模版。后又恢复回来，只剩下了网页，，没有网站配置和md原始文件。\n\n","source":"_posts/重建博客.md","raw":"---\ntitle: 重建博客\ndate: 2019-09-08 21:51:25\ntags:\n---\n\n2019年4月的时候用hexo和github page创建过一个博客，但是在更换电脑的时候意外丢失了模版。后又恢复回来，只剩下了网页，，没有网站配置和md原始文件。\n\n","slug":"重建博客","published":1,"updated":"2020-11-21T07:26:16.631Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1twc000hp92r8stwwcl1","content":"<p>2019年4月的时候用hexo和github page创建过一个博客，但是在更换电脑的时候意外丢失了模版。后又恢复回来，只剩下了网页，，没有网站配置和md原始文件。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>2019年4月的时候用hexo和github page创建过一个博客，但是在更换电脑的时候意外丢失了模版。后又恢复回来，只剩下了网页，，没有网站配置和md原始文件。</p>\n"},{"title":"阅读日报20190911","date":"2019-09-11T15:23:22.000Z","_content":"\n# 神经网络\n\n[神经网络浅讲：从神经元到深度学习](https://www.cnblogs.com/subconscious/p/5058741.html)\n\n对于神经网络，大神级别的导读和总结。\n\n不说了，去抄文中提到的概念去了。\n\n话说软件工程的项目用到的车牌识别技术EasyPR还是此文作者开发的呀。\n\n\n\n","source":"_posts/阅读日报20190911.md","raw":"---\ntitle: 阅读日报20190911\ndate: 2019-09-11 23:23:22\ntags: 阅读日报\n---\n\n# 神经网络\n\n[神经网络浅讲：从神经元到深度学习](https://www.cnblogs.com/subconscious/p/5058741.html)\n\n对于神经网络，大神级别的导读和总结。\n\n不说了，去抄文中提到的概念去了。\n\n话说软件工程的项目用到的车牌识别技术EasyPR还是此文作者开发的呀。\n\n\n\n","slug":"阅读日报20190911","published":1,"updated":"2020-11-21T07:26:16.632Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1twe000jp92r05gyh0i5","content":"<h1 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h1><p><a href=\"https://www.cnblogs.com/subconscious/p/5058741.html\" target=\"_blank\" rel=\"noopener\">神经网络浅讲：从神经元到深度学习</a></p>\n<p>对于神经网络，大神级别的导读和总结。</p>\n<p>不说了，去抄文中提到的概念去了。</p>\n<p>话说软件工程的项目用到的车牌识别技术EasyPR还是此文作者开发的呀。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h1><p><a href=\"https://www.cnblogs.com/subconscious/p/5058741.html\" target=\"_blank\" rel=\"noopener\">神经网络浅讲：从神经元到深度学习</a></p>\n<p>对于神经网络，大神级别的导读和总结。</p>\n<p>不说了，去抄文中提到的概念去了。</p>\n<p>话说软件工程的项目用到的车牌识别技术EasyPR还是此文作者开发的呀。</p>\n"},{"title":"Kaldi入门(一):yesno项目","date":"2020-05-01T09:37:05.000Z","_content":"\n这个学期选了一门自然语言处理课，结果这门课主要的研究课题是自动语音识别（ASR）。既然入了这个坑。就先好好了解一下如何做ASR吧。\n老师Tom Ko要求使用Kaldi这个工具来做ASR。课上到一半才知道Kaldi中有几千行的脚本代码是老师提交的。好吧，脚本好难的。\n为了入门Kaldi，课程的第5次Lab是一个mini projec: yesno\n\n首先要下载并编译Kaldi，安装的过程不是我的学习重点，可以先参考[Kaldi的下载安装与编译]([https://blog.csdn.net/snowdroptulip/article/details/78896915](https://blog.csdn.net/snowdroptulip/article/details/78896915)\n)，在漫长的编译过程之后假设已经安装好了Kaldi。\n\n## 项目目录结构\nyesno项目的脚本和README都在`kaldi/egs/yesno`之下。\nREADME.txt文件中包含数据集描述：\n```shell\nThe \"yesno\" corpus is a very small dataset of recordings of one individual\nsaying yes or no multiple times per recording, in Hebrew.  It is available from\nhttp://www.openslr.org/1.\nIt is mainly included here as an easy way to test out the Kaldi scripts.\n\nThe test set is perfectly recognized at the monophone stage, so the dataset is\nnot exactly challenging.\n\nThe scripts are in s5/.\n```\n数据脚本路径： kaldi/egs/yesno/s5。在下面执行的很多操作都可以直接调用已经写好的脚本来执行，之所以深入到具体的流程中是为了加强对ASR流程的理解。\n\n## 下载数据集\n第一步是从网络上下载数据集文件`waves_yesno.tar.gz`到s5/路径下并解压。\n原始的数据是60个.wav文件。文件名是八个用下划线分隔的01组合。需要将音频数据转化成kaldi能够处理的格式。\n\n\n## 转换成Kaldi能处理的格式\n下载完数据集后，将数据集划分为31个训练，30个测试（数量大致相当）。在s5/下创建data文件夹，把划分好的音频文件放入train_yesno和test_yesno。\n\nKaldi使用以下几个文件来表示数据：\n1. Text\n音频的文本记录。每一个音频文件一行。格式为`<utt_id> <transcript>`。<utt_id>为音频的id，一般用不带扩展名的文件名表示。utt_id在wav.scp文件中与具体的文件映射。<transcript>是音频对应的文字。\n2. wav.scp\n将文件映射到唯一的utt_id。\n格式为`<utt_id> <path or command to get wave file>`\n第二个参数既可以是对应utt_id的音频文件路径，也可以是能够获得音频文件的指令。\n3. utt2spk\n对于每一个音频文件，标记是哪一个人发音的。因为yesno数据集中只有一个发音者，用global来表示所有的utt_id\n文件内每一行的格式为`<utt_id> <speaker_id>`\n\n4. spk2utt\n和3反过来。文件内每一行对应一个发音者，第一个是speaker的id，后面用空格分隔开60个utt_id。格式为`<speaker_id> <utt_id1> <utt_id2> ...`\n\n本步骤可直接调用脚本：\n```bash\ncd kaldi/egs/yesno/s5\nlocal/prepare_data.sh waves_yesno\n```\n读了一下prepare_data.sh的脚本\n```bash\n#!/usr/bin/env bash\n\nmkdir -p data/local\nlocal=`pwd`/local\nscripts=`pwd`/scripts\n\nexport PATH=$PATH:`pwd`/../../../tools/irstlm/bin\n\necho \"Preparing train and test data\"\n\ntrain_base_name=train_yesno\ntest_base_name=test_yesno\nwaves_dir=$1\n\nls -1 $waves_dir > data/local/waves_all.list\n\ncd data/local\n\n../../local/create_yesno_waves_test_train.pl waves_all.list waves.test waves.train\n\n../../local/create_yesno_wav_scp.pl ${waves_dir} waves.test > ${test_base_name}_wav.scp\n\n../../local/create_yesno_wav_scp.pl ${waves_dir} waves.train > ${train_base_name}_wav.scp\n\n../../local/create_yesno_txt.pl waves.test > ${test_base_name}.txt\n\n../../local/create_yesno_txt.pl waves.train > ${train_base_name}.txt\n\ncp ../../input/task.arpabo lm_tg.arpa\n\ncd ../..\n\n# This stage was copied from WSJ example\nfor x in train_yesno test_yesno; do\n  mkdir -p data/$x\n  cp data/local/${x}_wav.scp data/$x/wav.scp\n  cp data/local/$x.txt data/$x/text\n  cat data/$x/text | awk '{printf(\"%s global\\n\", $1);}' > data/$x/utt2spk\n  utils/utt2spk_to_spk2utt.pl <data/$x/utt2spk >data/$x/spk2utt\ndone\n```\n## 建立词典\n\n\n对于当前项目，我们只有两个词 Ken(yes) 和 Lo（no)。但是在真实的语言中，词的数量不可能这么少，并且还有停顿和环境噪声。kaldi将这些非语言的声音称作slience（SIL）。\n加上SIL一共需要三个词来表示当前这个yesno语言模型。\n\n调用脚本：\n```bash\nlocal/prepare_dict.sh\n```\n将会在s5/data/local/dict中看到新生成的5个文件。\n1. lexicon.txt\n```\n<SIL> SIL\nYES Y\nNO N\n```\n2. lexicon_words.txt\n比1少第一行\n3. nonsilence_phones.txt\n```\nY\nN\n```\n4. silence_phones.txt\n```\nSIL\n```\n5. optional_silence.txt\n和4一样\n\n这个脚本本身只是将input文件夹下面的lexicon_nosil.txt，lexicon.txt， phones.txt复制到dict所在目录，并且加上SIL。\n\n\n## 语言模型\n\n接下来要做语言模型。\n项目提供了一个一元的语言模型。然而我们需要将这个模型转换成一个WFST（一种有穷自动机）\n执行:\n ```bash\nutils/prepare_lang.sh --position-dependent-phones false data/local/dict/ \"<SIL>\" data/local/lang/ data/lang\nlocal/prepare_lm.sh\n ```\n\nprepare_lang.sh的开头注释如下：\n```\n# This script prepares a directory such as data/lang/, in the standard format,\n# given a source directory containing a dictionary lexicon.txt in a form like:\n# word phone1 phone2 ... phoneN\n# per line (alternate prons would be separate lines), or a dictionary with probabilities\n# called lexiconp.txt in a form:\n# word pron-prob phone1 phone2 ... phoneN\n# (with 0.0 < pron-prob <= 1.0); note: if lexiconp.txt exists, we use it even if\n# lexicon.txt exists.\n# and also files silence_phones.txt, nonsilence_phones.txt, optional_silence.txt\n# and extra_questions.txt\n# Here, silence_phones.txt and nonsilence_phones.txt are lists of silence and\n# non-silence phones respectively (where silence includes various kinds of\n# noise, laugh, cough, filled pauses etc., and nonsilence phones includes the\n# \"real\" phones.)\n# In each line of those files is a list of phones, and the phones on each line\n# are assumed to correspond to the same \"base phone\", i.e. they will be\n# different stress or tone variations of the same basic phone.\n# The file \"optional_silence.txt\" contains just a single phone (typically SIL)\n# which is used for optional silence in the lexicon.\n# extra_questions.txt might be empty; typically will consist of lists of phones,\n# all members of each list with the same stress or tone; and also possibly a\n# list for the silence phones.  This will augment the automatically generated\n# questions (note: the automatically generated ones will treat all the\n# stress/tone versions of a phone the same, so will not \"get to ask\" about\n# stress or tone).\n```\n通过阅读脚本和脚本中的注释。可以知道`prepare_lang.sh`的用法\n```\nUsage: utils/prepare_lang.sh <dict-src-dir> <oov-dict-entry> <tmp-dir> <lang-dir>\ne.g.: utils/prepare_lang.sh data/local/dict <SPOKEN_NOISE> data/local/lang data/lang\n--position-dependent-phones (true|false)        # default: true; if true, use _B, _E, _S & _I\n```\n<dict-src-dir>是我们在上一部分生成的词典目录。需要包含lexico.txt，extra_questions.txt，nonsilence_phones.txt，optional_silence.txt  silence_phones.txt。\n`position_dependent_phones`参数为false，导致解码后不能算出单词边界。很多的脚本，特别是评分脚本将不能正常运行。\n第二个指令将语言模型转换成G.fst格式并保存在data/lang_test_tg 目录下。\n这个脚本的核心内容是调用了arpa2fst和fstisstochastic，再创建了G.fst之后检查是否有空字符（<s>,</s>之类的）的循环。\n```\n  arpa2fst --disambig-symbol=#0 --read-symbol-table=$test/words.txt input/task.arpabo $test/G.fst\n\n  fstisstochastic $test/G.fst\n```\n[arpa2fst 原理详解](https://blog.csdn.net/yutianzuijin/article/details/78756130)\n> arpa文件可以很容易地表示任意n-gram语言模型，不过在实际中n通常等于3、4或者5。arpa文件的每一行表示一个文法项，它通常包含三部分内容：probability word(s) [backoff probability]。probability表示该词或词组发生的概率，word(s)表示具体的词或者词组。backoff probablitiy是可选项，表示回退概率。\n\n在yesno这个toy project中只使用1元的语言模型。对应的arpa文件在`input/task.arpabo`\n```\n\\data\\\nngram 1=4\n\n\\1-grams:\n-1\tNO\n-1\tYES\n-99 <s>\n-1 </s>\n\n\\end\\\n```\nfstisstochastic命令则如同其名字的含义一样，用来检查G.fst是否是随机的。\n在`s5/data/lang`目录下会出现：\n1. `phones.txt`:将phone转换成数字。其中#0，#1是空字，用来表示句子的开头和结尾。<eps> 是一个特别的含义表示这个弧上没有符号。\n```\n<eps> 0\nSIL 1\nY 2\nN 3\n#0 4\n#1 5\n```\n2. `words.txt`\n```\n<eps> 0\n<SIL> 1\nNO 2\nYES 3\n#0 4\n<s> 5\n</s> 6\n```\n3. `L_disambig.fst, L.fs`t: the dict can be recognized by Kaldi\n4. `topo`: phone states transition(HMM)\n5. `oov`: out of vocabulary. 仅包含`<SIL>`\n6. ` phones`: some information about phones\n\n## 特征提取和训练\nMFCC特征提取和GMM-HMM建模\n\n提取梅尔倒谱系数\n```\nsteps/make_mfcc.sh --nj $num $input_dir $log_dir $output_dir\n```\n[语音信号处理（二）—— MFCC详解](https://zhuanlan.zhihu.com/p/60371062)\n梅尔倒谱系数是一种非线性的时频表示法，其应用了人耳对低频声音的听觉敏感度更高的原理。\n\n接着正则化倒谱特征\n```\nsteps/compute_cmvn_stats.sh \nutils/fix_data_dir.sh $input_dir\n```\n[Kaldi中的特征提取(二）- 特征变换](http://placebokkk.github.io/kaldi/2019/08/05/asr-kaldi-feat2.html)\n对训练集和测试集做同样的操作。\n这里我采用的参数是 $num=1 $output_dir=mfcc\n那么结果将会保存在mfcc文件夹下。\ncmvn_test_yesno.ark  cmvn_test_yesno.scp  cmvn_train_yesno.ark  cmvn_train_yesno.scp。\nark文件包含特征向量（用cat打开是乱码），scp文件是关系文件，从发音者到ark文件的对应。\n也可以直接跑脚本：\n```\nnum=1 \noutput_dir=mfcc\nfor x in train_yesno test_yesno; do\n  steps/make_mfcc.sh --nj 1 data/$x exp/make_mfcc/$x mfcc  \n  steps/compute_cmvn_stats.sh data/$x exp/make_mfcc/$x mfcc        \n  utils/fix_data_dir.sh data/$x \ndone \n```\n## 单音素模型训练\n```\nsteps/train_mono.sh —nj $N —cmd $MAIN_CMD $DATA_DIR $LANG_DIR $OUTPUT_DIR\n```\n参数说明：\n—nj：job的数量，来自同一个speaker的语音不能并行处理。所以在本项目中只能选择为1。\n—cmd：为了使用本机的资源，调用”utils/run.pl”\n\n运行脚本：\n```\ntrain_cmd=\"utils/run.pl\" steps/train_mono.sh --nj 1 --cmd \"$train_cmd\" \\   \n--totgauss 400 \\   \ndata/train_yesno data/lang exp/mono0a\n```\n到现在我们已经完成了模型的训练。\n\n## 解码和测试\n\n接下来用测试集来验证一下模型的准确与否。\n第一步是创建一个全连接的FST网络。\n```\nutils/mkgraph.sh data/lang_test_tg exp/mono0a exp/mono0a/graph_tgpr\n```\n这个指令背后的脚本很长。基本上做的事情是创建以一个HCLG（HMM+Context+Lexicon+Grammer）的解码器并保存在exp/mono0a/graph_tgpr中。\n还记得我们的每一条语音都是8个连续的Ken或Lo吗(Ken，Lo是希伯来语的yes no）?训练好的模型的工作就是找出这8个词的顺序。\n`steps/decode.sh [options] <graph-dir> <data-dir> <decode-dir>`用来寻找每一个测试音频的最佳路径\n\n```\ndecode_cmd=\"utils/run.pl\" steps/decode.sh --nj 1 --cmd \"$decode_cmd\" \\\nexp/mono0a/graph_tgpr data/test_yesno exp/mono0a/decode_test_yesno \n```\n最后是查看结果的环节。\n在decode.sh内部最后会调用score.sh，这个脚本则生成预测的结果并且计算测试集Word error rate（WER）。  \n调用下列命令可以看到最好的效果：\n```\nfor x in exp/*/decode*; do [ -d $x ] && grep WER $x/wer_* | utils/best_wer.sh;\ndone\n```\n```\n%WER 0.00 [ 0 / 232, 0 ins, 0 del, 0 sub ] exp/mono0a/decode_test_yesno/wer_10_0.0\n```\n解读一下结果，花了很长时间才弄懂这些东西是什么意思。\nWER后跟着的0.00是说字的错误率为0，即准确率为100%。\n测试集一共29条音频，每条音频有8个字（单音素的字）。一共232个字。\n参考Stanford的cs224s-17.lec04.pdf\n\n![WER的计算方法](https://upload-images.jianshu.io/upload_images/4328467-dc5c0fcaf24b3750.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n![](https://upload-images.jianshu.io/upload_images/4328467-1c85e0672f7772cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n而wer结果文件中的10和0.0分别是lmwt(Language Weight)和wip(word insertion penalty，词插入惩罚)。lmwt用来平衡LM（语言模型）在多大程度上帮助AM（语音模型）![LMWT用途 \nhttps://skemman.is/bitstream/1946/31280/1/msc_anna_vigdis_2018.pdf](https://upload-images.jianshu.io/upload_images/4328467-3e2341231bb987fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n至于[wip](https://www.zhihu.com/question/31416764/answer/353868805)：\n> word insertion penalty, 简写WIP, 是HMM识别匹配过程中用于设置句长的一个参数，可以用来调节生成句子中的单词个数，当前主流的语音识别系统主要采用的都是音素识别，即根据单词的音标而不是单词来进行匹配，这就导致了，在识别过程中，可能很难确定单词的gap，如果让系统自由识别，根据参数初始化的模型来进行匹配的话有可能会生成一些诡异的由长单词构成的句子，或者有很多短单词构成的句子，这些匹配率很低的句子对HMM参数的优化作用很小，同时也很大概率会导致学习速率奇慢或者局部最优解这样的问题，所以通过设置这个参数来得到一些更符合语义结构的生成片段。\n\n最后，我们调用的所有脚本都在run.sh中。","source":"_posts/Kaldi入门-一-yesno项目.md","raw":"---\ntitle: 'Kaldi入门(一):yesno项目'\ndate: 2020-05-01 17:37:05\ntags: \n- Kaldi\n- ASR\n---\n\n这个学期选了一门自然语言处理课，结果这门课主要的研究课题是自动语音识别（ASR）。既然入了这个坑。就先好好了解一下如何做ASR吧。\n老师Tom Ko要求使用Kaldi这个工具来做ASR。课上到一半才知道Kaldi中有几千行的脚本代码是老师提交的。好吧，脚本好难的。\n为了入门Kaldi，课程的第5次Lab是一个mini projec: yesno\n\n首先要下载并编译Kaldi，安装的过程不是我的学习重点，可以先参考[Kaldi的下载安装与编译]([https://blog.csdn.net/snowdroptulip/article/details/78896915](https://blog.csdn.net/snowdroptulip/article/details/78896915)\n)，在漫长的编译过程之后假设已经安装好了Kaldi。\n\n## 项目目录结构\nyesno项目的脚本和README都在`kaldi/egs/yesno`之下。\nREADME.txt文件中包含数据集描述：\n```shell\nThe \"yesno\" corpus is a very small dataset of recordings of one individual\nsaying yes or no multiple times per recording, in Hebrew.  It is available from\nhttp://www.openslr.org/1.\nIt is mainly included here as an easy way to test out the Kaldi scripts.\n\nThe test set is perfectly recognized at the monophone stage, so the dataset is\nnot exactly challenging.\n\nThe scripts are in s5/.\n```\n数据脚本路径： kaldi/egs/yesno/s5。在下面执行的很多操作都可以直接调用已经写好的脚本来执行，之所以深入到具体的流程中是为了加强对ASR流程的理解。\n\n## 下载数据集\n第一步是从网络上下载数据集文件`waves_yesno.tar.gz`到s5/路径下并解压。\n原始的数据是60个.wav文件。文件名是八个用下划线分隔的01组合。需要将音频数据转化成kaldi能够处理的格式。\n\n\n## 转换成Kaldi能处理的格式\n下载完数据集后，将数据集划分为31个训练，30个测试（数量大致相当）。在s5/下创建data文件夹，把划分好的音频文件放入train_yesno和test_yesno。\n\nKaldi使用以下几个文件来表示数据：\n1. Text\n音频的文本记录。每一个音频文件一行。格式为`<utt_id> <transcript>`。<utt_id>为音频的id，一般用不带扩展名的文件名表示。utt_id在wav.scp文件中与具体的文件映射。<transcript>是音频对应的文字。\n2. wav.scp\n将文件映射到唯一的utt_id。\n格式为`<utt_id> <path or command to get wave file>`\n第二个参数既可以是对应utt_id的音频文件路径，也可以是能够获得音频文件的指令。\n3. utt2spk\n对于每一个音频文件，标记是哪一个人发音的。因为yesno数据集中只有一个发音者，用global来表示所有的utt_id\n文件内每一行的格式为`<utt_id> <speaker_id>`\n\n4. spk2utt\n和3反过来。文件内每一行对应一个发音者，第一个是speaker的id，后面用空格分隔开60个utt_id。格式为`<speaker_id> <utt_id1> <utt_id2> ...`\n\n本步骤可直接调用脚本：\n```bash\ncd kaldi/egs/yesno/s5\nlocal/prepare_data.sh waves_yesno\n```\n读了一下prepare_data.sh的脚本\n```bash\n#!/usr/bin/env bash\n\nmkdir -p data/local\nlocal=`pwd`/local\nscripts=`pwd`/scripts\n\nexport PATH=$PATH:`pwd`/../../../tools/irstlm/bin\n\necho \"Preparing train and test data\"\n\ntrain_base_name=train_yesno\ntest_base_name=test_yesno\nwaves_dir=$1\n\nls -1 $waves_dir > data/local/waves_all.list\n\ncd data/local\n\n../../local/create_yesno_waves_test_train.pl waves_all.list waves.test waves.train\n\n../../local/create_yesno_wav_scp.pl ${waves_dir} waves.test > ${test_base_name}_wav.scp\n\n../../local/create_yesno_wav_scp.pl ${waves_dir} waves.train > ${train_base_name}_wav.scp\n\n../../local/create_yesno_txt.pl waves.test > ${test_base_name}.txt\n\n../../local/create_yesno_txt.pl waves.train > ${train_base_name}.txt\n\ncp ../../input/task.arpabo lm_tg.arpa\n\ncd ../..\n\n# This stage was copied from WSJ example\nfor x in train_yesno test_yesno; do\n  mkdir -p data/$x\n  cp data/local/${x}_wav.scp data/$x/wav.scp\n  cp data/local/$x.txt data/$x/text\n  cat data/$x/text | awk '{printf(\"%s global\\n\", $1);}' > data/$x/utt2spk\n  utils/utt2spk_to_spk2utt.pl <data/$x/utt2spk >data/$x/spk2utt\ndone\n```\n## 建立词典\n\n\n对于当前项目，我们只有两个词 Ken(yes) 和 Lo（no)。但是在真实的语言中，词的数量不可能这么少，并且还有停顿和环境噪声。kaldi将这些非语言的声音称作slience（SIL）。\n加上SIL一共需要三个词来表示当前这个yesno语言模型。\n\n调用脚本：\n```bash\nlocal/prepare_dict.sh\n```\n将会在s5/data/local/dict中看到新生成的5个文件。\n1. lexicon.txt\n```\n<SIL> SIL\nYES Y\nNO N\n```\n2. lexicon_words.txt\n比1少第一行\n3. nonsilence_phones.txt\n```\nY\nN\n```\n4. silence_phones.txt\n```\nSIL\n```\n5. optional_silence.txt\n和4一样\n\n这个脚本本身只是将input文件夹下面的lexicon_nosil.txt，lexicon.txt， phones.txt复制到dict所在目录，并且加上SIL。\n\n\n## 语言模型\n\n接下来要做语言模型。\n项目提供了一个一元的语言模型。然而我们需要将这个模型转换成一个WFST（一种有穷自动机）\n执行:\n ```bash\nutils/prepare_lang.sh --position-dependent-phones false data/local/dict/ \"<SIL>\" data/local/lang/ data/lang\nlocal/prepare_lm.sh\n ```\n\nprepare_lang.sh的开头注释如下：\n```\n# This script prepares a directory such as data/lang/, in the standard format,\n# given a source directory containing a dictionary lexicon.txt in a form like:\n# word phone1 phone2 ... phoneN\n# per line (alternate prons would be separate lines), or a dictionary with probabilities\n# called lexiconp.txt in a form:\n# word pron-prob phone1 phone2 ... phoneN\n# (with 0.0 < pron-prob <= 1.0); note: if lexiconp.txt exists, we use it even if\n# lexicon.txt exists.\n# and also files silence_phones.txt, nonsilence_phones.txt, optional_silence.txt\n# and extra_questions.txt\n# Here, silence_phones.txt and nonsilence_phones.txt are lists of silence and\n# non-silence phones respectively (where silence includes various kinds of\n# noise, laugh, cough, filled pauses etc., and nonsilence phones includes the\n# \"real\" phones.)\n# In each line of those files is a list of phones, and the phones on each line\n# are assumed to correspond to the same \"base phone\", i.e. they will be\n# different stress or tone variations of the same basic phone.\n# The file \"optional_silence.txt\" contains just a single phone (typically SIL)\n# which is used for optional silence in the lexicon.\n# extra_questions.txt might be empty; typically will consist of lists of phones,\n# all members of each list with the same stress or tone; and also possibly a\n# list for the silence phones.  This will augment the automatically generated\n# questions (note: the automatically generated ones will treat all the\n# stress/tone versions of a phone the same, so will not \"get to ask\" about\n# stress or tone).\n```\n通过阅读脚本和脚本中的注释。可以知道`prepare_lang.sh`的用法\n```\nUsage: utils/prepare_lang.sh <dict-src-dir> <oov-dict-entry> <tmp-dir> <lang-dir>\ne.g.: utils/prepare_lang.sh data/local/dict <SPOKEN_NOISE> data/local/lang data/lang\n--position-dependent-phones (true|false)        # default: true; if true, use _B, _E, _S & _I\n```\n<dict-src-dir>是我们在上一部分生成的词典目录。需要包含lexico.txt，extra_questions.txt，nonsilence_phones.txt，optional_silence.txt  silence_phones.txt。\n`position_dependent_phones`参数为false，导致解码后不能算出单词边界。很多的脚本，特别是评分脚本将不能正常运行。\n第二个指令将语言模型转换成G.fst格式并保存在data/lang_test_tg 目录下。\n这个脚本的核心内容是调用了arpa2fst和fstisstochastic，再创建了G.fst之后检查是否有空字符（<s>,</s>之类的）的循环。\n```\n  arpa2fst --disambig-symbol=#0 --read-symbol-table=$test/words.txt input/task.arpabo $test/G.fst\n\n  fstisstochastic $test/G.fst\n```\n[arpa2fst 原理详解](https://blog.csdn.net/yutianzuijin/article/details/78756130)\n> arpa文件可以很容易地表示任意n-gram语言模型，不过在实际中n通常等于3、4或者5。arpa文件的每一行表示一个文法项，它通常包含三部分内容：probability word(s) [backoff probability]。probability表示该词或词组发生的概率，word(s)表示具体的词或者词组。backoff probablitiy是可选项，表示回退概率。\n\n在yesno这个toy project中只使用1元的语言模型。对应的arpa文件在`input/task.arpabo`\n```\n\\data\\\nngram 1=4\n\n\\1-grams:\n-1\tNO\n-1\tYES\n-99 <s>\n-1 </s>\n\n\\end\\\n```\nfstisstochastic命令则如同其名字的含义一样，用来检查G.fst是否是随机的。\n在`s5/data/lang`目录下会出现：\n1. `phones.txt`:将phone转换成数字。其中#0，#1是空字，用来表示句子的开头和结尾。<eps> 是一个特别的含义表示这个弧上没有符号。\n```\n<eps> 0\nSIL 1\nY 2\nN 3\n#0 4\n#1 5\n```\n2. `words.txt`\n```\n<eps> 0\n<SIL> 1\nNO 2\nYES 3\n#0 4\n<s> 5\n</s> 6\n```\n3. `L_disambig.fst, L.fs`t: the dict can be recognized by Kaldi\n4. `topo`: phone states transition(HMM)\n5. `oov`: out of vocabulary. 仅包含`<SIL>`\n6. ` phones`: some information about phones\n\n## 特征提取和训练\nMFCC特征提取和GMM-HMM建模\n\n提取梅尔倒谱系数\n```\nsteps/make_mfcc.sh --nj $num $input_dir $log_dir $output_dir\n```\n[语音信号处理（二）—— MFCC详解](https://zhuanlan.zhihu.com/p/60371062)\n梅尔倒谱系数是一种非线性的时频表示法，其应用了人耳对低频声音的听觉敏感度更高的原理。\n\n接着正则化倒谱特征\n```\nsteps/compute_cmvn_stats.sh \nutils/fix_data_dir.sh $input_dir\n```\n[Kaldi中的特征提取(二）- 特征变换](http://placebokkk.github.io/kaldi/2019/08/05/asr-kaldi-feat2.html)\n对训练集和测试集做同样的操作。\n这里我采用的参数是 $num=1 $output_dir=mfcc\n那么结果将会保存在mfcc文件夹下。\ncmvn_test_yesno.ark  cmvn_test_yesno.scp  cmvn_train_yesno.ark  cmvn_train_yesno.scp。\nark文件包含特征向量（用cat打开是乱码），scp文件是关系文件，从发音者到ark文件的对应。\n也可以直接跑脚本：\n```\nnum=1 \noutput_dir=mfcc\nfor x in train_yesno test_yesno; do\n  steps/make_mfcc.sh --nj 1 data/$x exp/make_mfcc/$x mfcc  \n  steps/compute_cmvn_stats.sh data/$x exp/make_mfcc/$x mfcc        \n  utils/fix_data_dir.sh data/$x \ndone \n```\n## 单音素模型训练\n```\nsteps/train_mono.sh —nj $N —cmd $MAIN_CMD $DATA_DIR $LANG_DIR $OUTPUT_DIR\n```\n参数说明：\n—nj：job的数量，来自同一个speaker的语音不能并行处理。所以在本项目中只能选择为1。\n—cmd：为了使用本机的资源，调用”utils/run.pl”\n\n运行脚本：\n```\ntrain_cmd=\"utils/run.pl\" steps/train_mono.sh --nj 1 --cmd \"$train_cmd\" \\   \n--totgauss 400 \\   \ndata/train_yesno data/lang exp/mono0a\n```\n到现在我们已经完成了模型的训练。\n\n## 解码和测试\n\n接下来用测试集来验证一下模型的准确与否。\n第一步是创建一个全连接的FST网络。\n```\nutils/mkgraph.sh data/lang_test_tg exp/mono0a exp/mono0a/graph_tgpr\n```\n这个指令背后的脚本很长。基本上做的事情是创建以一个HCLG（HMM+Context+Lexicon+Grammer）的解码器并保存在exp/mono0a/graph_tgpr中。\n还记得我们的每一条语音都是8个连续的Ken或Lo吗(Ken，Lo是希伯来语的yes no）?训练好的模型的工作就是找出这8个词的顺序。\n`steps/decode.sh [options] <graph-dir> <data-dir> <decode-dir>`用来寻找每一个测试音频的最佳路径\n\n```\ndecode_cmd=\"utils/run.pl\" steps/decode.sh --nj 1 --cmd \"$decode_cmd\" \\\nexp/mono0a/graph_tgpr data/test_yesno exp/mono0a/decode_test_yesno \n```\n最后是查看结果的环节。\n在decode.sh内部最后会调用score.sh，这个脚本则生成预测的结果并且计算测试集Word error rate（WER）。  \n调用下列命令可以看到最好的效果：\n```\nfor x in exp/*/decode*; do [ -d $x ] && grep WER $x/wer_* | utils/best_wer.sh;\ndone\n```\n```\n%WER 0.00 [ 0 / 232, 0 ins, 0 del, 0 sub ] exp/mono0a/decode_test_yesno/wer_10_0.0\n```\n解读一下结果，花了很长时间才弄懂这些东西是什么意思。\nWER后跟着的0.00是说字的错误率为0，即准确率为100%。\n测试集一共29条音频，每条音频有8个字（单音素的字）。一共232个字。\n参考Stanford的cs224s-17.lec04.pdf\n\n![WER的计算方法](https://upload-images.jianshu.io/upload_images/4328467-dc5c0fcaf24b3750.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n![](https://upload-images.jianshu.io/upload_images/4328467-1c85e0672f7772cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n而wer结果文件中的10和0.0分别是lmwt(Language Weight)和wip(word insertion penalty，词插入惩罚)。lmwt用来平衡LM（语言模型）在多大程度上帮助AM（语音模型）![LMWT用途 \nhttps://skemman.is/bitstream/1946/31280/1/msc_anna_vigdis_2018.pdf](https://upload-images.jianshu.io/upload_images/4328467-3e2341231bb987fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n至于[wip](https://www.zhihu.com/question/31416764/answer/353868805)：\n> word insertion penalty, 简写WIP, 是HMM识别匹配过程中用于设置句长的一个参数，可以用来调节生成句子中的单词个数，当前主流的语音识别系统主要采用的都是音素识别，即根据单词的音标而不是单词来进行匹配，这就导致了，在识别过程中，可能很难确定单词的gap，如果让系统自由识别，根据参数初始化的模型来进行匹配的话有可能会生成一些诡异的由长单词构成的句子，或者有很多短单词构成的句子，这些匹配率很低的句子对HMM参数的优化作用很小，同时也很大概率会导致学习速率奇慢或者局部最优解这样的问题，所以通过设置这个参数来得到一些更符合语义结构的生成片段。\n\n最后，我们调用的所有脚本都在run.sh中。","slug":"Kaldi入门-一-yesno项目","published":1,"updated":"2020-11-21T07:26:16.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckhub1txd0014p92r04xwpri6","content":"<p>这个学期选了一门自然语言处理课，结果这门课主要的研究课题是自动语音识别（ASR）。既然入了这个坑。就先好好了解一下如何做ASR吧。<br>老师Tom Ko要求使用Kaldi这个工具来做ASR。课上到一半才知道Kaldi中有几千行的脚本代码是老师提交的。好吧，脚本好难的。<br>为了入门Kaldi，课程的第5次Lab是一个mini projec: yesno</p>\n<p>首先要下载并编译Kaldi，安装的过程不是我的学习重点，可以先参考<a href=\"[https://blog.csdn.net/snowdroptulip/article/details/78896915](https://blog.csdn.net/snowdroptulip/article/details/78896915)\">Kaldi的下载安装与编译</a>，在漫长的编译过程之后假设已经安装好了Kaldi。</p>\n<h2 id=\"项目目录结构\"><a href=\"#项目目录结构\" class=\"headerlink\" title=\"项目目录结构\"></a>项目目录结构</h2><p>yesno项目的脚本和README都在<code>kaldi/egs/yesno</code>之下。<br>README.txt文件中包含数据集描述：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">The \"yesno\" corpus is a very small dataset of recordings of one individual</span><br><span class=\"line\">saying yes or no multiple times per recording, in Hebrew.  It is available from</span><br><span class=\"line\">http://www.openslr.org/1.</span><br><span class=\"line\">It is mainly included here as an easy way to test out the Kaldi scripts.</span><br><span class=\"line\"></span><br><span class=\"line\">The test set is perfectly recognized at the monophone stage, so the dataset is</span><br><span class=\"line\">not exactly challenging.</span><br><span class=\"line\"></span><br><span class=\"line\">The scripts are in s5/.</span><br></pre></td></tr></table></figure>\n\n<p>数据脚本路径： kaldi/egs/yesno/s5。在下面执行的很多操作都可以直接调用已经写好的脚本来执行，之所以深入到具体的流程中是为了加强对ASR流程的理解。</p>\n<h2 id=\"下载数据集\"><a href=\"#下载数据集\" class=\"headerlink\" title=\"下载数据集\"></a>下载数据集</h2><p>第一步是从网络上下载数据集文件<code>waves_yesno.tar.gz</code>到s5/路径下并解压。<br>原始的数据是60个.wav文件。文件名是八个用下划线分隔的01组合。需要将音频数据转化成kaldi能够处理的格式。</p>\n<h2 id=\"转换成Kaldi能处理的格式\"><a href=\"#转换成Kaldi能处理的格式\" class=\"headerlink\" title=\"转换成Kaldi能处理的格式\"></a>转换成Kaldi能处理的格式</h2><p>下载完数据集后，将数据集划分为31个训练，30个测试（数量大致相当）。在s5/下创建data文件夹，把划分好的音频文件放入train_yesno和test_yesno。</p>\n<p>Kaldi使用以下几个文件来表示数据：</p>\n<ol>\n<li><p>Text<br>音频的文本记录。每一个音频文件一行。格式为<code>&lt;utt_id&gt; &lt;transcript&gt;</code>。<utt_id>为音频的id，一般用不带扩展名的文件名表示。utt_id在wav.scp文件中与具体的文件映射。<transcript>是音频对应的文字。</transcript></utt_id></p>\n</li>\n<li><p>wav.scp<br>将文件映射到唯一的utt_id。<br>格式为<code>&lt;utt_id&gt; &lt;path or command to get wave file&gt;</code><br>第二个参数既可以是对应utt_id的音频文件路径，也可以是能够获得音频文件的指令。</p>\n</li>\n<li><p>utt2spk<br>对于每一个音频文件，标记是哪一个人发音的。因为yesno数据集中只有一个发音者，用global来表示所有的utt_id<br>文件内每一行的格式为<code>&lt;utt_id&gt; &lt;speaker_id&gt;</code></p>\n</li>\n<li><p>spk2utt<br>和3反过来。文件内每一行对应一个发音者，第一个是speaker的id，后面用空格分隔开60个utt_id。格式为<code>&lt;speaker_id&gt; &lt;utt_id1&gt; &lt;utt_id2&gt; ...</code></p>\n</li>\n</ol>\n<p>本步骤可直接调用脚本：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> kaldi/egs/yesno/s5</span><br><span class=\"line\"><span class=\"built_in\">local</span>/prepare_data.sh waves_yesno</span><br></pre></td></tr></table></figure>\n\n<p>读了一下prepare_data.sh的脚本</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/usr/bin/env bash</span></span><br><span class=\"line\"></span><br><span class=\"line\">mkdir -p data/<span class=\"built_in\">local</span></span><br><span class=\"line\"><span class=\"built_in\">local</span>=`<span class=\"built_in\">pwd</span>`/<span class=\"built_in\">local</span></span><br><span class=\"line\">scripts=`<span class=\"built_in\">pwd</span>`/scripts</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:`<span class=\"built_in\">pwd</span>`/../../../tools/irstlm/bin</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"Preparing train and test data\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_base_name=train_yesno</span><br><span class=\"line\">test_base_name=test_yesno</span><br><span class=\"line\">waves_dir=<span class=\"variable\">$1</span></span><br><span class=\"line\"></span><br><span class=\"line\">ls -1 <span class=\"variable\">$waves_dir</span> &gt; data/<span class=\"built_in\">local</span>/waves_all.list</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">cd</span> data/<span class=\"built_in\">local</span></span><br><span class=\"line\"></span><br><span class=\"line\">../../<span class=\"built_in\">local</span>/create_yesno_waves_test_train.pl waves_all.list waves.test waves.train</span><br><span class=\"line\"></span><br><span class=\"line\">../../<span class=\"built_in\">local</span>/create_yesno_wav_scp.pl <span class=\"variable\">$&#123;waves_dir&#125;</span> waves.test &gt; <span class=\"variable\">$&#123;test_base_name&#125;</span>_wav.scp</span><br><span class=\"line\"></span><br><span class=\"line\">../../<span class=\"built_in\">local</span>/create_yesno_wav_scp.pl <span class=\"variable\">$&#123;waves_dir&#125;</span> waves.train &gt; <span class=\"variable\">$&#123;train_base_name&#125;</span>_wav.scp</span><br><span class=\"line\"></span><br><span class=\"line\">../../<span class=\"built_in\">local</span>/create_yesno_txt.pl waves.test &gt; <span class=\"variable\">$&#123;test_base_name&#125;</span>.txt</span><br><span class=\"line\"></span><br><span class=\"line\">../../<span class=\"built_in\">local</span>/create_yesno_txt.pl waves.train &gt; <span class=\"variable\">$&#123;train_base_name&#125;</span>.txt</span><br><span class=\"line\"></span><br><span class=\"line\">cp ../../input/task.arpabo lm_tg.arpa</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">cd</span> ../..</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># This stage was copied from WSJ example</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> train_yesno test_yesno; <span class=\"keyword\">do</span></span><br><span class=\"line\">  mkdir -p data/<span class=\"variable\">$x</span></span><br><span class=\"line\">  cp data/<span class=\"built_in\">local</span>/<span class=\"variable\">$&#123;x&#125;</span>_wav.scp data/<span class=\"variable\">$x</span>/wav.scp</span><br><span class=\"line\">  cp data/<span class=\"built_in\">local</span>/<span class=\"variable\">$x</span>.txt data/<span class=\"variable\">$x</span>/text</span><br><span class=\"line\">  cat data/<span class=\"variable\">$x</span>/text | awk <span class=\"string\">'&#123;printf(\"%s global\\n\", $1);&#125;'</span> &gt; data/<span class=\"variable\">$x</span>/utt2spk</span><br><span class=\"line\">  utils/utt2spk_to_spk2utt.pl &lt;data/<span class=\"variable\">$x</span>/utt2spk &gt;data/<span class=\"variable\">$x</span>/spk2utt</span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"建立词典\"><a href=\"#建立词典\" class=\"headerlink\" title=\"建立词典\"></a>建立词典</h2><p>对于当前项目，我们只有两个词 Ken(yes) 和 Lo（no)。但是在真实的语言中，词的数量不可能这么少，并且还有停顿和环境噪声。kaldi将这些非语言的声音称作slience（SIL）。<br>加上SIL一共需要三个词来表示当前这个yesno语言模型。</p>\n<p>调用脚本：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">local</span>/prepare_dict.sh</span><br></pre></td></tr></table></figure>\n\n<p>将会在s5/data/local/dict中看到新生成的5个文件。</p>\n<ol>\n<li><p>lexicon.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;SIL&gt; SIL</span><br><span class=\"line\">YES Y</span><br><span class=\"line\">NO N</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>lexicon_words.txt<br>比1少第一行</p>\n</li>\n<li><p>nonsilence_phones.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Y</span><br><span class=\"line\">N</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>silence_phones.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SIL</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>optional_silence.txt<br>和4一样</p>\n</li>\n</ol>\n<p>这个脚本本身只是将input文件夹下面的lexicon_nosil.txt，lexicon.txt， phones.txt复制到dict所在目录，并且加上SIL。</p>\n<h2 id=\"语言模型\"><a href=\"#语言模型\" class=\"headerlink\" title=\"语言模型\"></a>语言模型</h2><p>接下来要做语言模型。<br>项目提供了一个一元的语言模型。然而我们需要将这个模型转换成一个WFST（一种有穷自动机）<br>执行:<br> <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">utils/prepare_lang.sh --position-dependent-phones <span class=\"literal\">false</span> data/<span class=\"built_in\">local</span>/dict/ <span class=\"string\">\"&lt;SIL&gt;\"</span> data/<span class=\"built_in\">local</span>/lang/ data/lang</span><br><span class=\"line\"><span class=\"built_in\">local</span>/prepare_lm.sh</span><br></pre></td></tr></table></figure></p>\n<p>prepare_lang.sh的开头注释如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># This script prepares a directory such as data/lang/, in the standard format,</span><br><span class=\"line\"># given a source directory containing a dictionary lexicon.txt in a form like:</span><br><span class=\"line\"># word phone1 phone2 ... phoneN</span><br><span class=\"line\"># per line (alternate prons would be separate lines), or a dictionary with probabilities</span><br><span class=\"line\"># called lexiconp.txt in a form:</span><br><span class=\"line\"># word pron-prob phone1 phone2 ... phoneN</span><br><span class=\"line\"># (with 0.0 &lt; pron-prob &lt;= 1.0); note: if lexiconp.txt exists, we use it even if</span><br><span class=\"line\"># lexicon.txt exists.</span><br><span class=\"line\"># and also files silence_phones.txt, nonsilence_phones.txt, optional_silence.txt</span><br><span class=\"line\"># and extra_questions.txt</span><br><span class=\"line\"># Here, silence_phones.txt and nonsilence_phones.txt are lists of silence and</span><br><span class=\"line\"># non-silence phones respectively (where silence includes various kinds of</span><br><span class=\"line\"># noise, laugh, cough, filled pauses etc., and nonsilence phones includes the</span><br><span class=\"line\"># &quot;real&quot; phones.)</span><br><span class=\"line\"># In each line of those files is a list of phones, and the phones on each line</span><br><span class=\"line\"># are assumed to correspond to the same &quot;base phone&quot;, i.e. they will be</span><br><span class=\"line\"># different stress or tone variations of the same basic phone.</span><br><span class=\"line\"># The file &quot;optional_silence.txt&quot; contains just a single phone (typically SIL)</span><br><span class=\"line\"># which is used for optional silence in the lexicon.</span><br><span class=\"line\"># extra_questions.txt might be empty; typically will consist of lists of phones,</span><br><span class=\"line\"># all members of each list with the same stress or tone; and also possibly a</span><br><span class=\"line\"># list for the silence phones.  This will augment the automatically generated</span><br><span class=\"line\"># questions (note: the automatically generated ones will treat all the</span><br><span class=\"line\"># stress/tone versions of a phone the same, so will not &quot;get to ask&quot; about</span><br><span class=\"line\"># stress or tone).</span><br></pre></td></tr></table></figure>\n\n<p>通过阅读脚本和脚本中的注释。可以知道<code>prepare_lang.sh</code>的用法</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Usage: utils/prepare_lang.sh &lt;dict-src-dir&gt; &lt;oov-dict-entry&gt; &lt;tmp-dir&gt; &lt;lang-dir&gt;</span><br><span class=\"line\">e.g.: utils/prepare_lang.sh data/local/dict &lt;SPOKEN_NOISE&gt; data/local/lang data/lang</span><br><span class=\"line\">--position-dependent-phones (true|false)        # default: true; if true, use _B, _E, _S &amp; _I</span><br></pre></td></tr></table></figure>\n\n<p><dict-src-dir>是我们在上一部分生成的词典目录。需要包含lexico.txt，extra_questions.txt，nonsilence_phones.txt，optional_silence.txt  silence_phones.txt。<br><code>position_dependent_phones</code>参数为false，导致解码后不能算出单词边界。很多的脚本，特别是评分脚本将不能正常运行。<br>第二个指令将语言模型转换成G.fst格式并保存在data/lang_test_tg 目录下。<br>这个脚本的核心内容是调用了arpa2fst和fstisstochastic，再创建了G.fst之后检查是否有空字符（<s>,</s>之类的）的循环。</dict-src-dir></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">arpa2fst --disambig-symbol=#0 --read-symbol-table=$test/words.txt input/task.arpabo $test/G.fst</span><br><span class=\"line\"></span><br><span class=\"line\">fstisstochastic $test/G.fst</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://blog.csdn.net/yutianzuijin/article/details/78756130\" target=\"_blank\" rel=\"noopener\">arpa2fst 原理详解</a></p>\n<blockquote>\n<p>arpa文件可以很容易地表示任意n-gram语言模型，不过在实际中n通常等于3、4或者5。arpa文件的每一行表示一个文法项，它通常包含三部分内容：probability word(s) [backoff probability]。probability表示该词或词组发生的概率，word(s)表示具体的词或者词组。backoff probablitiy是可选项，表示回退概率。</p>\n</blockquote>\n<p>在yesno这个toy project中只使用1元的语言模型。对应的arpa文件在<code>input/task.arpabo</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\data\\</span><br><span class=\"line\">ngram 1=4</span><br><span class=\"line\"></span><br><span class=\"line\">\\1-grams:</span><br><span class=\"line\">-1\tNO</span><br><span class=\"line\">-1\tYES</span><br><span class=\"line\">-99 &lt;s&gt;</span><br><span class=\"line\">-1 &lt;/s&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\\end\\</span><br></pre></td></tr></table></figure>\n\n<p>fstisstochastic命令则如同其名字的含义一样，用来检查G.fst是否是随机的。<br>在<code>s5/data/lang</code>目录下会出现：</p>\n<ol>\n<li><p><code>phones.txt</code>:将phone转换成数字。其中#0，#1是空字，用来表示句子的开头和结尾。<eps> 是一个特别的含义表示这个弧上没有符号。</eps></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;eps&gt; 0</span><br><span class=\"line\">SIL 1</span><br><span class=\"line\">Y 2</span><br><span class=\"line\">N 3</span><br><span class=\"line\">#0 4</span><br><span class=\"line\">#1 5</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><code>words.txt</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;eps&gt; 0</span><br><span class=\"line\">&lt;SIL&gt; 1</span><br><span class=\"line\">NO 2</span><br><span class=\"line\">YES 3</span><br><span class=\"line\">#0 4</span><br><span class=\"line\">&lt;s&gt; 5</span><br><span class=\"line\">&lt;/s&gt; 6</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><code>L_disambig.fst, L.fs</code>t: the dict can be recognized by Kaldi</p>\n</li>\n<li><p><code>topo</code>: phone states transition(HMM)</p>\n</li>\n<li><p><code>oov</code>: out of vocabulary. 仅包含<code>&lt;SIL&gt;</code></p>\n</li>\n<li><p><code>phones</code>: some information about phones</p>\n</li>\n</ol>\n<h2 id=\"特征提取和训练\"><a href=\"#特征提取和训练\" class=\"headerlink\" title=\"特征提取和训练\"></a>特征提取和训练</h2><p>MFCC特征提取和GMM-HMM建模</p>\n<p>提取梅尔倒谱系数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">steps/make_mfcc.sh --nj $num $input_dir $log_dir $output_dir</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://zhuanlan.zhihu.com/p/60371062\" target=\"_blank\" rel=\"noopener\">语音信号处理（二）—— MFCC详解</a><br>梅尔倒谱系数是一种非线性的时频表示法，其应用了人耳对低频声音的听觉敏感度更高的原理。</p>\n<p>接着正则化倒谱特征</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">steps/compute_cmvn_stats.sh </span><br><span class=\"line\">utils/fix_data_dir.sh $input_dir</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"http://placebokkk.github.io/kaldi/2019/08/05/asr-kaldi-feat2.html\" target=\"_blank\" rel=\"noopener\">Kaldi中的特征提取(二）- 特征变换</a><br>对训练集和测试集做同样的操作。<br>这里我采用的参数是 $num=1 $output_dir=mfcc<br>那么结果将会保存在mfcc文件夹下。<br>cmvn_test_yesno.ark  cmvn_test_yesno.scp  cmvn_train_yesno.ark  cmvn_train_yesno.scp。<br>ark文件包含特征向量（用cat打开是乱码），scp文件是关系文件，从发音者到ark文件的对应。<br>也可以直接跑脚本：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num=1 </span><br><span class=\"line\">output_dir=mfcc</span><br><span class=\"line\">for x in train_yesno test_yesno; do</span><br><span class=\"line\">  steps/make_mfcc.sh --nj 1 data/$x exp/make_mfcc/$x mfcc  </span><br><span class=\"line\">  steps/compute_cmvn_stats.sh data/$x exp/make_mfcc/$x mfcc        </span><br><span class=\"line\">  utils/fix_data_dir.sh data/$x </span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"单音素模型训练\"><a href=\"#单音素模型训练\" class=\"headerlink\" title=\"单音素模型训练\"></a>单音素模型训练</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">steps/train_mono.sh —nj $N —cmd $MAIN_CMD $DATA_DIR $LANG_DIR $OUTPUT_DIR</span><br></pre></td></tr></table></figure>\n\n<p>参数说明：<br>—nj：job的数量，来自同一个speaker的语音不能并行处理。所以在本项目中只能选择为1。<br>—cmd：为了使用本机的资源，调用”utils/run.pl”</p>\n<p>运行脚本：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_cmd=&quot;utils/run.pl&quot; steps/train_mono.sh --nj 1 --cmd &quot;$train_cmd&quot; \\   </span><br><span class=\"line\">--totgauss 400 \\   </span><br><span class=\"line\">data/train_yesno data/lang exp/mono0a</span><br></pre></td></tr></table></figure>\n\n<p>到现在我们已经完成了模型的训练。</p>\n<h2 id=\"解码和测试\"><a href=\"#解码和测试\" class=\"headerlink\" title=\"解码和测试\"></a>解码和测试</h2><p>接下来用测试集来验证一下模型的准确与否。<br>第一步是创建一个全连接的FST网络。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">utils/mkgraph.sh data/lang_test_tg exp/mono0a exp/mono0a/graph_tgpr</span><br></pre></td></tr></table></figure>\n\n<p>这个指令背后的脚本很长。基本上做的事情是创建以一个HCLG（HMM+Context+Lexicon+Grammer）的解码器并保存在exp/mono0a/graph_tgpr中。<br>还记得我们的每一条语音都是8个连续的Ken或Lo吗(Ken，Lo是希伯来语的yes no）?训练好的模型的工作就是找出这8个词的顺序。<br><code>steps/decode.sh [options] &lt;graph-dir&gt; &lt;data-dir&gt; &lt;decode-dir&gt;</code>用来寻找每一个测试音频的最佳路径</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">decode_cmd=&quot;utils/run.pl&quot; steps/decode.sh --nj 1 --cmd &quot;$decode_cmd&quot; \\</span><br><span class=\"line\">exp/mono0a/graph_tgpr data/test_yesno exp/mono0a/decode_test_yesno</span><br></pre></td></tr></table></figure>\n\n<p>最后是查看结果的环节。<br>在decode.sh内部最后会调用score.sh，这个脚本则生成预测的结果并且计算测试集Word error rate（WER）。<br>调用下列命令可以看到最好的效果：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for x in exp/*/decode*; do [ -d $x ] &amp;&amp; grep WER $x/wer_* | utils/best_wer.sh;</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%WER 0.00 [ 0 / 232, 0 ins, 0 del, 0 sub ] exp/mono0a/decode_test_yesno/wer_10_0.0</span><br></pre></td></tr></table></figure>\n\n<p>解读一下结果，花了很长时间才弄懂这些东西是什么意思。<br>WER后跟着的0.00是说字的错误率为0，即准确率为100%。<br>测试集一共29条音频，每条音频有8个字（单音素的字）。一共232个字。<br>参考Stanford的cs224s-17.lec04.pdf</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/4328467-dc5c0fcaf24b3750.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"WER的计算方法\"><br><img src=\"https://upload-images.jianshu.io/upload_images/4328467-1c85e0672f7772cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt><br>而wer结果文件中的10和0.0分别是lmwt(Language Weight)和wip(word insertion penalty，词插入惩罚)。lmwt用来平衡LM（语言模型）在多大程度上帮助AM（语音模型）<img src=\"https://upload-images.jianshu.io/upload_images/4328467-3e2341231bb987fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"LMWT用途 \nhttps://skemman.is/bitstream/1946/31280/1/msc_anna_vigdis_2018.pdf\"><br>至于<a href=\"https://www.zhihu.com/question/31416764/answer/353868805\" target=\"_blank\" rel=\"noopener\">wip</a>：</p>\n<blockquote>\n<p>word insertion penalty, 简写WIP, 是HMM识别匹配过程中用于设置句长的一个参数，可以用来调节生成句子中的单词个数，当前主流的语音识别系统主要采用的都是音素识别，即根据单词的音标而不是单词来进行匹配，这就导致了，在识别过程中，可能很难确定单词的gap，如果让系统自由识别，根据参数初始化的模型来进行匹配的话有可能会生成一些诡异的由长单词构成的句子，或者有很多短单词构成的句子，这些匹配率很低的句子对HMM参数的优化作用很小，同时也很大概率会导致学习速率奇慢或者局部最优解这样的问题，所以通过设置这个参数来得到一些更符合语义结构的生成片段。</p>\n</blockquote>\n<p>最后，我们调用的所有脚本都在run.sh中。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>这个学期选了一门自然语言处理课，结果这门课主要的研究课题是自动语音识别（ASR）。既然入了这个坑。就先好好了解一下如何做ASR吧。<br>老师Tom Ko要求使用Kaldi这个工具来做ASR。课上到一半才知道Kaldi中有几千行的脚本代码是老师提交的。好吧，脚本好难的。<br>为了入门Kaldi，课程的第5次Lab是一个mini projec: yesno</p>\n<p>首先要下载并编译Kaldi，安装的过程不是我的学习重点，可以先参考<a href=\"[https://blog.csdn.net/snowdroptulip/article/details/78896915](https://blog.csdn.net/snowdroptulip/article/details/78896915)\">Kaldi的下载安装与编译</a>，在漫长的编译过程之后假设已经安装好了Kaldi。</p>\n<h2 id=\"项目目录结构\"><a href=\"#项目目录结构\" class=\"headerlink\" title=\"项目目录结构\"></a>项目目录结构</h2><p>yesno项目的脚本和README都在<code>kaldi/egs/yesno</code>之下。<br>README.txt文件中包含数据集描述：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">The \"yesno\" corpus is a very small dataset of recordings of one individual</span><br><span class=\"line\">saying yes or no multiple times per recording, in Hebrew.  It is available from</span><br><span class=\"line\">http://www.openslr.org/1.</span><br><span class=\"line\">It is mainly included here as an easy way to test out the Kaldi scripts.</span><br><span class=\"line\"></span><br><span class=\"line\">The test set is perfectly recognized at the monophone stage, so the dataset is</span><br><span class=\"line\">not exactly challenging.</span><br><span class=\"line\"></span><br><span class=\"line\">The scripts are in s5/.</span><br></pre></td></tr></table></figure>\n\n<p>数据脚本路径： kaldi/egs/yesno/s5。在下面执行的很多操作都可以直接调用已经写好的脚本来执行，之所以深入到具体的流程中是为了加强对ASR流程的理解。</p>\n<h2 id=\"下载数据集\"><a href=\"#下载数据集\" class=\"headerlink\" title=\"下载数据集\"></a>下载数据集</h2><p>第一步是从网络上下载数据集文件<code>waves_yesno.tar.gz</code>到s5/路径下并解压。<br>原始的数据是60个.wav文件。文件名是八个用下划线分隔的01组合。需要将音频数据转化成kaldi能够处理的格式。</p>\n<h2 id=\"转换成Kaldi能处理的格式\"><a href=\"#转换成Kaldi能处理的格式\" class=\"headerlink\" title=\"转换成Kaldi能处理的格式\"></a>转换成Kaldi能处理的格式</h2><p>下载完数据集后，将数据集划分为31个训练，30个测试（数量大致相当）。在s5/下创建data文件夹，把划分好的音频文件放入train_yesno和test_yesno。</p>\n<p>Kaldi使用以下几个文件来表示数据：</p>\n<ol>\n<li><p>Text<br>音频的文本记录。每一个音频文件一行。格式为<code>&lt;utt_id&gt; &lt;transcript&gt;</code>。<utt_id>为音频的id，一般用不带扩展名的文件名表示。utt_id在wav.scp文件中与具体的文件映射。<transcript>是音频对应的文字。</transcript></utt_id></p>\n</li>\n<li><p>wav.scp<br>将文件映射到唯一的utt_id。<br>格式为<code>&lt;utt_id&gt; &lt;path or command to get wave file&gt;</code><br>第二个参数既可以是对应utt_id的音频文件路径，也可以是能够获得音频文件的指令。</p>\n</li>\n<li><p>utt2spk<br>对于每一个音频文件，标记是哪一个人发音的。因为yesno数据集中只有一个发音者，用global来表示所有的utt_id<br>文件内每一行的格式为<code>&lt;utt_id&gt; &lt;speaker_id&gt;</code></p>\n</li>\n<li><p>spk2utt<br>和3反过来。文件内每一行对应一个发音者，第一个是speaker的id，后面用空格分隔开60个utt_id。格式为<code>&lt;speaker_id&gt; &lt;utt_id1&gt; &lt;utt_id2&gt; ...</code></p>\n</li>\n</ol>\n<p>本步骤可直接调用脚本：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> kaldi/egs/yesno/s5</span><br><span class=\"line\"><span class=\"built_in\">local</span>/prepare_data.sh waves_yesno</span><br></pre></td></tr></table></figure>\n\n<p>读了一下prepare_data.sh的脚本</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/usr/bin/env bash</span></span><br><span class=\"line\"></span><br><span class=\"line\">mkdir -p data/<span class=\"built_in\">local</span></span><br><span class=\"line\"><span class=\"built_in\">local</span>=`<span class=\"built_in\">pwd</span>`/<span class=\"built_in\">local</span></span><br><span class=\"line\">scripts=`<span class=\"built_in\">pwd</span>`/scripts</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:`<span class=\"built_in\">pwd</span>`/../../../tools/irstlm/bin</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"Preparing train and test data\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_base_name=train_yesno</span><br><span class=\"line\">test_base_name=test_yesno</span><br><span class=\"line\">waves_dir=<span class=\"variable\">$1</span></span><br><span class=\"line\"></span><br><span class=\"line\">ls -1 <span class=\"variable\">$waves_dir</span> &gt; data/<span class=\"built_in\">local</span>/waves_all.list</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">cd</span> data/<span class=\"built_in\">local</span></span><br><span class=\"line\"></span><br><span class=\"line\">../../<span class=\"built_in\">local</span>/create_yesno_waves_test_train.pl waves_all.list waves.test waves.train</span><br><span class=\"line\"></span><br><span class=\"line\">../../<span class=\"built_in\">local</span>/create_yesno_wav_scp.pl <span class=\"variable\">$&#123;waves_dir&#125;</span> waves.test &gt; <span class=\"variable\">$&#123;test_base_name&#125;</span>_wav.scp</span><br><span class=\"line\"></span><br><span class=\"line\">../../<span class=\"built_in\">local</span>/create_yesno_wav_scp.pl <span class=\"variable\">$&#123;waves_dir&#125;</span> waves.train &gt; <span class=\"variable\">$&#123;train_base_name&#125;</span>_wav.scp</span><br><span class=\"line\"></span><br><span class=\"line\">../../<span class=\"built_in\">local</span>/create_yesno_txt.pl waves.test &gt; <span class=\"variable\">$&#123;test_base_name&#125;</span>.txt</span><br><span class=\"line\"></span><br><span class=\"line\">../../<span class=\"built_in\">local</span>/create_yesno_txt.pl waves.train &gt; <span class=\"variable\">$&#123;train_base_name&#125;</span>.txt</span><br><span class=\"line\"></span><br><span class=\"line\">cp ../../input/task.arpabo lm_tg.arpa</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">cd</span> ../..</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># This stage was copied from WSJ example</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> train_yesno test_yesno; <span class=\"keyword\">do</span></span><br><span class=\"line\">  mkdir -p data/<span class=\"variable\">$x</span></span><br><span class=\"line\">  cp data/<span class=\"built_in\">local</span>/<span class=\"variable\">$&#123;x&#125;</span>_wav.scp data/<span class=\"variable\">$x</span>/wav.scp</span><br><span class=\"line\">  cp data/<span class=\"built_in\">local</span>/<span class=\"variable\">$x</span>.txt data/<span class=\"variable\">$x</span>/text</span><br><span class=\"line\">  cat data/<span class=\"variable\">$x</span>/text | awk <span class=\"string\">'&#123;printf(\"%s global\\n\", $1);&#125;'</span> &gt; data/<span class=\"variable\">$x</span>/utt2spk</span><br><span class=\"line\">  utils/utt2spk_to_spk2utt.pl &lt;data/<span class=\"variable\">$x</span>/utt2spk &gt;data/<span class=\"variable\">$x</span>/spk2utt</span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"建立词典\"><a href=\"#建立词典\" class=\"headerlink\" title=\"建立词典\"></a>建立词典</h2><p>对于当前项目，我们只有两个词 Ken(yes) 和 Lo（no)。但是在真实的语言中，词的数量不可能这么少，并且还有停顿和环境噪声。kaldi将这些非语言的声音称作slience（SIL）。<br>加上SIL一共需要三个词来表示当前这个yesno语言模型。</p>\n<p>调用脚本：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">local</span>/prepare_dict.sh</span><br></pre></td></tr></table></figure>\n\n<p>将会在s5/data/local/dict中看到新生成的5个文件。</p>\n<ol>\n<li><p>lexicon.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;SIL&gt; SIL</span><br><span class=\"line\">YES Y</span><br><span class=\"line\">NO N</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>lexicon_words.txt<br>比1少第一行</p>\n</li>\n<li><p>nonsilence_phones.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Y</span><br><span class=\"line\">N</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>silence_phones.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SIL</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>optional_silence.txt<br>和4一样</p>\n</li>\n</ol>\n<p>这个脚本本身只是将input文件夹下面的lexicon_nosil.txt，lexicon.txt， phones.txt复制到dict所在目录，并且加上SIL。</p>\n<h2 id=\"语言模型\"><a href=\"#语言模型\" class=\"headerlink\" title=\"语言模型\"></a>语言模型</h2><p>接下来要做语言模型。<br>项目提供了一个一元的语言模型。然而我们需要将这个模型转换成一个WFST（一种有穷自动机）<br>执行:<br> <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">utils/prepare_lang.sh --position-dependent-phones <span class=\"literal\">false</span> data/<span class=\"built_in\">local</span>/dict/ <span class=\"string\">\"&lt;SIL&gt;\"</span> data/<span class=\"built_in\">local</span>/lang/ data/lang</span><br><span class=\"line\"><span class=\"built_in\">local</span>/prepare_lm.sh</span><br></pre></td></tr></table></figure></p>\n<p>prepare_lang.sh的开头注释如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># This script prepares a directory such as data/lang/, in the standard format,</span><br><span class=\"line\"># given a source directory containing a dictionary lexicon.txt in a form like:</span><br><span class=\"line\"># word phone1 phone2 ... phoneN</span><br><span class=\"line\"># per line (alternate prons would be separate lines), or a dictionary with probabilities</span><br><span class=\"line\"># called lexiconp.txt in a form:</span><br><span class=\"line\"># word pron-prob phone1 phone2 ... phoneN</span><br><span class=\"line\"># (with 0.0 &lt; pron-prob &lt;= 1.0); note: if lexiconp.txt exists, we use it even if</span><br><span class=\"line\"># lexicon.txt exists.</span><br><span class=\"line\"># and also files silence_phones.txt, nonsilence_phones.txt, optional_silence.txt</span><br><span class=\"line\"># and extra_questions.txt</span><br><span class=\"line\"># Here, silence_phones.txt and nonsilence_phones.txt are lists of silence and</span><br><span class=\"line\"># non-silence phones respectively (where silence includes various kinds of</span><br><span class=\"line\"># noise, laugh, cough, filled pauses etc., and nonsilence phones includes the</span><br><span class=\"line\"># &quot;real&quot; phones.)</span><br><span class=\"line\"># In each line of those files is a list of phones, and the phones on each line</span><br><span class=\"line\"># are assumed to correspond to the same &quot;base phone&quot;, i.e. they will be</span><br><span class=\"line\"># different stress or tone variations of the same basic phone.</span><br><span class=\"line\"># The file &quot;optional_silence.txt&quot; contains just a single phone (typically SIL)</span><br><span class=\"line\"># which is used for optional silence in the lexicon.</span><br><span class=\"line\"># extra_questions.txt might be empty; typically will consist of lists of phones,</span><br><span class=\"line\"># all members of each list with the same stress or tone; and also possibly a</span><br><span class=\"line\"># list for the silence phones.  This will augment the automatically generated</span><br><span class=\"line\"># questions (note: the automatically generated ones will treat all the</span><br><span class=\"line\"># stress/tone versions of a phone the same, so will not &quot;get to ask&quot; about</span><br><span class=\"line\"># stress or tone).</span><br></pre></td></tr></table></figure>\n\n<p>通过阅读脚本和脚本中的注释。可以知道<code>prepare_lang.sh</code>的用法</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Usage: utils/prepare_lang.sh &lt;dict-src-dir&gt; &lt;oov-dict-entry&gt; &lt;tmp-dir&gt; &lt;lang-dir&gt;</span><br><span class=\"line\">e.g.: utils/prepare_lang.sh data/local/dict &lt;SPOKEN_NOISE&gt; data/local/lang data/lang</span><br><span class=\"line\">--position-dependent-phones (true|false)        # default: true; if true, use _B, _E, _S &amp; _I</span><br></pre></td></tr></table></figure>\n\n<p><dict-src-dir>是我们在上一部分生成的词典目录。需要包含lexico.txt，extra_questions.txt，nonsilence_phones.txt，optional_silence.txt  silence_phones.txt。<br><code>position_dependent_phones</code>参数为false，导致解码后不能算出单词边界。很多的脚本，特别是评分脚本将不能正常运行。<br>第二个指令将语言模型转换成G.fst格式并保存在data/lang_test_tg 目录下。<br>这个脚本的核心内容是调用了arpa2fst和fstisstochastic，再创建了G.fst之后检查是否有空字符（<s>,</s>之类的）的循环。</dict-src-dir></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">arpa2fst --disambig-symbol=#0 --read-symbol-table=$test/words.txt input/task.arpabo $test/G.fst</span><br><span class=\"line\"></span><br><span class=\"line\">fstisstochastic $test/G.fst</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://blog.csdn.net/yutianzuijin/article/details/78756130\" target=\"_blank\" rel=\"noopener\">arpa2fst 原理详解</a></p>\n<blockquote>\n<p>arpa文件可以很容易地表示任意n-gram语言模型，不过在实际中n通常等于3、4或者5。arpa文件的每一行表示一个文法项，它通常包含三部分内容：probability word(s) [backoff probability]。probability表示该词或词组发生的概率，word(s)表示具体的词或者词组。backoff probablitiy是可选项，表示回退概率。</p>\n</blockquote>\n<p>在yesno这个toy project中只使用1元的语言模型。对应的arpa文件在<code>input/task.arpabo</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\data\\</span><br><span class=\"line\">ngram 1=4</span><br><span class=\"line\"></span><br><span class=\"line\">\\1-grams:</span><br><span class=\"line\">-1\tNO</span><br><span class=\"line\">-1\tYES</span><br><span class=\"line\">-99 &lt;s&gt;</span><br><span class=\"line\">-1 &lt;/s&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\\end\\</span><br></pre></td></tr></table></figure>\n\n<p>fstisstochastic命令则如同其名字的含义一样，用来检查G.fst是否是随机的。<br>在<code>s5/data/lang</code>目录下会出现：</p>\n<ol>\n<li><p><code>phones.txt</code>:将phone转换成数字。其中#0，#1是空字，用来表示句子的开头和结尾。<eps> 是一个特别的含义表示这个弧上没有符号。</eps></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;eps&gt; 0</span><br><span class=\"line\">SIL 1</span><br><span class=\"line\">Y 2</span><br><span class=\"line\">N 3</span><br><span class=\"line\">#0 4</span><br><span class=\"line\">#1 5</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><code>words.txt</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;eps&gt; 0</span><br><span class=\"line\">&lt;SIL&gt; 1</span><br><span class=\"line\">NO 2</span><br><span class=\"line\">YES 3</span><br><span class=\"line\">#0 4</span><br><span class=\"line\">&lt;s&gt; 5</span><br><span class=\"line\">&lt;/s&gt; 6</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><code>L_disambig.fst, L.fs</code>t: the dict can be recognized by Kaldi</p>\n</li>\n<li><p><code>topo</code>: phone states transition(HMM)</p>\n</li>\n<li><p><code>oov</code>: out of vocabulary. 仅包含<code>&lt;SIL&gt;</code></p>\n</li>\n<li><p><code>phones</code>: some information about phones</p>\n</li>\n</ol>\n<h2 id=\"特征提取和训练\"><a href=\"#特征提取和训练\" class=\"headerlink\" title=\"特征提取和训练\"></a>特征提取和训练</h2><p>MFCC特征提取和GMM-HMM建模</p>\n<p>提取梅尔倒谱系数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">steps/make_mfcc.sh --nj $num $input_dir $log_dir $output_dir</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://zhuanlan.zhihu.com/p/60371062\" target=\"_blank\" rel=\"noopener\">语音信号处理（二）—— MFCC详解</a><br>梅尔倒谱系数是一种非线性的时频表示法，其应用了人耳对低频声音的听觉敏感度更高的原理。</p>\n<p>接着正则化倒谱特征</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">steps/compute_cmvn_stats.sh </span><br><span class=\"line\">utils/fix_data_dir.sh $input_dir</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"http://placebokkk.github.io/kaldi/2019/08/05/asr-kaldi-feat2.html\" target=\"_blank\" rel=\"noopener\">Kaldi中的特征提取(二）- 特征变换</a><br>对训练集和测试集做同样的操作。<br>这里我采用的参数是 $num=1 $output_dir=mfcc<br>那么结果将会保存在mfcc文件夹下。<br>cmvn_test_yesno.ark  cmvn_test_yesno.scp  cmvn_train_yesno.ark  cmvn_train_yesno.scp。<br>ark文件包含特征向量（用cat打开是乱码），scp文件是关系文件，从发音者到ark文件的对应。<br>也可以直接跑脚本：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num=1 </span><br><span class=\"line\">output_dir=mfcc</span><br><span class=\"line\">for x in train_yesno test_yesno; do</span><br><span class=\"line\">  steps/make_mfcc.sh --nj 1 data/$x exp/make_mfcc/$x mfcc  </span><br><span class=\"line\">  steps/compute_cmvn_stats.sh data/$x exp/make_mfcc/$x mfcc        </span><br><span class=\"line\">  utils/fix_data_dir.sh data/$x </span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"单音素模型训练\"><a href=\"#单音素模型训练\" class=\"headerlink\" title=\"单音素模型训练\"></a>单音素模型训练</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">steps/train_mono.sh —nj $N —cmd $MAIN_CMD $DATA_DIR $LANG_DIR $OUTPUT_DIR</span><br></pre></td></tr></table></figure>\n\n<p>参数说明：<br>—nj：job的数量，来自同一个speaker的语音不能并行处理。所以在本项目中只能选择为1。<br>—cmd：为了使用本机的资源，调用”utils/run.pl”</p>\n<p>运行脚本：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_cmd=&quot;utils/run.pl&quot; steps/train_mono.sh --nj 1 --cmd &quot;$train_cmd&quot; \\   </span><br><span class=\"line\">--totgauss 400 \\   </span><br><span class=\"line\">data/train_yesno data/lang exp/mono0a</span><br></pre></td></tr></table></figure>\n\n<p>到现在我们已经完成了模型的训练。</p>\n<h2 id=\"解码和测试\"><a href=\"#解码和测试\" class=\"headerlink\" title=\"解码和测试\"></a>解码和测试</h2><p>接下来用测试集来验证一下模型的准确与否。<br>第一步是创建一个全连接的FST网络。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">utils/mkgraph.sh data/lang_test_tg exp/mono0a exp/mono0a/graph_tgpr</span><br></pre></td></tr></table></figure>\n\n<p>这个指令背后的脚本很长。基本上做的事情是创建以一个HCLG（HMM+Context+Lexicon+Grammer）的解码器并保存在exp/mono0a/graph_tgpr中。<br>还记得我们的每一条语音都是8个连续的Ken或Lo吗(Ken，Lo是希伯来语的yes no）?训练好的模型的工作就是找出这8个词的顺序。<br><code>steps/decode.sh [options] &lt;graph-dir&gt; &lt;data-dir&gt; &lt;decode-dir&gt;</code>用来寻找每一个测试音频的最佳路径</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">decode_cmd=&quot;utils/run.pl&quot; steps/decode.sh --nj 1 --cmd &quot;$decode_cmd&quot; \\</span><br><span class=\"line\">exp/mono0a/graph_tgpr data/test_yesno exp/mono0a/decode_test_yesno</span><br></pre></td></tr></table></figure>\n\n<p>最后是查看结果的环节。<br>在decode.sh内部最后会调用score.sh，这个脚本则生成预测的结果并且计算测试集Word error rate（WER）。<br>调用下列命令可以看到最好的效果：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for x in exp/*/decode*; do [ -d $x ] &amp;&amp; grep WER $x/wer_* | utils/best_wer.sh;</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%WER 0.00 [ 0 / 232, 0 ins, 0 del, 0 sub ] exp/mono0a/decode_test_yesno/wer_10_0.0</span><br></pre></td></tr></table></figure>\n\n<p>解读一下结果，花了很长时间才弄懂这些东西是什么意思。<br>WER后跟着的0.00是说字的错误率为0，即准确率为100%。<br>测试集一共29条音频，每条音频有8个字（单音素的字）。一共232个字。<br>参考Stanford的cs224s-17.lec04.pdf</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/4328467-dc5c0fcaf24b3750.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"WER的计算方法\"><br><img src=\"https://upload-images.jianshu.io/upload_images/4328467-1c85e0672f7772cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt><br>而wer结果文件中的10和0.0分别是lmwt(Language Weight)和wip(word insertion penalty，词插入惩罚)。lmwt用来平衡LM（语言模型）在多大程度上帮助AM（语音模型）<img src=\"https://upload-images.jianshu.io/upload_images/4328467-3e2341231bb987fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"LMWT用途 \nhttps://skemman.is/bitstream/1946/31280/1/msc_anna_vigdis_2018.pdf\"><br>至于<a href=\"https://www.zhihu.com/question/31416764/answer/353868805\" target=\"_blank\" rel=\"noopener\">wip</a>：</p>\n<blockquote>\n<p>word insertion penalty, 简写WIP, 是HMM识别匹配过程中用于设置句长的一个参数，可以用来调节生成句子中的单词个数，当前主流的语音识别系统主要采用的都是音素识别，即根据单词的音标而不是单词来进行匹配，这就导致了，在识别过程中，可能很难确定单词的gap，如果让系统自由识别，根据参数初始化的模型来进行匹配的话有可能会生成一些诡异的由长单词构成的句子，或者有很多短单词构成的句子，这些匹配率很低的句子对HMM参数的优化作用很小，同时也很大概率会导致学习速率奇慢或者局部最优解这样的问题，所以通过设置这个参数来得到一些更符合语义结构的生成片段。</p>\n</blockquote>\n<p>最后，我们调用的所有脚本都在run.sh中。</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"ckhub1tvn0000p92rvjm9bpif","tag_id":"ckhub1tvw0003p92rk7ab3qqi","_id":"ckhub1twd000ip92r74hgl0ej"},{"post_id":"ckhub1tvn0000p92rvjm9bpif","tag_id":"ckhub1tw20007p92rfyrfs2vv","_id":"ckhub1twf000kp92ryhr73wh8"},{"post_id":"ckhub1tvn0000p92rvjm9bpif","tag_id":"ckhub1tw4000ap92rb6y0ueqq","_id":"ckhub1twf000mp92rqcnxnu1v"},{"post_id":"ckhub1tvn0000p92rvjm9bpif","tag_id":"ckhub1tw8000dp92rpaamfq6w","_id":"ckhub1twf000np92rgjgrunuq"},{"post_id":"ckhub1tvt0002p92rlrjmeo7l","tag_id":"ckhub1twc000gp92r0wo30bd5","_id":"ckhub1twg000pp92rcr5znwml"},{"post_id":"ckhub1tvt0002p92rlrjmeo7l","tag_id":"ckhub1twf000lp92rerb7iu9n","_id":"ckhub1twg000qp92r9kmiuqcv"},{"post_id":"ckhub1tvx0004p92r9uvmxsar","tag_id":"ckhub1twg000op92rv03hqyt6","_id":"ckhub1twh000tp92r55m25cde"},{"post_id":"ckhub1tvx0004p92r9uvmxsar","tag_id":"ckhub1twg000rp92rozyppebi","_id":"ckhub1twh000up92rm2fr6i08"},{"post_id":"ckhub1tw10006p92raj4y6282","tag_id":"ckhub1twh000sp92r0iqrxqnj","_id":"ckhub1twi000wp92r0tjuyyt6"},{"post_id":"ckhub1tw20008p92ruwivphej","tag_id":"ckhub1twh000vp92rcjj4jd5q","_id":"ckhub1twk000zp92r8btt2nt1"},{"post_id":"ckhub1tw20008p92ruwivphej","tag_id":"ckhub1twj000xp92r5sd8imi8","_id":"ckhub1twk0010p92rsi95kaf1"},{"post_id":"ckhub1twa000fp92rq3opatat","tag_id":"ckhub1twj000yp92rs77779cv","_id":"ckhub1twl0012p92rdkn3bgwt"},{"post_id":"ckhub1twe000jp92r05gyh0i5","tag_id":"ckhub1twk0011p92rdmdqdjan","_id":"ckhub1twl0013p92rupa6z1b0"},{"post_id":"ckhub1txd0014p92r04xwpri6","tag_id":"ckhub1txe0015p92rmtdy7710","_id":"ckhub1txf0017p92r46bvzgzb"},{"post_id":"ckhub1txd0014p92r04xwpri6","tag_id":"ckhub1txe0016p92rlahsko6q","_id":"ckhub1txf0018p92rxspspbe1"}],"Tag":[{"name":"OpenCV","_id":"ckhub1tvw0003p92rk7ab3qqi"},{"name":"Numpy","_id":"ckhub1tw20007p92rfyrfs2vv"},{"name":"Python","_id":"ckhub1tw4000ap92rb6y0ueqq"},{"name":"编程记录","_id":"ckhub1tw8000dp92rpaamfq6w"},{"name":"MachineLearning","_id":"ckhub1twc000gp92r0wo30bd5"},{"name":"Coursera","_id":"ckhub1twf000lp92rerb7iu9n"},{"name":"软件测试","_id":"ckhub1twg000op92rv03hqyt6"},{"name":"Java","_id":"ckhub1twg000rp92rozyppebi"},{"name":"个人","_id":"ckhub1twh000sp92r0iqrxqnj"},{"name":"运维","_id":"ckhub1twh000vp92rcjj4jd5q"},{"name":"Postgres","_id":"ckhub1twj000xp92r5sd8imi8"},{"name":"Linux","_id":"ckhub1twj000yp92rs77779cv"},{"name":"阅读日报","_id":"ckhub1twk0011p92rdmdqdjan"},{"name":"Kaldi","_id":"ckhub1txe0015p92rmtdy7710"},{"name":"ASR","_id":"ckhub1txe0016p92rlahsko6q"}]}}